{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEON\n",
    "#### create training dataset from aeon raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.typing as npt\n",
    "import warnings\n",
    "import aeon\n",
    "import aeon.io.api as aeon_api\n",
    "\n",
    "from aeon.schema.schemas import exp02, social02\n",
    "from aeon.analysis.utils import *\n",
    "from dotmap import DotMap\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "def get_experiment_times(\n",
    "    root: str | os.PathLike, start_time: pd.Timestamp, end_time: pd.Timestamp\n",
    ") -> DotMap:\n",
    "    \"\"\"\n",
    "    Retrieve experiment start and stop times from environment states\n",
    "    (i.e. times outside of maintenance mode) occurring within the\n",
    "    given start and end times.\n",
    "\n",
    "    Args:\n",
    "        root (str or os.PathLike): The root path where epoch data is stored.\n",
    "        start_time (pandas.Timestamp): Start time.\n",
    "        end_time (pandas.Timestamp): End time.\n",
    "\n",
    "    Returns:\n",
    "        DotMap: A DotMap object containing two keys: 'start' and 'stop',\n",
    "        corresponding to pairs of experiment start and stop times.\n",
    "\n",
    "    Notes:\n",
    "    This function uses the last 'Maintenance' event (if available, otherwise\n",
    "    `end_time`) as the last 'Experiment' stop time. If the first retrieved state \n",
    "    is 'Maintenance' (e.g. 'Experiment' mode entered before `start_time`), \n",
    "    `start_time` is used as the first 'Experiment' start time.\n",
    "    \"\"\"\n",
    "\n",
    "    experiment_times = DotMap()\n",
    "    env_states = aeon.load(\n",
    "        root,\n",
    "        social02.Environment.EnvironmentState,\n",
    "        #aeon.io.reader.Csv(\"Environment_EnvironmentState_*\", [\"state\"]),\n",
    "        start_time,\n",
    "        end_time,\n",
    "    )\n",
    "    if env_states.empty:\n",
    "        warnings.warn(\"The environment state df is empty. \"\n",
    "                      \"Using input `start_time` and `end_time` as experiment times.\")\n",
    "        experiment_times.start = [start_time]\n",
    "        experiment_times.stop = [end_time]\n",
    "        return experiment_times\n",
    "    if env_states[\"state\"].iloc[-1] != \"Maintenance\":\n",
    "        warnings.warn(\"No 'Maintenance' event at the end of the search range. \"\n",
    "                      \"Using input `end_time` as last experiment stop time.\")\n",
    "        # Pad with a \"Maintenance\" event at the end\n",
    "        env_states = pd.concat(\n",
    "            [\n",
    "                env_states,\n",
    "                pd.DataFrame(\n",
    "                    \"Maintenance\",\n",
    "                    index=[end_time],\n",
    "                    columns=env_states.columns,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    # Use the last \"Maintenance\" event as end time\n",
    "    end_time = (env_states[env_states.state == \"Maintenance\"]).index[-1]\n",
    "    env_states = env_states[~env_states.index.duplicated(keep=\"first\")]\n",
    "    # Retain only events between visit start and stop times\n",
    "    env_states = env_states.iloc[\n",
    "        env_states.index.get_indexer([start_time], method=\"bfill\")[\n",
    "            0\n",
    "        ] : env_states.index.get_indexer([end_time], method=\"ffill\")[0] + 1\n",
    "    ]\n",
    "    # Retain only events where state changes (experiment-maintenance pairs)\n",
    "    env_states = env_states[env_states[\"state\"].ne(env_states[\"state\"].shift())]\n",
    "    if env_states[\"state\"].iloc[0] == \"Maintenance\":\n",
    "        warnings.warn(\"No 'Experiment' event at the start of the search range. \"\n",
    "                      \"Using input `end_time` as last experiment stop time.\")\n",
    "        # Pad with an \"Experiment\" event at the start\n",
    "        env_states = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(\n",
    "                    \"Experiment\",\n",
    "                    index=[start_time],\n",
    "                    columns=env_states.columns,\n",
    "                ),\n",
    "                env_states,\n",
    "            ]\n",
    "        )\n",
    "    experiment_times.start = env_states[\n",
    "        env_states[\"state\"] == \"Experiment\"\n",
    "    ].index.values\n",
    "    experiment_times.stop = env_states[\n",
    "        env_states[\"state\"] == \"Maintenance\"\n",
    "    ].index.values\n",
    "\n",
    "    return experiment_times\n",
    "\n",
    "\n",
    "def exclude_maintenance_data(\n",
    "    data: pd.DataFrame, experiment_times: DotMap\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Excludes rows not in experiment times (i.e., corresponding to maintenance times)\n",
    "    from the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The data to filter. Expected to have a pandas.DateTimeIndex.\n",
    "        experiment_times (DotMap): A DotMap object containing experiment start and stop times.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A pandas DataFrame containing filtered data.\n",
    "    \"\"\"\n",
    "    filtered_data = pd.concat(\n",
    "        [\n",
    "            data.loc[start:stop]\n",
    "            for start, stop in zip(experiment_times.start, experiment_times.stop)\n",
    "        ]\n",
    "    )\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def get_raw_tracking_data(\n",
    "    root: str | os.PathLike,\n",
    "    subj_id: str,\n",
    "    start: pd.Timestamp,\n",
    "    end: pd.Timestamp,\n",
    "    source_reader: aeon.io.reader.Video = exp02.CameraTop.Video,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve pos tracking and video data and assigns subject ID.\n",
    "\n",
    "    Args:\n",
    "        root (str or os.PathLike): The root path, or prioritised sequence of paths, where epoch data is stored.\n",
    "        subj_id (str): The subject ID string to be assigned.\n",
    "        start (pandas.Timestamp): The left bound of the time range to extract.\n",
    "        end (pandas.Timestamp): The right bound of the time range to extract.\n",
    "        source_reader (aeon.io.reader.Video, optional): The frame source reader. Default is exp02.CameraTop.Video.\n",
    "    Returns: \n",
    "        pandas.DataFrame: A pandas DataFrame containing pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "\n",
    "    subj_video = aeon_api.load(root, source_reader, start=start, end=end)\n",
    "    subj_pos = aeon_api.load(root, exp02.CameraTop.Position, start=start, end=end)\n",
    "    # replace \"raw\" in root with \"processed\"\n",
    "    processed_root = root.replace(\"raw\", \"processed\")\n",
    "    if subj_video.empty:\n",
    "        subj_video = aeon_api.load(processed_root, source_reader, start=start, end=end)\n",
    "        warnings.warn('subj_video is empty, retrieving data from processed')\n",
    "    if subj_pos.empty:\n",
    "        subj_pos = aeon_api.load(processed_root, exp02.CameraTop.Position, start=start, end=end)\n",
    "        warnings.warn('subj_pos is empty, retrieving data from processed')\n",
    "    subj_data = pd.merge_asof(\n",
    "        subj_video,\n",
    "        subj_pos,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        direction=\"nearest\",\n",
    "        tolerance=pd.Timedelta(\"1ms\"),\n",
    "    )[[\"x\", \"y\", \"id\", \"area\", \"_frame\", \"_path\"]]\n",
    "    subj_data.dropna(inplace=True)\n",
    "    subj_data[\"id\"] = subj_id\n",
    "    \n",
    "    return subj_data\n",
    "\n",
    "\n",
    "def sample_n_from_bins(subj_data: pd.DataFrame, n_samples: int = 1, n_bins: int = 50, range: npt.ArrayLike =[[0, 1440], [0, 1080]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Uniformly samples n number of data from x number of bins.\n",
    "\n",
    "    Args:\n",
    "        subj_data (pandas.DataFrame): A pandas DataFrame containing pos tracking and video data, and subject ID.\n",
    "        n_samples (int, optional): The number of samples to take from each bin. Default is 1.\n",
    "        n_bins (int, optional): The number of bins to use for sampling. Default is 50.\n",
    "        range (list of lists, optional): The leftmost and rightmost edges of the bins along each dimension \n",
    "            (if not specified explicitly in the bins parameters): [[xmin, xmax], [ymin, ymax]]. All values \n",
    "            outside of this range will be considered outliers and not tallied in the histogram. Default is \n",
    "            [[0, 1440], [0, 1080]].\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A pandas DataFrame containing uniformly-sampled pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "\n",
    "    hist_data = stats.binned_statistic_2d(\n",
    "        subj_data.x,\n",
    "        subj_data.y,\n",
    "        values=subj_data,\n",
    "        statistic=\"count\",\n",
    "        bins=n_bins,\n",
    "        range=range,\n",
    "    )\n",
    "    subj_data[\"bin\"] = hist_data.binnumber\n",
    "    sampled_data = (\n",
    "        subj_data.groupby([\"bin\"]).sample(n=n_samples, replace=True).drop_duplicates()\n",
    "    )\n",
    "    return sampled_data\n",
    "\n",
    "\n",
    "def create_session_dataset(\n",
    "    session: dict,\n",
    "    subj_ids: list = None,\n",
    "    plot_dist: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a dataset for a given session dict.\n",
    "    Args:\n",
    "        session (dict): A dictionary containing the root path, subject IDs, and their start and end times.\n",
    "        subj_ids (list, optional): A list of subject ids. If None, all subjects are selected.\n",
    "        plot_dist (bool, optional): Whether to plot the 1d and 2d histograms of x, y pos tracking for each subject.\n",
    "    Returns:\n",
    "        pandas.DataFrame: A pandas dataframe containing uniformly-sampled pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "    all_subj_data = pd.DataFrame()\n",
    "    if not subj_ids:\n",
    "        subj_ids = session[\"subjects\"].keys()\n",
    "    for subj in subj_ids:\n",
    "        subj_dict = {\n",
    "            \"id\": subj,\n",
    "            \"root\": session.get(\"root\", session[\"subjects\"][subj].get(\"root\")),\n",
    "            \"start\": session[\"subjects\"][subj][\"start\"],\n",
    "            \"end\": session[\"subjects\"][subj][\"end\"],\n",
    "        }\n",
    "        subj_data = (\n",
    "            create_subject_dataset(\n",
    "                subj_dict,\n",
    "                min_area=500,\n",
    "                n_samples=4,\n",
    "                n_bins=10,\n",
    "            )  # sample fewer points for manual annotation\n",
    "            if \"multi_\" in subj\n",
    "            else create_subject_dataset(\n",
    "                subj_dict,\n",
    "            )\n",
    "        )\n",
    "        all_subj_data = pd.concat([all_subj_data, subj_data])\n",
    "    if plot_dist:\n",
    "        fig = plot_position_histograms(all_subj_data)\n",
    "        fig.show()\n",
    "    return all_subj_data\n",
    "\n",
    "\n",
    "def plot_position_histograms(data: pd.DataFrame, n_bins: int = 50) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plots the 1d and 2d histograms of x, y pos tracking for each subject in a given DataFrame.\n",
    "    Args:\n",
    "        data (pandas.DataFrame): A pandas DataFrame containing x, y pos tracking and subject ID(s).\n",
    "        n_bins (int, optional): The number of bins to use for plotting histograms. Default is 50.\n",
    "    Returns:\n",
    "        matplotlib.pyplot: A plot containing 1d and 2d histograms of x, y pos tracking for each subject.\n",
    "    \"\"\"\n",
    "    subj_ids = data[\"id\"].unique()\n",
    "    fig, ax = plt.subplots(2, len(subj_ids))\n",
    "    n_bins = 50\n",
    "    if len(subj_ids) == 1:\n",
    "        data[[\"x\", \"y\"]].plot.hist(bins=n_bins, alpha=0.5, ax=ax[0], title=subj_ids[0])\n",
    "        ax[1].hist2d(\n",
    "            data.x,\n",
    "            data.y,\n",
    "            bins=(n_bins, n_bins),\n",
    "            cmap=plt.cm.jet,\n",
    "        )\n",
    "    else:\n",
    "        for i, subj_id in enumerate(subj_ids):\n",
    "            subj_data = data[data[\"id\"] == subj_id]\n",
    "            subj_data[[\"x\", \"y\"]].plot.hist(\n",
    "                bins=n_bins, alpha=0.5, ax=ax[0, i], title=subj_id\n",
    "            )\n",
    "            ax[1, i].hist2d(\n",
    "                subj_data.x, subj_data.y, bins=(n_bins, n_bins), cmap=plt.cm.jet\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_subject_dataset(\n",
    "    subject: dict,\n",
    "    min_area: float = None,\n",
    "    n_samples: int = 1,\n",
    "    n_bins: int = 50,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a dataset for a given subject dict.\n",
    "\n",
    "    Args:\n",
    "        subject (dict): A dictionary containing the root path, subject ID, and their start and end times.\n",
    "        min_area (float, optional): The minimum area of the subject to be included in the dataset. Default is None.\n",
    "        n_samples (int, optional): The number of samples to take from each bin. Default is 1.\n",
    "        n_bins (int, optional): The number of bins to use for sampling. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A pandas DataFrame containing uniformly-sampled pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "    subj_data = get_raw_tracking_data(\n",
    "        subject[\"root\"],\n",
    "        subject[\"id\"],\n",
    "        subject[\"start\"],\n",
    "        subject[\"end\"],\n",
    "    )\n",
    "    if min_area:\n",
    "        subj_data = subj_data[subj_data.area >= min_area] # when animals fuse\n",
    "    subj_data = sample_n_from_bins(subj_data, n_samples=n_samples, n_bins=n_bins)\n",
    "    return subj_data\n",
    "\n",
    "\n",
    "def create_fully_labelled_dataset(session: dict, subj_ids: list = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a fully labelled dataset for all or selected subjects of \n",
    "    a given session dict. Useful to \"bookmark\" frames for use with SLEAP's \n",
    "    predict \"only-labeled-frames\" option. \n",
    "\n",
    "    Args:\n",
    "        session (dict): A session dictionary.\n",
    "        subj_ids (list, optional): A list of subject ids. If None, all \n",
    "            subjects are selected.\n",
    "    Returns: \n",
    "        pandas.DataFrame: A pandas DataFrame containing pos tracking, \n",
    "            video data, and subject ID.\n",
    "    \"\"\"\n",
    "    all_subj_data = pd.DataFrame()\n",
    "    if not subj_ids:\n",
    "        subj_ids = session[\"subjects\"].keys()\n",
    "        subj_ids = [subj for subj in subj_ids if \"multi_\" not in subj]\n",
    "    for subj in subj_ids:\n",
    "        root = session.get(\"root\", session[\"subjects\"][subj].get(\"root\"))\n",
    "        subj_data = get_raw_tracking_data(\n",
    "            root,\n",
    "            subj,\n",
    "            session[\"subjects\"][subj][\"start\"],\n",
    "            session[\"subjects\"][subj][\"end\"],\n",
    "            source_reader=exp02.CameraTop.Video,\n",
    "        )\n",
    "        all_subj_data = pd.concat([all_subj_data, subj_data])\n",
    "            \n",
    "    return all_subj_data   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries for each session\n",
    "aeon2 = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON2/experiment0.2/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1101818\": {\n",
    "            \"start\": pd.Timestamp(\"2022-06-23 08:39:04.261089801\"),\n",
    "            \"end\": pd.Timestamp(\"2022-06-23 11:14:46.121759892\"),\n",
    "        },\n",
    "        \"BAA-1101819\": {\n",
    "            \"start\": pd.Timestamp(\"2022-06-21 13:28:10.593659878\"),\n",
    "            \"end\": pd.Timestamp(\"2022-06-21 16:34:29.241280079\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"BAA-1101818_819\",\n",
    "    \"multianimal_videos\": [\n",
    "        r\"Z:\\aeon\\data\\raw\\AEON2\\experiment0.2\\2022-06-22T08-51-10\\CameraTop\\CameraTop_2022-06-22T11-00-00.avi\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "aeon3 = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/presocial0.1\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3_NTP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-03 16:40:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-03 16:55:00\"),\n",
    "        },\n",
    "        \"AEON3_TP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-03 17:01:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-03 17:22:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-03 17:23:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-03 17:43:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"AEON3_NTP_TP_local\",\n",
    "    \"multianimal_videos\": [\n",
    "        r\"C:\\Users\\chuan\\Documents\\SLEAP_Test\\aeon3_multi\\CameraTop_2023-03-03T17-00-00_multianimal.avi\"\n",
    "    ],\n",
    "    # multianimal \"2023-03-03 17:23:00\" \"2023-03-03 17:43:00\"\n",
    "}\n",
    "\n",
    "aeon3b = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/presocial0.1\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-16 15:05:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-16 15:44:00\"),\n",
    "        },\n",
    "        \"AEON3B_TP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-16 16:00:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-16 16:36:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-16 16:37:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-16 17:19:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"AEON3B_NTP_TP_local\",\n",
    "    \"multianimal_videos\": [\n",
    "        r\"C:\\Users\\chuan\\Documents\\SLEAP_Test\\aeon3_multi\\CameraTop_2023-03-16T16-00-00_multianimal.avi\",\n",
    "        r\"C:\\Users\\chuan\\Documents\\SLEAP_Test\\aeon3_multi\\CameraTop_2023-03-16T17-00-00_multianimal.avi\",\n",
    "    ],\n",
    "    # multianimal \"2023-03-16 16:37:00\" \"2023-03-16 17:19:00\"\n",
    "}\n",
    "\n",
    "aeon3b_pattern = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/presocial0.1/\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 09:40:40\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 10:07:00\"),\n",
    "        },\n",
    "        \"AEON3B_TP1\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 10:10:44\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 10:46:30\"),\n",
    "        },\n",
    "        \"AEON3B_TP2\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 10:50:10\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 11:22:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 11:23:30\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 12:31:00\"),\n",
    "        }\n",
    "    },\n",
    "    \"session\": \"AEON3B_pattern_local\",\n",
    "    \"multianimal_videos\": [\n",
    "        r\"C:\\Users\\chuan\\Documents\\SLEAP_Test\\aeon3_multi\\CameraTop_2023-03-29T11-00-00_TP1TP2.avi\", # TP1 and TP2: 11:22:37 - 11:22:57\n",
    "        r\"C:\\Users\\chuan\\Documents\\SLEAP_Test\\aeon3_multi\\CameraTop_2023-03-29T11-00-00_multianimal.avi\", # all mice: 11:23:30 - 12:31:00\n",
    "        r\"C:\\Users\\chuan\\Documents\\SLEAP_Test\\aeon3_multi\\CameraTop_2023-03-29T12-00-00_multianimal.avi\", # all mice\n",
    "    ], \n",
    "}\n",
    "\n",
    "aeon3b_pattern_nest = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/presocial0.1/\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 09:40:40\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 10:07:00\"),\n",
    "        },\n",
    "        \"AEON3B_TP1\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 10:10:44\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 10:46:30\"),\n",
    "        },\n",
    "        \"AEON3B_TP2\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 10:50:10\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 11:22:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 11:23:30\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 12:31:00\"),\n",
    "        }\n",
    "    },\n",
    "    \"nest_coords\": [\n",
    "        (1223,475),\n",
    "        (1223,588),\n",
    "        (1352,479),\n",
    "        (1352,581),\n",
    "    ],\n",
    "    \"session\": \"AEON3B_pattern_local_nest\",\n",
    "    \"multianimal_videos\": [\n",
    "        r\"Y:\\ProjectAeon\\sleap_tracking\\CameraNest_2023-03-29T11-12-00_pattern_nest.avi\"# all mice\n",
    "    ], \n",
    "}\n",
    "\n",
    "aeon3b_pattern_tattoo = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/presocial0.1/\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": { # BAA-1103352\n",
    "            \"start\": pd.Timestamp(\"2023-06-16 16:13:30\"),\n",
    "            \"end\": pd.Timestamp(\"2023-06-16 17:00\"),\n",
    "        },\n",
    "        \"AEON3B_TP1\": { # BAA-1103353\n",
    "            \"start\": pd.Timestamp(\"2023-06-16 17:04\"),\n",
    "            \"end\": pd.Timestamp(\"2023-06-16 18:04\"),\n",
    "        },\n",
    "        \"AEON3B_TP2\": { # BAA-1103351\n",
    "            \"start\": pd.Timestamp(\"2023-06-16 18:08:19\"),\n",
    "            \"end\": pd.Timestamp(\"2023-06-16 18:38:52\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-06-16 18:41:30\"),\n",
    "            \"end\": pd.Timestamp(\"2023-06-16 19:12:00\"),\n",
    "        }\n",
    "    },\n",
    "    \"session\": \"AEON3B_pattern_tattoo\",\n",
    "    \"multianimal_videos\": [\n",
    "        r\"Y:\\ProjectAeon\\sleap_tracking\\CameraTop_2023-06-16T18-19-00_pattern_tattoo.avi\", # all mice\n",
    "    ], \n",
    "}\n",
    "\n",
    "aeon3b_pattern_tattoo2 = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/multianimal-test/\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": { # BAA-1103352\n",
    "            \"start\": pd.Timestamp(\"2023-07-04 16:28:10\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-04 17:01:57\"),\n",
    "        },\n",
    "        \"AEON3B_TP1\": { # CAA-1120139\n",
    "            \"start\": pd.Timestamp(\"2023-07-04 17:06:22\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-04 17:34:52\"),\n",
    "        },\n",
    "        \"AEON3B_TP2\": { # BAA-1103351\n",
    "            \"start\": pd.Timestamp(\"2023-07-04 15:36:19\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-04 16:16:19\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-07-04 17:37:08\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-04 18:10:13\"),\n",
    "        }\n",
    "    },\n",
    "    \"session\": \"AEON3B_pattern_tattoo2\",\n",
    "    \"multianimal_videos\": [\n",
    "        r\"Y:\\ProjectAeon\\sleap_tracking\\CameraTop_2023-07-04T17-18-00_pattern_tattoo.avi\", # all mice\n",
    "    ], \n",
    "}\n",
    "\n",
    "aeon3b_pattern_tattoo3 = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/multianimal-test/\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": { # BAA-1103352\n",
    "            \"start\": pd.Timestamp(\"2023-07-04 16:28:10\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-04 17:01:57\"),\n",
    "        },\n",
    "        \"AEON3B_TP1\": { # BAA-1103369\n",
    "            \"start\": pd.Timestamp(\"2023-07-28 14:24:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-28 15:15:00\"),\n",
    "        },\n",
    "        \"AEON3B_TP2\": { # BAA-1103351\n",
    "            \"start\": pd.Timestamp(\"2023-07-04 15:36:19\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-04 16:16:19\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-07-28 15:21:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-28 16:18:00\"),\n",
    "        },\n",
    "        \"multi_animal2\": {\n",
    "            \"start\": pd.Timestamp(\"2023-08-01 10:19:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-08-01 11:07:00\"),\n",
    "        },\n",
    "        \"multi_animal3\": {\n",
    "            \"start\": pd.Timestamp(\"2023-08-03 10:40:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-08-03 11:28:00\"),\n",
    "        },\n",
    "        \"multi_animal4\": {\n",
    "            \"start\": pd.Timestamp(\"2023-08-08 14:14:10.119999886\"),\n",
    "            \"end\": pd.Timestamp(\"2023-08-08 15:01:32.288000107\"),\n",
    "        }, \n",
    "        \"multi_animal5\": {\n",
    "            \"start\": pd.Timestamp(\"2023-08-11 11:54:01.340000153\"),\n",
    "            \"end\": pd.Timestamp(\"2023-08-11 13:20:36.303999901\"),\n",
    "        }, \n",
    "    },\n",
    "    \"session\": \"AEON3B_pattern_tattoo3\",\n",
    "}\n",
    "\n",
    "social_01 = {\n",
    "    \"subjects\": {\n",
    "        \"CAA-1120747\": { # CAA-1120747 \n",
    "            \"root\": \"Z:/aeon/data/raw/AEON4/social0.1/\",\n",
    "            \"start\": pd.Timestamp(\"2023-11-27 13:00:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-11-27 14:00:00\"),\n",
    "        },\n",
    "        \"CAA-1120746\": { # CAA-1120746\n",
    "            \"root\": \"Z:/aeon/data/raw/AEON3/social0.1/\",\n",
    "            \"start\": pd.Timestamp(\"2023-11-22 12:00:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-11-22 13:00:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"social_01\",\n",
    "}\n",
    "\n",
    "aeon3_social_dev_b5350ff = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/model-training/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104047\": { # no tattoo\n",
    "            \"start\": pd.Timestamp(\"2024-01-09 16:14:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-01-09 16:45:00\"),\n",
    "        },\n",
    "        \"BAA-1104045\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-01-09 16:50:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-01-09 17:20:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2024-01-09 15:37:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-01-09 16:09:00\"),\n",
    "        }\n",
    "    },\n",
    "    \"session\": \"aeon3_social_dev_b5350ff\",\n",
    "}\n",
    "\n",
    "aeon4_social_dev_b5350ff = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON4/model-training/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104049\": { # no tattoo\n",
    "            \"start\": pd.Timestamp(\"2024-01-09 16:46:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-01-09 17:15:00\"),\n",
    "        },\n",
    "        \"BAA-1104048\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-01-09 16:10:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-01-09 16:39:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2024-01-09 15:21:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-01-09 16:07:00\"),\n",
    "        }\n",
    "    },\n",
    "    \"session\": \"aeon4_social_dev_b5350ff\",\n",
    "}\n",
    "\n",
    "aeon4_social_dev_b5350ff2 = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON4/model-training/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104049\": { # no tattoo\n",
    "            \"start\": pd.Timestamp(\"2024-01-21 20:33:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-01-21 21:13:00\"),\n",
    "        },\n",
    "        \"BAA-1104048\": { # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-01-21 19:47:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-01-21 20:27:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2024-01-21 19:12:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-01-21 19:43:00\"),\n",
    "        }\n",
    "    },\n",
    "    \"session\": \"aeon4_social_dev_b5350ff2\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load subject states\n",
    "aeon_api.load(aeon4_social_dev_b5350ff2[\"root\"], aeon.io.reader.Subject(\"Environment_SubjectState_*\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment states\n",
    "aeon_api.load(\n",
    "    aeon4_social_dev_b5350ff2[\"root\"],\n",
    "    aeon.io.reader.Csv(\"Environment_EnvironmentState_*\", [\"state\"]),\n",
    "    pd.Timestamp(\"2024-01-21 20:28:14.791999817\"),\n",
    "    pd.Timestamp(\"2024-01-21 21:14:02.607999802\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract single + multi-animal frames as a single csv\n",
    "all_subj_data = create_session_dataset(aeon4_social_dev_b5350ff2)\n",
    "all_subj_data.to_csv(f'{aeon4_social_dev_b5350ff2[\"session\"]}.csv')\n",
    "all_subj_data.groupby([\"id\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fully labelled datasets for SLEAP model validation\n",
    "single_animal_data = create_fully_labelled_dataset(aeon4_social_dev_b5350ff2, subj_ids=[subj for subj in aeon4_social_dev_b5350ff2[\"subjects\"].keys() if \"multi_\" not in subj])\n",
    "single_animal_data.to_csv(f'{aeon4_social_dev_b5350ff2[\"session\"]}_single_animal_frames.csv')\n",
    "multi_animal_data = create_fully_labelled_dataset(aeon4_social_dev_b5350ff2, subj_ids=[subj for subj in aeon4_social_dev_b5350ff2[\"subjects\"].keys() if \"multi_\" in subj])\n",
    "multi_animal_data.to_csv(f'{aeon4_social_dev_b5350ff2[\"session\"]}_multi_animal_frames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve timestamps with temporal discontinuities for RETRAINING\n",
    "import numpy as np\n",
    "\n",
    "def adjust_ids(group, classes):\n",
    "    if len(group['id'].unique()) != 2:\n",
    "        # Get the id of the row with the higher class_likelihood\n",
    "        max_id = group.loc[group[\"class_likelihood\"].idxmax(), \"id\"]\n",
    "        # Set the id of the row with the lower class_likelihood to the id that's not max_id\n",
    "        group.loc[group[\"class_likelihood\"].idxmin(), \"id\"] = [id for id in classes if id != max_id][0]\n",
    "    return group\n",
    "\n",
    "# load SLEAP Pose data\n",
    "root = \"/ceph/aeon/aeon/data/raw/AEON3/social0.2/\"\n",
    "start_time = pd.Timestamp(\"2024-02-09 17:00\")\n",
    "end_time = pd.Timestamp(\"2024-02-09 18:00\")\n",
    "exp_times = get_experiment_times(root, start_time, end_time)\n",
    "pos = aeon_api.load(root, social02.CameraTop.Pose, start=start_time, end=end_time)\n",
    "pos = exclude_maintenance_data(pos, exp_times)\n",
    "pos[\"class\"] = pos[\"class\"].astype(int)\n",
    "classes = [\"BAA-1104045\", \"BAA-1104047\"] # from SLEAP model config # AEON3[\"BAA-1104045\", \"BAA-1104047\"] # AEON4[\"BAA-1104048\", \"BAA-1104049\"]\n",
    "# assign subject IDs\n",
    "pos[\"id\"] = pos[\"class\"].map(lambda x: classes[x])\n",
    "pos_sorted = pos.sort_values(by=[\"class\", pos.index.name])\n",
    "pos_sorted[\"dist\"] = pos_sorted.groupby(\"class\")[[\"x\", \"y\"]].diff().apply(lambda x: np.linalg.norm(x), axis=1)\n",
    "# set 100 as threshold for temporal discontinuities\n",
    "violations = pos_sorted[pos_sorted[\"dist\"] > 100]\n",
    "sampled_violations = (violations.\n",
    "    groupby(\"class\").\n",
    "    apply(sample_n_from_bins, n_samples=2, n_bins=10, include_groups=False).\n",
    "    reset_index().\n",
    "    set_index(\"time\").\n",
    "    sort_index()\n",
    ")\n",
    "# retrieve frames with temporal discontinuities\n",
    "top_frames = aeon_api.load(root, social02.CameraTop.Video, time=sampled_violations.index.unique())\n",
    "# merge frames with pose data\n",
    "top_frames_pos = pos[[\"id\", \"x\", \"y\", \"class_likelihood\"]].merge(top_frames, left_index=True, right_index=True, how=\"inner\")\n",
    "top_frames_pos = top_frames_pos.reset_index()\n",
    "top_frames_pos = top_frames_pos.groupby(\"time\")[top_frames_pos.columns].apply(adjust_ids, classes=classes).set_index(\"time\")\n",
    "top_frames_pos.to_csv(\"/ceph/aeon/aeon/code/scratchpad/sleap/social0.2/AEON3/aeon3_social_02_retrain.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import sleap\n",
    "import numpy as np\n",
    "\n",
    "from sleap.io.pathutils import fix_path_separator\n",
    "from sleap.gui.suggestions import VideoFrameSuggestions\n",
    "from sleap.nn.config import *\n",
    "from sleap.nn.inference import main as sleap_track\n",
    "from sleap.nn.inference import TopDownMultiClassPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_tracks(labels: sleap.Labels) -> sleap.Labels:\n",
    "    \"\"\"\n",
    "    Removes instances from SLEAP Labels object where track = None.\n",
    "\n",
    "    Args:\n",
    "        labels (sleap.Labels): A SLEAP Labels object.\n",
    "    Returns: \n",
    "        sleap.Labels: A SLEAP Labels object with only tracked instances in each frame.\n",
    "    \"\"\"\n",
    "    lfs = [lf.remove_untracked() for lf in labels.labeled_frames]\n",
    "    return sleap.Labels(labeled_frames=lfs, videos=labels.videos, skeletons=labels.skeletons, tracks=labels.tracks)\n",
    "\n",
    "\n",
    "def generate_slp_dataset(subj_data: pd.DataFrame, skeleton: sleap.Skeleton) -> sleap.Labels:\n",
    "    \"\"\"\n",
    "    Generates .slp dataset for a given session dict.\n",
    "\n",
    "    Args:\n",
    "        subj_data (pandas.DataFrame): A pandas DataFrame containing the labeled data for a given session.\n",
    "        skeleton (sleap.Skeleton): A sleap Skeleton object.\n",
    "    Returns: \n",
    "        sleap.Labels: A SLEAP Labels object containing labeled frames.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create tracks dictionary from subj_ids that are not multi_animal\n",
    "    tracks_dict = {\n",
    "        subj: sleap.Track(spawned_on=0, name=subj)\n",
    "        for subj in subj_data[\"id\"].unique()\n",
    "        if \"multi_\" not in subj\n",
    "    }\n",
    "\n",
    "    lfs = []\n",
    "\n",
    "    # create video dictionary from new labels\n",
    "    videos_dict = {\n",
    "        video: sleap.Video.from_filename(video, grayscale=True)\n",
    "        for video in subj_data._path.unique()\n",
    "    }\n",
    "    \n",
    "    # for each unique frame, create a new labeled frame\n",
    "    for _, row in subj_data.drop_duplicates(subset=[\"_path\", \"_frame\"]).iterrows():\n",
    "        instances = []\n",
    "        if \"multi_\" in row.id:\n",
    "            # duplicate instance for each track\n",
    "            for track in tracks_dict.keys():\n",
    "                instances.append(\n",
    "                    sleap.Instance(\n",
    "                        skeleton=skeleton,\n",
    "                        track=tracks_dict[track],\n",
    "                        points={\"centroid\": sleap.instance.Point(row.x, row.y)},\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            # create a new instance for each row\n",
    "            instances += [\n",
    "                (\n",
    "                    sleap.Instance(\n",
    "                        skeleton=skeleton,\n",
    "                        track=tracks_dict[inst.id],\n",
    "                        points={\"centroid\": sleap.instance.Point(inst.x, inst.y)},\n",
    "                    )\n",
    "                ) for _, inst in subj_data[(subj_data[\"_path\"] == row._path) & (subj_data[\"_frame\"] == row._frame)].iterrows()]\n",
    "        # create a new labeled frame\n",
    "        lf = sleap.instance.LabeledFrame(\n",
    "            video=videos_dict[row._path],\n",
    "            frame_idx=row._frame,\n",
    "            instances=instances,\n",
    "        )\n",
    "        lfs.append(lf)\n",
    "\n",
    "    return sleap.Labels(labeled_frames=lfs)\n",
    "\n",
    "\n",
    "def update_slp_video_paths(labels: \"sleap.Labels\", old_path: str, new_path: str) -> sleap.Labels:\n",
    "    \"\"\"\n",
    "    Updates video paths in a SLEAP labels object (e.g., to move training from local to remote machine).\n",
    "    \n",
    "    Args:\n",
    "        labels (sleap.Labels): A SLEAP Labels object.\n",
    "        old_path (str): Old path to video files.\n",
    "        new_path (str): New path to video files.\n",
    "\n",
    "    Returns:\n",
    "        sleap.Labels: A SLEAP Labels object with updated video paths.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    videos =  [sleap.Video.from_filename(fix_path_separator(vid.filename).replace(old_path, new_path), grayscale=True) for vid in labels.videos]\n",
    "\n",
    "    lfs = []\n",
    "    for lf in labels.labeled_frames:\n",
    "        lf = sleap.instance.LabeledFrame(\n",
    "                video=videos[labels.videos.index(lf.video)],\n",
    "                frame_idx=lf.frame_idx,\n",
    "                instances=lf.instances,\n",
    "            )\n",
    "        lfs.append(lf)\n",
    "    \n",
    "    return sleap.Labels(labeled_frames=lfs, videos=videos, skeletons=labels.skeletons, tracks=labels.tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new skeleton\n",
    "skeleton = sleap.Skeleton()\n",
    "skeleton.add_node(\"centroid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate slp retrain dataset\n",
    "subj_data = pd.read_csv(\"/ceph/aeon/aeon/code/scratchpad/sleap/social0.2/AEON3/aeon3_social_02_retrain.csv\")\n",
    "labels = generate_slp_dataset(subj_data, skeleton)\n",
    "sleap.Labels.save_file(labels, \"/ceph/aeon/aeon/code/scratchpad/sleap/social0.2/AEON3/aeon3_social_02_retrain.slp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate slp training dataset for all subjects\n",
    "subj_data = pd.read_csv(f'{aeon4_social_dev_b5350ff2[\"session\"]}.csv')\n",
    "labels = generate_slp_dataset(subj_data, skeleton)\n",
    "sleap.Labels.save_file(labels, f'{aeon4_social_dev_b5350ff2[\"session\"]}.slp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate separate single, multi-animal .slp datasets for prediction/evaluation\n",
    "# single-animal frames are GT for evaluation\n",
    "# multi-animal frames are labelled to be used as bookmarks for prediction\n",
    "csv_files = [\n",
    "    f'{aeon4_social_dev_b5350ff2[\"session\"]}_single_animal_frames.csv',\n",
    "    f'{aeon4_social_dev_b5350ff2[\"session\"]}_multi_animal_frames.csv',\n",
    "]\n",
    "for csv_file in csv_files:\n",
    "    subj_data = pd.read_csv(csv_file)\n",
    "    labels = generate_slp_dataset(aeon4_social_dev_b5350ff2, subj_data, skeleton)\n",
    "    labels = update_slp_video_paths(\n",
    "        labels=labels,\n",
    "        old_path=\"Z:\", \n",
    "        new_path=\"/ceph/aeon\"\n",
    "    )\n",
    "    sleap.Labels.save_file(labels, f'{Path(csv_file).stem}_ceph.slp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial parameters\n",
    "subj_id = f'{aeon3b_pattern_tattoo3[\"session\"]}'\n",
    "run_name_centroid = f'{subj_id}_topdown_top.centroid'\n",
    "run_name_centered_instance = f'{subj_id}_topdown_top.centered_instance_multiclass'\n",
    "root = \"/ceph/aeon/aeon/code/scratchpad/sleap/tail_pattern/\"\n",
    "runs_folder = root + \"models/\"\n",
    "predictions_folder = root + \"predictions/\"\n",
    "groundtruth_folder = root + \"groundtruth/\"\n",
    "\n",
    "try:\n",
    "    skeleton\n",
    "except NameError:\n",
    "    # create new skeleton\n",
    "    skeleton = sleap.Skeleton()\n",
    "    skeleton.add_node(\"centroid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update local video paths to ceph\n",
    "new_labels = update_slp_video_paths(\n",
    "    labels=sleap.load_file(f'{subj_id}.slp'), \n",
    "    old_path=\"Z:\", \n",
    "    new_path=\"/ceph/aeon\")\n",
    "sleap.Labels.save_file(new_labels, f'{root}{subj_id}.slp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels into train/val/test\n",
    "labels = sleap.load_file(f'{root}{subj_id}.slp')\n",
    "\n",
    "# generate a 0.8/0.1/0.1 train/val/test split\n",
    "labels_train, labels_val_test = labels.split(n=0.8) \n",
    "labels_val, labels_test = labels_val_test.split(n=0.5)\n",
    "\n",
    "# Save with images\n",
    "labels_train.save(f'{root}{subj_id}.train.pkg.slp')#, with_images=True)\n",
    "labels_val.save(f'{root}{subj_id}.val.pkg.slp')#, with_images=True)\n",
    "labels_test.save(f'{root}{subj_id}.test.pkg.slp')#, with_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid model\n",
    "# initalise default training job config\n",
    "cfg = TrainingJobConfig()\n",
    "cfg.data.labels.training_labels = f'{root}{subj_id}.train.pkg.slp'\n",
    "cfg.data.labels.validation_labels = f'{root}{subj_id}.val.pkg.slp'\n",
    "cfg.data.labels.test_labels = f'{root}{subj_id}.test.pkg.slp'\n",
    "\n",
    "# preprocessing and training params\n",
    "cfg.data.preprocessing.input_scaling = 0.75 #0.5\n",
    "cfg.data.instance_cropping.center_on_part = \"centroid\"\n",
    "cfg.data.instance_cropping.crop_size = 96 # set crop size manually\n",
    "cfg.optimization.augmentation_config.rotate = True\n",
    "cfg.optimization.epochs = 600 #200\n",
    "cfg.optimization.batch_size = 4\n",
    "\n",
    "cfg.optimization.initial_learning_rate = 0.0001\n",
    "cfg.optimization.learning_rate_schedule.reduce_on_plateau = True\n",
    "cfg.optimization.learning_rate_schedule.reduction_factor = 0.5\n",
    "cfg.optimization.learning_rate_schedule.plateau_min_delta = 1e-06 \n",
    "cfg.optimization.learning_rate_schedule.plateau_patience = 20 #5\n",
    "cfg.optimization.learning_rate_schedule.plateau_cooldown = 3\n",
    "cfg.optimization.learning_rate_schedule.min_learning_rate = 1e-08\n",
    "\n",
    "cfg.optimization.early_stopping.stop_training_on_plateau = True\n",
    "cfg.optimization.early_stopping.plateau_min_delta = 1e-08\n",
    "cfg.optimization.early_stopping.plateau_patience = 30 #20\n",
    "\n",
    "# configure nn and model\n",
    "cfg.model.backbone.unet = UNetConfig(\n",
    "    max_stride=16,\n",
    "    filters=16,\n",
    "    filters_rate=2.00,\n",
    "    output_stride=2,\n",
    "    #up_interpolate=True, # save computations but may lower accuracy\n",
    ")\n",
    "cfg.model.heads.centroid = CentroidsHeadConfig(\n",
    "    anchor_part=\"centroid\",\n",
    "    sigma=2.5,\n",
    "    output_stride=2\n",
    ")\n",
    "\n",
    "# configure outputs\n",
    "cfg.outputs.run_name = run_name_centroid\n",
    "cfg.outputs.save_outputs = True\n",
    "cfg.outputs.runs_folder = runs_folder\n",
    "cfg.outputs.save_visualizations = True\n",
    "cfg.outputs.checkpointing.initial_model = True\n",
    "cfg.outputs.checkpointing.best_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "trainer.setup()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part detection model: centered instance + multi-class\n",
    "# initalise default training job config\n",
    "cfg = TrainingJobConfig()\n",
    "\n",
    "# update path to 0.8/0.1/0.1 train/val/test split\n",
    "cfg.data.labels.training_labels = f'{root}{subj_id}.train.pkg.slp'\n",
    "cfg.data.labels.validation_labels = f'{root}{subj_id}.val.pkg.slp'\n",
    "cfg.data.labels.test_labels = f'{root}{subj_id}.test.pkg.slp'\n",
    "cfg.data.labels.skeletons = [skeleton] # load skeleton\n",
    "\n",
    "# preprocessing and training params\n",
    "cfg.data.preprocessing.input_scaling = 1.0\n",
    "cfg.data.instance_cropping.center_on_part = \"centroid\"\n",
    "cfg.data.instance_cropping.crop_size = 96 # set crop size manually\n",
    "cfg.optimization.augmentation_config.rotate = True\n",
    "cfg.optimization.epochs = 600\n",
    "cfg.optimization.batch_size = 8 # 4\n",
    "\n",
    "cfg.optimization.initial_learning_rate = 0.0001\n",
    "cfg.optimization.learning_rate_schedule.reduce_on_plateau = True\n",
    "cfg.optimization.learning_rate_schedule.reduction_factor = 0.1 #0.5\n",
    "cfg.optimization.learning_rate_schedule.plateau_min_delta = 1e-08 #1e-06 \n",
    "cfg.optimization.learning_rate_schedule.plateau_patience = 20 #5\n",
    "cfg.optimization.learning_rate_schedule.plateau_cooldown = 3\n",
    "cfg.optimization.learning_rate_schedule.min_learning_rate = 1e-08\n",
    "\n",
    "cfg.optimization.early_stopping.stop_training_on_plateau = True\n",
    "cfg.optimization.early_stopping.plateau_min_delta = 1e-08\n",
    "cfg.optimization.early_stopping.plateau_patience = 30 #20\n",
    "\n",
    "# configure nn and model\n",
    "cfg.model.backbone.unet = UNetConfig(\n",
    "    max_stride=16, #32,\n",
    "    output_stride=2, #4,\n",
    "    filters=16, #24,\n",
    "    filters_rate=1.5,\n",
    "    #up_interpolate=True, # save computations but may lower accuracy\n",
    ")\n",
    "confmaps=CenteredInstanceConfmapsHeadConfig(\n",
    "    anchor_part=\"centroid\",\n",
    "    sigma=1.5, #2.5, \n",
    "    output_stride=2, #4, \n",
    "    loss_weight=1.0, \n",
    ") \n",
    "# load labels.slp to get track names\n",
    "labels = sleap.load_file(f'{root}{subj_id}.slp')\n",
    "class_vectors=ClassVectorsHeadConfig(\n",
    "    classes = [track.name for track in labels.tracks],\n",
    "    output_stride=2, #16, #4,\n",
    "    num_fc_layers=3,\n",
    "    num_fc_units=256,\n",
    "    global_pool=True,\n",
    "    loss_weight=0.01 # TODO: try 1.0\n",
    ")\n",
    "cfg.model.heads.multi_class_topdown = MultiClassTopDownConfig(\n",
    "    confmaps=confmaps,\n",
    "    class_vectors=class_vectors\n",
    ")\n",
    "\n",
    "# configure outputs\n",
    "cfg.outputs.run_name = run_name_centered_instance\n",
    "cfg.outputs.save_outputs = True\n",
    "cfg.outputs.runs_folder = runs_folder\n",
    "cfg.outputs.save_visualizations = True\n",
    "cfg.outputs.checkpointing.initial_model = True\n",
    "cfg.outputs.checkpointing.best_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume training\n",
    "# Load config.\n",
    "model_path = \"models/AEON3_NTP_TP_local_topdown_top.centroid/\"\n",
    "cfg = sleap.load_config(model_path)\n",
    "\n",
    "# Create and initialize the trainer.\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "trainer.setup()\n",
    "\n",
    "# Replace the randomly initialized weights with the saved weights.\n",
    "trainer.keras_model.load_weights(f'{model_path}best_model.h5')\n",
    "\n",
    "trainer.config.optimization.epochs = 200\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.keras_model.outputs[0].shape) # confmaps  \n",
    "print(trainer.keras_model.outputs[1].shape) # id part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial parameters\n",
    "subj_id = f'{aeon3b_pattern_tattoo3[\"session\"]}'\n",
    "run_name_centroid = f'{subj_id}_topdown_top.centroid'\n",
    "run_name_centered_instance = f'{subj_id}_topdown_top.centered_instance_multiclass'\n",
    "root = \"/ceph/aeon/aeon/code/scratchpad/sleap/tail_pattern/\"\n",
    "runs_folder = root + \"models/\"\n",
    "predictions_folder = root + \"predictions/\"\n",
    "groundtruth_folder = root + \"groundtruth/\"\n",
    "\n",
    "print(run_name_centroid, run_name_centered_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a single multi-animal video\n",
    "subj_idx = 0\n",
    "video_idx = 0\n",
    "\n",
    "multi_subj_ids = [subj_id for subj_id in aeon3b_pattern_tattoo3[\"subjects\"].keys() if \"multi_\" in subj_id]\n",
    "\n",
    "# select one multi-animal session only\n",
    "input_file = f'{root}{aeon3b_pattern_tattoo3[\"session\"]}_{multi_subj_ids[subj_idx]}.slp'\n",
    "\n",
    "# infer on user-labeled frames on the first video only\n",
    "output_file_pr = f'{predictions_folder}{subj_id}_{multi_subj_ids[subj_idx]}_pr.slp'\n",
    "sleap_track(\n",
    "    [\n",
    "        input_file,\n",
    "        \"--model\",\tf'{runs_folder}{run_name_centroid}',\n",
    "        \"--model\",\tf'{runs_folder}{run_name_centered_instance}',\n",
    "        \"--only-labeled-frames\",\n",
    "        \"--video.index\", str(video_idx),\n",
    "        \"--output\", output_file_pr,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 100 suggestions based on low-scoring frames to be proofread\n",
    "labels_pr = sleap.load_file(output_file_pr)\n",
    "output_file_low_gt = f'{groundtruth_folder}{subj_id}_1_{multi_subj_ids[subj_idx]}_low_gt.slp'\n",
    "labels_missing = sleap.Labels([label for label in labels_pr.labels if label.n_predicted_instances < 3])\n",
    "suggestions = VideoFrameSuggestions.suggest(\n",
    "    labels=labels_missing,\n",
    "    params=dict(\n",
    "        videos=[labels_missing.videos[video_idx]],\n",
    "        method=\"prediction_score\",\n",
    "        score_limit=0.5,\n",
    "        instance_limit_lower=1,\n",
    "        instance_limit_upper=3,\n",
    "    ),\n",
    ")\n",
    "\n",
    "if len(suggestions) > 100:\n",
    "    suggestions = random.sample(suggestions, 100)\n",
    "\n",
    "lfs = []\n",
    "for suggestion in suggestions:\n",
    "    matching_frames = labels_missing.find(video=labels_missing.videos[video_idx], frame_idx=suggestion.frame_idx)\n",
    "    if matching_frames:\n",
    "        lf = matching_frames[0]\n",
    "        instances = []\n",
    "        for instance in lf.instances_to_show:\n",
    "            instances.append(\n",
    "                sleap.Instance(\n",
    "                    skeleton=instance.skeleton,\n",
    "                    track=instance.track,\n",
    "                    points={\"centroid\": sleap.instance.Point(instance.points[0].x, instance.points[0].y)}\n",
    "\n",
    "                )\n",
    "            )\n",
    "        lfs.append(sleap.instance.LabeledFrame(\n",
    "            video=lf.video,\n",
    "            frame_idx=lf.frame_idx,\n",
    "            instances=instances,\n",
    "        ))\n",
    "sleap.Labels.save_file(sleap.Labels(labeled_frames=lfs), output_file_low_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 100 random consecutive frames as ground truth data for evaluation\n",
    "output_file_rand_gt = f'{groundtruth_folder}{subj_id}_{multi_subj_ids[subj_idx]}_rand_gt.slp'\n",
    "random_frame_from = True\n",
    "consecutive = True\n",
    "num_frames = 100 # 3000 = 1min\n",
    "\n",
    "if consecutive:\n",
    "        frame_from = random.randint(0, len(labels_pr.labeled_frames) - num_frames) if random_frame_from else 0\n",
    "        selected_frame_idx = list(range(frame_from, frame_from + num_frames))\n",
    "else: \n",
    "    selected_frame_idx = random.sample(range(len(labels_pr.labeled_frames)), num_frames)\n",
    "\n",
    "sfs = []\n",
    "for idx in selected_frame_idx:\n",
    "    sf = labels_pr.labeled_frames[idx]\n",
    "    instances = []\n",
    "    for instance in sf.instances_to_show:\n",
    "        instances.append(\n",
    "            sleap.Instance(\n",
    "                skeleton=instance.skeleton,\n",
    "                track=instance.track,\n",
    "                points={\"centroid\": sleap.instance.Point(instance.points[0].x, instance.points[0].y)}\n",
    "\n",
    "            )\n",
    "        )\n",
    "    sfs.append(sleap.instance.LabeledFrame(\n",
    "        video=sf.video,\n",
    "        frame_idx=sf.frame_idx,\n",
    "        instances=instances,\n",
    "    ))\n",
    "\n",
    "sleap.Labels.save_file(sleap.Labels(labeled_frames=sfs), output_file_rand_gt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate single-animal predictions with ground truth data\n",
    "gt_file = \"/ceph/aeon/aeon/code/scratchpad/sleap/social_dev_b5350ff/predictions/aeon4_social_dev_b5350ff2_single_animal_frames_ceph.slp\"\n",
    "pr_file = \"/ceph/aeon/aeon/code/scratchpad/sleap/social_dev_b5350ff/predictions/aeon4_social_dev_b5350ff2_single_animal_frames_ceph_pr.slp\"\n",
    "labels_gt = sleap.load_file(gt_file)\n",
    "labels_pr = sleap.load_file(pr_file)\n",
    "metrics = sleap.nn.evals.evaluate(labels_gt, labels_pr, oks_scale=96)\n",
    "framepairs = sleap.nn.evals.find_frame_pairs(labels_gt, labels_pr)\n",
    "matches = sleap.nn.evals.match_frame_pairs(framepairs, scale=96)\n",
    "positive_pairs = matches[0]\n",
    "false_negatives = matches[1]\n",
    "track_names = [track.name for track in labels_gt.tracks]\n",
    "correct_id = {track_name: [] for track_name in track_names}\n",
    "for positive_pair in positive_pairs:\n",
    "    gt = positive_pair[0] if isinstance(positive_pair[1], sleap.PredictedInstance) else positive_pair[1]\n",
    "    correct_id[gt.track.name].append(positive_pair[0].track.name == positive_pair[1].track.name)\n",
    "track_sums = 0\n",
    "track_lens = 0\n",
    "for track in track_names:\n",
    "    track_sums += sum(correct_id[track])\n",
    "    track_lens += len(correct_id[track])\n",
    "    print(f'{track} Accuracy: {round(sum(correct_id[track]) / len(correct_id[track]), 3)}')\n",
    "print(\"ID accuracy:\", round(track_sums/track_lens, 3))\n",
    "print(\"Total tracks:\", len(labels_gt.all_instances))\n",
    "print(\"Tracks identified:\", len(labels_pr.all_instances))\n",
    "print(\"Tracks correctly identified:\", track_sums)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "predictor = TopDownMultiClassPredictor.from_trained_models(\n",
    "    centroid_model_path=\"/ceph/aeon/aeon/code/scratchpad/sleap/social_dev_b5350ff/models/aeon3_social_dev_b5350ff_ceph_topdown_top.centroid\",\n",
    "    confmap_model_path=\"/ceph/aeon/aeon/code/scratchpad/sleap/social_dev_b5350ff/models/aeon3_social_dev_b5350ff_ceph_topdown_top.centered_instance_multiclass\",\n",
    "    resize_input_layer=False, # SLEAP 1.3.0+ \n",
    ")\n",
    "predictor.export_model(\n",
    "    \"/ceph/aeon/aeon/data/processed/test-node1/4310907/2024-01-12T19-00-00/topdown-multianimal-id-133/\", \n",
    "    max_instances=2, \n",
    "    unrag_outputs=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0e3000693f8b9aa662d7863f075a07b6f4295c6c6941a96ccabdea0fbe2a07d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
