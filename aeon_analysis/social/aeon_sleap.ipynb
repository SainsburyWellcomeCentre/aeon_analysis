{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create training dataset from aeon raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 0.2\n",
    "# solo BAA-1101818 (tail+ear painted)\n",
    "# 2022-06-23 08:39:04.261089801\tBAA-1101818\t26.4\tEnter \n",
    "# 2022-06-23 11:14:46.121759892\tBAA-1101818\t28.0\tExit\n",
    "# 2022-06-24 09:32:37.183360100\tBAA-1101818\t26.9\tEnter (ear repainted)\n",
    "# 2022-06-24 12:29:54.365859985\tBAA-1101818\t27.8\tExit\n",
    "\n",
    "# solo BAA-1101819\n",
    "# 2022-06-21 13:28:10.593659878\tBAA-1101819\t25.4\tEnter\n",
    "# 2022-06-21 16:34:29.241280079\tBAA-1101819\t26.4\tExit\n",
    "\n",
    "# multianimal BAA-1101818 and BAA-1101819\n",
    "# 2022-06-22 10:40:00\t        BAA-1101819\t24.9\tEnter\n",
    "# 2022-06-22 13:29:04.050240040\tBAA-1101818\t28.4\tExit \n",
    "# 2022-06-23 11:24:23.876420021\tBAA-1101819\t25.6\tEnter\n",
    "# 2022-06-23 14:19:39.241819859\tBAA-1101818\t26.4\tExit\n",
    "\n",
    "# exp 0.3\n",
    "# multianimal BAA-1102505 and BAA-1102506\n",
    "# 1904-01-03 22:03:16.696000000\tBAA-1102505\t20\tEnter\n",
    "# 1904-01-03 22:03:30.928000000\tBAA-1102506\t20\tEnter\n",
    "# 1904-01-03 23:57:31.952000000\tBAA-1102505\t20\tExit\n",
    "# 1904-01-03 23:57:37.824000000\tBAA-1102506\t20\tExit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import aeon.io.api as aeon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from aeon.schema.dataset import exp02\n",
    "from aeon.analysis.utils import *\n",
    "from dotmap import DotMap\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# function to retrieve aeon data\n",
    "def get_raw_tracking_data(root, subj_id, start, end):\n",
    "    subj_video = aeon.load(root, exp02.CameraTop.Video, start=start, end=end)\n",
    "    subj_pos = aeon.load(root, exp02.CameraTop.Position, start=start, end=end)\n",
    "    subj_data = pd.merge_asof(\n",
    "        subj_video,\n",
    "        subj_pos,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        direction=\"nearest\",\n",
    "        tolerance=pd.Timedelta(\"1ms\"),\n",
    "    )[[\"x\", \"y\", \"id\", \"area\", \"_frame\", \"_path\"]]\n",
    "    subj_data.dropna(inplace=True)\n",
    "    subj_data[\"id\"] = subj_id\n",
    "    return subj_data\n",
    "\n",
    "\n",
    "# function to sample n number of data from x number of bins\n",
    "def sample_n_from_bins(subj_data, n=3, bin_size=50):\n",
    "    hist_data = stats.binned_statistic_2d(\n",
    "        subj_data.x, subj_data.y, values=subj_data, statistic=\"count\", bins=bin_size\n",
    "    )\n",
    "    subj_data[\"bin\"] = hist_data.binnumber\n",
    "    sampled_data = (\n",
    "        subj_data.groupby([\"bin\"]).sample(n=n, replace=True).drop_duplicates()\n",
    "    )\n",
    "    return sampled_data\n",
    "\n",
    "\n",
    "# function to plot distribution of sampled data\n",
    "def plot_sampled_data(files: list):\n",
    "    fig, ax = plt.subplots(2, len(files))\n",
    "    data = [pd.read_csv(file)[[\"x\", \"y\"]] for file in files]\n",
    "    if len(files) == 1:\n",
    "        data[0][[\"x\", \"y\"]].plot.hist(\n",
    "            bins=100, alpha=0.5, ax=ax[0], title=Path(files[0]).stem\n",
    "        )\n",
    "        ax[1].hist2d(data[0].x, data[0].y, bins=(50, 50), cmap=plt.cm.jet)\n",
    "    else:\n",
    "        for i, sampled_data in enumerate(data):\n",
    "            sampled_data[[\"x\", \"y\"]].plot.hist(\n",
    "                bins=100, alpha=0.5, ax=ax[0, i], title=Path(files[i]).stem\n",
    "            )\n",
    "            ax[1, i].hist2d(\n",
    "                sampled_data.x, sampled_data.y, bins=(50, 50), cmap=plt.cm.jet\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_dataset(session: dict, plot_dist: bool = False):\n",
    "    # create dataset and visualise distribution for a given session dict\n",
    "    for subj in session[\"subjects\"].keys():\n",
    "        subj_data = get_raw_tracking_data(\n",
    "            session[\"root\"],\n",
    "            subj,\n",
    "            session[\"subjects\"][subj][\"start\"],\n",
    "            session[\"subjects\"][subj][\"end\"],\n",
    "        )\n",
    "        subj_data = sample_n_from_bins(subj_data)\n",
    "        subj_data.to_csv(subj + \".csv\")\n",
    "    if plot_dist:\n",
    "        fig = plot_sampled_data(\n",
    "            [subj_id + \".csv\" for subj_id in session[\"subjects\"].keys()]\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries for each session\n",
    "aeon2 = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON2/experiment0.2/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1101818\": {\n",
    "            \"start\": pd.Timestamp(\"2022-06-23 08:39:04.261089801\"),\n",
    "            \"end\": pd.Timestamp(\"2022-06-23 11:14:46.121759892\"),\n",
    "        },\n",
    "        \"BAA-1101819\": {\n",
    "            \"start\": pd.Timestamp(\"2022-06-21 13:28:10.593659878\"),\n",
    "            \"end\": pd.Timestamp(\"2022-06-21 16:34:29.241280079\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"BAA-1101818_819\",\n",
    "    \"multianimal_videos\": [\n",
    "        \"/ceph/aeon/aeon/data/raw/AEON2/experiment0.2/2022-06-22T08-51-10/CameraTop/CameraTop_2022-06-22T11-00-00.avi\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "aeon3 = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON3/presocial0.1\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3_NTP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-03 16:40:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-03 16:55:00\"),\n",
    "        },\n",
    "        \"AEON3_TP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-03 17:01:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-03 17:22:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"AEON3_NTP_TP_local\",\n",
    "    \"multianimal_videos\": [\n",
    "        \"/ceph/aeon/aeon/data/raw/AEON3/presocial0.1/2023-03-03T13-44-16/CameraTop/CameraTop_2023-03-03T17-00-00.avi\",\n",
    "    ],\n",
    "    # multianimal \"2023-03-03 17:23:00\" \"2023-03-03 17:43:00\"\n",
    "}\n",
    "\n",
    "aeon3b = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON3/presocial0.1\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-16 15:05:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-16 15:44:00\"),\n",
    "        },\n",
    "        \"AEON3B_TP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-16 16:00:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-16 16:36:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"AEON3B_NTP_TP_local\",\n",
    "    \"multianimal_videos\": [\n",
    "        \"/ceph/aeon/aeon/data/raw/AEON3/presocial0.1/2023-03-16T11-24-11/CameraTop/CameraTop_2023-03-16T16-00-00_multianimal.avi\",\n",
    "        \"/ceph/aeon/aeon/data/raw/AEON3/presocial0.1/2023-03-16T11-24-11/CameraTop/CameraTop_2023-03-16T17-00-00_multianimal.avi\",\n",
    "    ],\n",
    "    # multianimal \"2023-03-16 16:37:00\" \"2023-03-16 17:19:00\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(aeon3b, plot_dist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import sleap\n",
    "\n",
    "from pathlib import Path\n",
    "from sleap.io.dataset import Labels\n",
    "from sleap.io.video import Video\n",
    "from sleap.gui.suggestions import VideoFrameSuggestions\n",
    "from sleap.nn.config import *\n",
    "from sleap.nn.inference import main as sleap_track\n",
    "from sleap.nn.inference import TopDownMultiClassPredictor, Predictor, TopDownPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_slp_dataset(subj_id, skeleton, suggest_labels=False):\n",
    "    # load csv file as pandas dataframe\n",
    "    subj_data = pd.read_csv(subj_id + \".csv\")\n",
    "\n",
    "    # create a new track\n",
    "    track = sleap.Track(spawned_on=0, name=subj_id)\n",
    "\n",
    "    # create new labels dataset\n",
    "    new_labels = sleap.Labels(\n",
    "        labeled_frames=[],\n",
    "        videos=[sleap.Video.from_filename(vid) for vid in subj_data._path.unique()],\n",
    "        skeletons=[skeleton],\n",
    "    )\n",
    "\n",
    "    lfs = []\n",
    "    if suggest_labels:\n",
    "        # generate labelling suggestions\n",
    "        suggestions = VideoFrameSuggestions.suggest(\n",
    "            labels=new_labels,\n",
    "            params=dict(\n",
    "                videos=new_labels.videos,\n",
    "                method=\"image_features\",\n",
    "                per_video=200,\n",
    "                sample_method=\"stride\",\n",
    "                scale=1.0,\n",
    "                merge_video_features=\"parallel\",  # \"across all videos\"\n",
    "                feature_type=\"brisk\",\n",
    "                pca_components=5,\n",
    "                n_clusters=10,\n",
    "                per_cluster=20,\n",
    "            ),\n",
    "        )\n",
    "        # create a new labeled frame for each suggestion\n",
    "        for suggestion in suggestions:\n",
    "            data = subj_data[\n",
    "                (subj_data[\"_path\"] == suggestion.video.filename)\n",
    "                & (subj_data[\"_frame\"] == suggestion.frame_idx)\n",
    "            ]\n",
    "            if len(data):  # if frame is found\n",
    "                data = data.iloc[0]\n",
    "                lf = sleap.instance.LabeledFrame(\n",
    "                    video=suggestion.video,\n",
    "                    frame_idx=suggestion.frame_idx,\n",
    "                    instances=[\n",
    "                        sleap.Instance(\n",
    "                            skeleton=skeleton,\n",
    "                            track=track,\n",
    "                            points={\"centroid\": sleap.instance.Point(data.x, data.y)},\n",
    "                        )\n",
    "                    ],\n",
    "                )\n",
    "                lfs.append(lf)\n",
    "    else:\n",
    "        # create video dictionary from new labels\n",
    "        video_dict = {}\n",
    "        for video in new_labels.videos:\n",
    "            video_dict[video.filename] = video\n",
    "\n",
    "        # create a new labeled frame for each row in subj_data\n",
    "        for _, row in subj_data.iterrows():\n",
    "            # create a new labeled frame\n",
    "            lf = sleap.instance.LabeledFrame(\n",
    "                video=video_dict[row._path],\n",
    "                frame_idx=row._frame,\n",
    "                instances=[\n",
    "                    sleap.Instance(\n",
    "                        skeleton=skeleton,\n",
    "                        track=track,\n",
    "                        points={\"centroid\": sleap.instance.Point(row.x, row.y)},\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "            lfs.append(lf)\n",
    "\n",
    "    return sleap.Labels(labeled_frames=lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new skeleton\n",
    "skeleton = sleap.Skeleton()\n",
    "skeleton.add_node(\"centroid\")\n",
    "\n",
    "# generate slp datasets for each subject\n",
    "base_labels = None\n",
    "for i, subj in enumerate(aeon3b[\"subjects\"].keys()):\n",
    "    if i == 0:\n",
    "        base_labels = generate_slp_dataset(subj, skeleton)\n",
    "    elif base_labels:\n",
    "        merge_results = Labels.complex_merge_between(base_labels, generate_slp_dataset(subj, skeleton))\n",
    "\n",
    "sleap.Labels.save_file(base_labels, aeon3b[\"session\"] + \".slp\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial parameters\n",
    "subj_id = aeon3b[\"session\"]\n",
    "run_name_centroid = subj_id + \"_topdown_top.centroid\"\n",
    "run_name_centered_instance = subj_id + \"_topdown_top.centered_instance_multiclass\"\n",
    "runs_folder = \"models\"\n",
    "\n",
    "try:\n",
    "    skeleton\n",
    "except NameError:\n",
    "    # create new skeleton\n",
    "    skeleton = sleap.Skeleton()\n",
    "    skeleton.add_node(\"centroid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels into train/val/test\n",
    "labels = sleap.load_file(subj_id + \".slp\")\n",
    "\n",
    "# generate a 0.8/0.1/0.1 train/val/test split\n",
    "labels_train, labels_val_test = labels.split(n=0.8) \n",
    "labels_val, labels_test = labels_val_test.split(n=0.5)\n",
    "\n",
    "# Save with images\n",
    "labels_train.save(subj_id + \".train.pkg.slp\")#, with_images=True)\n",
    "labels_val.save(subj_id + \".val.pkg.slp\")#, with_images=True)\n",
    "labels_test.save(subj_id + \".test.pkg.slp\")#, with_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid model\n",
    "# initalise default training job config\n",
    "cfg = TrainingJobConfig()\n",
    "cfg.data.labels.training_labels = subj_id + \".train.pkg.slp\"\n",
    "cfg.data.labels.validation_labels = subj_id + \".val.pkg.slp\"\n",
    "cfg.data.labels.test_labels = subj_id + \".test.pkg.slp\"\n",
    "\n",
    "# preprocessing and training params\n",
    "cfg.data.preprocessing.input_scaling = 0.75 #0.5\n",
    "cfg.data.instance_cropping.center_on_part = \"centroid\"\n",
    "cfg.data.instance_cropping.crop_size = 128 # set crop size manually\n",
    "cfg.optimization.augmentation_config.rotate = True\n",
    "cfg.optimization.epochs = 600 #200\n",
    "cfg.optimization.batch_size = 4\n",
    "\n",
    "cfg.optimization.initial_learning_rate = 0.0001\n",
    "cfg.optimization.learning_rate_schedule.reduce_on_plateau = True\n",
    "cfg.optimization.learning_rate_schedule.reduction_factor = 0.5\n",
    "cfg.optimization.learning_rate_schedule.plateau_min_delta = 1e-06 \n",
    "cfg.optimization.learning_rate_schedule.plateau_patience = 20 #5\n",
    "cfg.optimization.learning_rate_schedule.plateau_cooldown = 3\n",
    "cfg.optimization.learning_rate_schedule.min_learning_rate = 1e-08\n",
    "\n",
    "cfg.optimization.early_stopping.stop_training_on_plateau = True\n",
    "cfg.optimization.early_stopping.plateau_min_delta = 1e-08\n",
    "cfg.optimization.early_stopping.plateau_patience = 30 #20\n",
    "\n",
    "# configure nn and model\n",
    "cfg.model.backbone.unet = UNetConfig(\n",
    "    max_stride=16,\n",
    "    filters=16,\n",
    "    filters_rate=2.00,\n",
    "    output_stride=2,\n",
    "    #up_interpolate=True, # save computations but may lower accuracy\n",
    ")\n",
    "cfg.model.heads.centroid = CentroidsHeadConfig(\n",
    "    anchor_part=\"centroid\",\n",
    "    sigma=2.5,\n",
    "    output_stride=2\n",
    ")\n",
    "\n",
    "# configure outputs\n",
    "cfg.outputs.run_name = run_name_centroid\n",
    "cfg.outputs.save_outputs = True\n",
    "cfg.outputs.runs_folder = runs_folder\n",
    "cfg.outputs.save_visualizations = True\n",
    "cfg.outputs.checkpointing.initial_model = True\n",
    "cfg.outputs.checkpointing.best_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "trainer.setup()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part detection model: centered instance + multi-class\n",
    "# initalise default training job config\n",
    "cfg = TrainingJobConfig()\n",
    "\n",
    "# update path to 0.8/0.1/0.1 train/val/test split\n",
    "cfg.data.labels.training_labels = subj_id + \".train.pkg.slp\"\n",
    "cfg.data.labels.validation_labels = subj_id + \".val.pkg.slp\"\n",
    "cfg.data.labels.test_labels = subj_id + \".test.pkg.slp\"\n",
    "cfg.data.labels.skeletons = [skeleton] # load skeleton\n",
    "\n",
    "# preprocessing and training params\n",
    "cfg.data.preprocessing.input_scaling = 1.0\n",
    "cfg.data.instance_cropping.center_on_part = \"centroid\"\n",
    "cfg.data.instance_cropping.crop_size = 128 # set crop size manually\n",
    "cfg.optimization.augmentation_config.rotate = True\n",
    "cfg.optimization.epochs = 600\n",
    "cfg.optimization.batch_size = 8 # 4\n",
    "\n",
    "cfg.optimization.initial_learning_rate = 0.0001\n",
    "cfg.optimization.learning_rate_schedule.reduce_on_plateau = True\n",
    "cfg.optimization.learning_rate_schedule.reduction_factor = 0.1 #0.5\n",
    "cfg.optimization.learning_rate_schedule.plateau_min_delta = 1e-08 #1e-06 \n",
    "cfg.optimization.learning_rate_schedule.plateau_patience = 20 #5\n",
    "cfg.optimization.learning_rate_schedule.plateau_cooldown = 3\n",
    "cfg.optimization.learning_rate_schedule.min_learning_rate = 1e-08\n",
    "\n",
    "cfg.optimization.early_stopping.stop_training_on_plateau = True\n",
    "cfg.optimization.early_stopping.plateau_min_delta = 1e-08\n",
    "cfg.optimization.early_stopping.plateau_patience = 25 #20\n",
    "\n",
    "# configure nn and model\n",
    "cfg.model.backbone.unet = UNetConfig(\n",
    "    max_stride=16, #32,\n",
    "    output_stride=2, #4,\n",
    "    filters=16, #24,\n",
    "    filters_rate=1.5,\n",
    "    #up_interpolate=True, # save computations but may lower accuracy\n",
    ")\n",
    "confmaps=CenteredInstanceConfmapsHeadConfig(\n",
    "    anchor_part=\"centroid\",\n",
    "    sigma=1.5, #2.5, \n",
    "    output_stride=2, #4, \n",
    "    loss_weight=1.0, \n",
    ")\n",
    "class_vectors=ClassVectorsHeadConfig(\n",
    "    classes = list(aeon3b[\"subjects\"].keys()),\n",
    "    output_stride=2, #16, #4,\n",
    "    num_fc_layers=3,\n",
    "    num_fc_units=256,\n",
    "    global_pool=True,\n",
    "    loss_weight=0.1 # TODO: try 1.0\n",
    ")\n",
    "cfg.model.heads.multi_class_topdown = MultiClassTopDownConfig(\n",
    "    confmaps=confmaps,\n",
    "    class_vectors=class_vectors\n",
    ")\n",
    "\n",
    "# configure outputs\n",
    "cfg.outputs.run_name = run_name_centered_instance\n",
    "cfg.outputs.save_outputs = True\n",
    "cfg.outputs.runs_folder = runs_folder\n",
    "cfg.outputs.save_visualizations = True\n",
    "cfg.outputs.checkpointing.initial_model = True\n",
    "cfg.outputs.checkpointing.best_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume training\n",
    "# Load config.\n",
    "model_path = \"models/AEON3_NTP_TP_local_topdown_top.centroid/\"\n",
    "cfg = sleap.load_config(model_path)\n",
    "\n",
    "# Create and initialize the trainer.\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "trainer.setup()\n",
    "\n",
    "# Replace the randomly initialized weights with the saved weights.\n",
    "trainer.keras_model.load_weights(model_path + \"best_model.h5\")\n",
    "\n",
    "trainer.config.optimization.epochs = 200\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.keras_model.outputs[0].shape) # confmaps  nx1x2\n",
    "print(trainer.keras_model.outputs[1].shape) # id part nx2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_id = aeon3b[\"session\"]\n",
    "run_name_centroid = subj_id + \"_topdown_top.centroid\"\n",
    "run_name_centered_instance = subj_id + \"_topdown_top.centered_instance_multiclass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer on selected frames\n",
    "video = sleap.load_video(aeon3b[\"multianimal_videos\"][1])\n",
    "\n",
    "# infer on 3000 consecutive frames (1min) in the video\n",
    "frame_from = random.randint(0, video.num_frames - 3000)\n",
    "frame_idx = \",\".join([str(idx) for idx in list(range(frame_from, frame_from + 3000))])\n",
    "\n",
    "sleap_track(\n",
    "    [\n",
    "        video.filename,\n",
    "        \"--model\",\t\"models/\" + run_name_centroid,\n",
    "        \"--model\",\t\"models/\" + run_name_centered_instance,\n",
    "        \"--frames\", frame_idx, # comment out to infer on entire video\n",
    "        \"--output\", \"predictions/\" + Path(video.filename).stem + \"_pr.slp\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### hold-out test set (single-animal videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_id = aeon3b[\"session\"]\n",
    "run_name_centroid = subj_id + \"_topdown_top.centroid\"\n",
    "run_name_centered_instance = subj_id + \"_topdown_top.centered_instance_multiclass\"\n",
    "# load model\n",
    "predictor = TopDownMultiClassPredictor.from_trained_models(\n",
    "    centroid_model_path=\"models/\" + run_name_centroid,\n",
    "    confmap_model_path=\"models/\" + run_name_centered_instance,\n",
    ")\n",
    "\n",
    "# load ground truth labels (hold-out test set)\n",
    "labels_gt = sleap.load_file(subj_id + \".test.pkg.slp\")\n",
    "labels_pr = predictor.predict(labels_gt)\n",
    "\n",
    "# compute metrics\n",
    "metrics = sleap.nn.evals.evaluate(labels_gt, labels_pr, oks_scale=128)\n",
    "\n",
    "# match ground truth and predicted frames\n",
    "framepairs = sleap.nn.evals.find_frame_pairs(labels_gt, labels_pr)\n",
    "matches = sleap.nn.evals.match_frame_pairs(framepairs, scale=128)\n",
    "positive_pairs = matches[0]\n",
    "false_negatives = matches[1]\n",
    "\n",
    "correct_id = []\n",
    "for positive_pair in positive_pairs:\n",
    "    correct_id.append(positive_pair[0].track.name == positive_pair[1].track.name)\n",
    "print(\"Tracks identified:\", len(correct_id))\n",
    "print(\"Tracks correctly identified:\", sum(correct_id))\n",
    "print(\"Total tracks:\", len(labels_gt.all_instances))\n",
    "print(\"ID accuracy:\", sum(correct_id)/len(correct_id))\n",
    "print(\"vis - TP:\", metrics[\"vis.tp\"])\n",
    "print(\"vis - FP:\", metrics[\"vis.fp\"])\n",
    "print(\"vis - TN:\", metrics[\"vis.tn\"])\n",
    "print(\"vis - FN:\", metrics[\"vis.fn\"])\n",
    "print(\"vis - Precision:\", metrics[\"vis.precision\"])\n",
    "print(\"vis - Recall:\", metrics[\"vis.recall\"])\n",
    "print(\"Error distance (50%):\", metrics[\"dist.p50\"])\n",
    "print(\"Error distance (90%):\", metrics[\"dist.p90\"])\n",
    "print(\"Error distance (95%):\", metrics[\"dist.p95\"])\n",
    "print(\"Error distance (99%):\", metrics[\"dist.p99\"])\n",
    "print(\"mAP:\", metrics[\"oks_voc.mAP\"])\n",
    "print(\"mAR:\", metrics[\"oks_voc.mAR\"])\n",
    "print(\"Visibility precision:\", metrics[\"vis.precision\"])\n",
    "print(\"Visibility recall:\", metrics[\"vis.recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### multi-animal video (unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sleap.io.convert import main as sleap_convert\n",
    "\n",
    "video = sleap.load_video(aeon3b[\"multianimal_videos\"][1])\n",
    "# convert .slp to .h5\n",
    "file_input_slp = \"predictions/\" + Path(video.filename).stem + \"_pr.slp\"\n",
    "file_output_h5 = \"predictions/\" + Path(video.filename).stem + \"_pr.slp.h5\"\n",
    "\n",
    "sleap_convert([\n",
    "    \"--format\", \"analysis\",\n",
    "    \"-o\", file_output_h5,\n",
    "    file_input_slp,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "files = [\n",
    "    \"predictions/\" + Path(video.filename).stem + \"_pr.slp.h5\",\n",
    "]\n",
    "locations = []\n",
    "occupancy_matrix = []\n",
    "# concat predictions in multiple files\n",
    "for file in files:\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        dset_names = list(f.keys())\n",
    "        locations.append(f[\"tracks\"][:].T)\n",
    "        occupancy_matrix.append(f[\"track_occupancy\"][:].T)\n",
    "\n",
    "locations = np.vstack(locations)\n",
    "occupancy_matrix = np.hstack(occupancy_matrix)\n",
    "frame_count, node_count, _, instance_count = locations.shape\n",
    "# find all frames where < 2 tracks are present\n",
    "missing_tracks = np.where(np.sum(occupancy_matrix, axis=0) < 2)[0]\n",
    "print(\"frame count:\", frame_count)\n",
    "print(\"node count:\", node_count)\n",
    "print(\"instance count:\", instance_count)\n",
    "print(\"track occupancy:\", np.sum(occupancy_matrix, axis=1)/locations.shape[0])\n",
    "print(\"frames with missing tracks:\", len(missing_tracks)/locations.shape[0], \"(\", len(missing_tracks), \"/\", locations.shape[0], \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot x, -y tracks for each instance\n",
    "cmap = mpl.colormaps['hsv']\n",
    "\n",
    "bodyparts = [\"centroid\"]\n",
    "mice = list(aeon3b[\"subjects\"].keys())\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16,8), sharex=True, sharey=True)\n",
    "offset = 100\n",
    "for m in range(len(mice)):\n",
    "    for i in range(len(bodyparts)):\n",
    "        #ax[m].plot(locations[:, i, 0, m] + i*offset, c=cmap(0.1 * i), label=bodyparts[i], linestyle = 'solid', alpha=0.5)\n",
    "        #ax[m].plot(-1 * (locations[:, i, 1, m] + i*offset), c=cmap(0.1 * i), linestyle = 'dotted', alpha=0.5)\n",
    "        ax[m].plot(locations[:, i, 0, m] + i*offset, c=\"r\", label=bodyparts[i], linestyle = 'solid')\n",
    "        ax[m].plot(-1 * (locations[:, i, 1, m] + i*offset), c=\"b\", linestyle = 'solid')\n",
    "    ax[m].set_title(mice[m])\n",
    "    ax[m].set_xlabel(\"frame\")\n",
    "    ax[m].set_ylabel(\"x and -y pos\")\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "plt.legend(handles, labels, loc=(1.1, 0))\n",
    "plt.tight_layout()   \n",
    "#plt.show()\n",
    "plt.savefig(\"predictions/\" + subj_id + \"_pr_tracks.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### render video with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sleap.io.visuals import main as sleap_render\n",
    "\n",
    "file_input_json = \"predictions/\" + Path(video.filename).stem + \"_pr.json\"\n",
    "file_output_avi = \"predictions/\" + Path(video.filename).stem + \"_pr.avi\"\n",
    "labels_pr = sleap.load_file(\"predictions/\" + Path(video.filename).stem + \"_pr.slp\")\n",
    "\n",
    "# save .slp as .json\n",
    "Labels.save_file(labels_pr, file_input_json)\n",
    "selected_frames = list(range(8000,11000))\n",
    "# convert to string\n",
    "frame_idx = \",\".join([str(idx) for idx in selected_frames])\n",
    "sleap_render(\n",
    "    [file_input_json, \n",
    "    \"--marker_size\", \"1\", \n",
    "    \"-o\", file_output_avi, \n",
    "    \"--fps\", \"50\", \n",
    "    \"--frames\",  frame_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "subj_id = aeon3b[\"session\"]\n",
    "run_name_centroid = subj_id + \"_topdown_top.centroid\"\n",
    "run_name_centered_instance = subj_id + \"_topdown_top.centered_instance_multiclass\"\n",
    "\n",
    "predictor = TopDownMultiClassPredictor.from_trained_models(\n",
    "    centroid_model_path=\"models/\" + run_name_centroid,\n",
    "    confmap_model_path=\"models/\" + run_name_centered_instance,\n",
    ")\n",
    "predictor.export_model(subj_id + \"_topdown_multiclass\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0e3000693f8b9aa662d7863f075a07b6f4295c6c6941a96ccabdea0fbe2a07d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
