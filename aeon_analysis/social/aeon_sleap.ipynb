{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create training dataset from aeon raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp 0.2\n",
    "# solo BAA-1101818 (tail+ear painted)\n",
    "# 2022-06-23 08:39:04.261089801\tBAA-1101818\t26.4\tEnter \n",
    "# 2022-06-23 11:14:46.121759892\tBAA-1101818\t28.0\tExit\n",
    "# 2022-06-24 09:32:37.183360100\tBAA-1101818\t26.9\tEnter (ear repainted)\n",
    "# 2022-06-24 12:29:54.365859985\tBAA-1101818\t27.8\tExit\n",
    "\n",
    "# solo BAA-1101819\n",
    "# 2022-06-21 13:28:10.593659878\tBAA-1101819\t25.4\tEnter\n",
    "# 2022-06-21 16:34:29.241280079\tBAA-1101819\t26.4\tExit\n",
    "\n",
    "# multianimal BAA-1101818 and BAA-1101819\n",
    "# 2022-06-22 10:40:00\t        BAA-1101819\t24.9\tEnter\n",
    "# 2022-06-22 13:29:04.050240040\tBAA-1101818\t28.4\tExit \n",
    "# 2022-06-23 11:24:23.876420021\tBAA-1101819\t25.6\tEnter\n",
    "# 2022-06-23 14:19:39.241819859\tBAA-1101818\t26.4\tExit\n",
    "\n",
    "# exp 0.3\n",
    "# multianimal BAA-1102505 and BAA-1102506\n",
    "# 1904-01-03 22:03:16.696000000\tBAA-1102505\t20\tEnter\n",
    "# 1904-01-03 22:03:30.928000000\tBAA-1102506\t20\tEnter\n",
    "# 1904-01-03 23:57:31.952000000\tBAA-1102505\t20\tExit\n",
    "# 1904-01-03 23:57:37.824000000\tBAA-1102506\t20\tExit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEON\n",
    "#### create training dataset from aeon raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import aeon.io.api as aeon\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.typing as npt\n",
    "import cv2\n",
    "\n",
    "from aeon.schema.dataset import exp02\n",
    "from aeon.analysis.utils import *\n",
    "from aeon.io.video import frames, export\n",
    "from dotmap import DotMap\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from matplotlib import path\n",
    "\n",
    "\n",
    "def get_raw_tracking_data(\n",
    "    root: str,\n",
    "    subj_id: str,\n",
    "    start: \"pd.Timestamp\",\n",
    "    end: \"pd.Timestamp\",\n",
    "    source_frame: \"aeon.io.reader.Video\" = exp02.CameraTop.Video,\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieves pos tracking and video data and assigns subject ID.\n",
    "\n",
    "    :param root: The root path, or prioritised sequence of paths, where epoch data is stored.\n",
    "    :param subj_id: The subject ID string to be assigned.\n",
    "    :param start: The left bound of the time range to extract.\n",
    "    :param end: The right bound of the time range to extract.\n",
    "    :returns: A pandas data frame containing pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "\n",
    "    subj_video = aeon.load(root, source_frame, start=start, end=end)\n",
    "    subj_pos = aeon.load(root, exp02.CameraTop.Position, start=start, end=end)\n",
    "    subj_data = pd.merge_asof(\n",
    "        subj_video,\n",
    "        subj_pos,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        direction=\"nearest\",\n",
    "        tolerance=pd.Timedelta(\"1ms\"),\n",
    "    )[[\"x\", \"y\", \"id\", \"area\", \"_frame\", \"_path\"]]\n",
    "    subj_data.dropna(inplace=True)\n",
    "    subj_data[\"id\"] = subj_id\n",
    "    return subj_data\n",
    "\n",
    "\n",
    "def sample_n_from_bins(subj_data: \"pd.DataFrame\", n_samples: int = 1, n_bins: int = 50, range: \"npt.ArrayLike\" =[[0, 1440], [0, 1080]]):\n",
    "    \"\"\"\n",
    "    Uniformly samples n number of data from x number of bins.\n",
    "\n",
    "    :param subj_data: A pandas data frame containing pos tracking and video data, and subject ID.\n",
    "    :param optional n_samples: The number of samples to take from each bin.\n",
    "    :param optional n_bins: The number of bins to use for sampling.\n",
    "    :param optional range: The leftmost and rightmost edges of the bins along each dimension (if not specified explicitly in the bins parameters): [[xmin, xmax], [ymin, ymax]]. All values outside of this range will be considered outliers and not tallied in the histogram.\n",
    "    :returns: A pandas data frame containing uniformly-sampled pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "\n",
    "    hist_data = stats.binned_statistic_2d(\n",
    "        subj_data.x,\n",
    "        subj_data.y,\n",
    "        values=subj_data,\n",
    "        statistic=\"count\",\n",
    "        bins=n_bins,\n",
    "        range=range,\n",
    "    )\n",
    "    subj_data[\"bin\"] = hist_data.binnumber\n",
    "    sampled_data = (\n",
    "        subj_data.groupby([\"bin\"]).sample(n=n_samples, replace=True).drop_duplicates()\n",
    "    )\n",
    "    return sampled_data\n",
    "\n",
    "\n",
    "def create_session_dataset(\n",
    "    session: dict,\n",
    "    subj_ids: list = None,\n",
    "    plot_dist: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dataset for a given session dict\n",
    "\n",
    "    :param session: A dictionary containing the root path, subject IDs, and their start and end times.\n",
    "    :param optional subj_ids: A list of subject ids to extract. If None, all subjects are extracted.\n",
    "    :param optional plot_dist: Whether to plot the 1d and 2d histograms of x, y pos tracking for each subject.\n",
    "    :returns: A pandas data frame containing uniformly-sampled pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "    all_subj_data = pd.DataFrame()\n",
    "    if not subj_ids:\n",
    "        subj_ids = session[\"subjects\"].keys()\n",
    "    for subj in subj_ids:\n",
    "        subj_dict = {\n",
    "            \"id\": subj,\n",
    "            \"root\": session[\"root\"],\n",
    "            \"start\": session[\"subjects\"][subj][\"start\"],\n",
    "            \"end\": session[\"subjects\"][subj][\"end\"],\n",
    "        }\n",
    "        subj_data = (\n",
    "            create_subject_dataset(\n",
    "                subj_dict,\n",
    "                min_area=500,\n",
    "                n_samples=4,\n",
    "                n_bins=10,\n",
    "            )  # sample fewer points for manual annotation\n",
    "            if \"multi_\" in subj\n",
    "            else create_subject_dataset(\n",
    "                subj_dict,\n",
    "            )\n",
    "        )\n",
    "        all_subj_data = pd.concat([all_subj_data, subj_data])\n",
    "    if plot_dist:\n",
    "        fig = plot_position_histograms(all_subj_data)\n",
    "        fig.show()\n",
    "    return all_subj_data\n",
    "\n",
    "\n",
    "def plot_position_histograms(data: \"pd.DataFrame\", n_bins: int = 50):\n",
    "    \"\"\"\n",
    "    Plots the 1d and 2d histograms of x, y pos tracking for each subject in a given data frame.\n",
    "\n",
    "    :param data: A pandas data frame containing x, y pos tracking and subject ID(s).\n",
    "    :returns: A plot containing 1d and 2d histograms of x, y pos tracking for each subject.\n",
    "    \"\"\"\n",
    "    subj_ids = data[\"id\"].unique()\n",
    "    fig, ax = plt.subplots(2, len(subj_ids))\n",
    "    n_bins = 50\n",
    "    if len(subj_ids) == 1:\n",
    "        data[[\"x\", \"y\"]].plot.hist(bins=n_bins, alpha=0.5, ax=ax[0], title=subj_ids[0])\n",
    "        ax[1].hist2d(\n",
    "            data.x,\n",
    "            data.y,\n",
    "            bins=(n_bins, n_bins),\n",
    "            cmap=plt.cm.jet,\n",
    "        )\n",
    "    else:\n",
    "        for i, subj_id in enumerate(subj_ids):\n",
    "            subj_data = data[data[\"id\"] == subj_id]\n",
    "            subj_data[[\"x\", \"y\"]].plot.hist(\n",
    "                bins=n_bins, alpha=0.5, ax=ax[0, i], title=subj_id\n",
    "            )\n",
    "            ax[1, i].hist2d(\n",
    "                subj_data.x, subj_data.y, bins=(n_bins, n_bins), cmap=plt.cm.jet\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_subject_dataset(\n",
    "    subject: dict,\n",
    "    min_area: float = None,\n",
    "    n_samples: int = 1,\n",
    "    n_bins: int = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dataset for a given subject dict\n",
    "\n",
    "    :param subject: A dictionary containing the root path, subject ID, and their start and end times.\n",
    "    :param optional min_area: The minimum area of the subject to be included in the dataset.\n",
    "    :param optional n_samples: The number of samples to take from each bin.\n",
    "    :param optional n_bins: The number of bins to use for sampling.\n",
    "    :returns: A pandas data frame containing uniformly-sampled pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "    subj_data = get_raw_tracking_data(\n",
    "        subject[\"root\"],\n",
    "        subject[\"id\"],\n",
    "        subject[\"start\"],\n",
    "        subject[\"end\"],\n",
    "    )\n",
    "    if min_area:\n",
    "        subj_data = subj_data[subj_data.area >= min_area] # when animals fuse\n",
    "    subj_data = sample_n_from_bins(subj_data, n_samples=n_samples, n_bins=n_bins)\n",
    "    return subj_data\n",
    "\n",
    "\n",
    "def extract_ma_videos(session: dict, dest_root: str = \"\", subj_ids: list = None):\n",
    "    \"\"\"\n",
    "    Extracts multi-animal videos for a given session.\n",
    "\n",
    "    :param session: A session dictionary.\n",
    "    :param dest_root: A string containing the destination root path. If empty, the current working directory is used.\n",
    "    :param optional subj_ids: A list of subject ids to extract. If None, all subjects are extracted.\n",
    "    :returns list: A list of extracted video paths.\n",
    "    \"\"\"\n",
    "\n",
    "    paths = []\n",
    "    if not subj_ids:\n",
    "        subj_ids = session[\"subjects\"].keys()\n",
    "    for subj in subj_ids:\n",
    "        if \"multi_\" in subj:\n",
    "            subj_data = get_raw_tracking_data(\n",
    "                session[\"root\"],\n",
    "                \"multi_animal\",\n",
    "                session[\"subjects\"][subj][\"start\"],\n",
    "                session[\"subjects\"][subj][\"end\"],\n",
    "                source_frame=exp02.CameraTop.Video,\n",
    "            )\n",
    "            filepath = f'{dest_root}{Path(subj_data.iloc[0][\"_path\"]).stem.split(\"-00-00\")[0]}-{session[\"subjects\"][subj][\"end\"].hour}-00_pattern_tattoo.avi'\n",
    "            paths.append(filepath)\n",
    "            export(frames(subj_data), filepath , fps=50)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def create_ma_dataset(session: dict, subj_ids: list = None):\n",
    "    \"\"\"\n",
    "    Creates a multi-animal-frames-only dataset for a given session dict.\n",
    "\n",
    "    :param session: A session dictionary.\n",
    "    :param optional subj_ids: A list of subject ids to extract. If None, all subjects with prefix \"multi_\" are extracted.\n",
    "    :returns list: A pandas data frame containing multi-animal pos tracking and video data, and subject ID.\n",
    "    \"\"\"\n",
    "    all_subj_data = pd.DataFrame()\n",
    "    if not subj_ids:\n",
    "        subj_ids = session[\"subjects\"].keys()\n",
    "    for subj in subj_ids:\n",
    "        if \"multi_\" in subj:\n",
    "            subj_data = get_raw_tracking_data(\n",
    "                session[\"root\"],\n",
    "                subj,\n",
    "                session[\"subjects\"][subj][\"start\"],\n",
    "                session[\"subjects\"][subj][\"end\"],\n",
    "                source_frame=exp02.CameraTop.Video,\n",
    "            )\n",
    "            all_subj_data = pd.concat([all_subj_data, subj_data])\n",
    "            \n",
    "    return all_subj_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries for each session\n",
    "aeon2 = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON2/experiment0.2/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1101818\": {\n",
    "            \"start\": pd.Timestamp(\"2022-06-23 08:39:04.261089801\"),\n",
    "            \"end\": pd.Timestamp(\"2022-06-23 11:14:46.121759892\"),\n",
    "        },\n",
    "        \"BAA-1101819\": {\n",
    "            \"start\": pd.Timestamp(\"2022-06-21 13:28:10.593659878\"),\n",
    "            \"end\": pd.Timestamp(\"2022-06-21 16:34:29.241280079\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"BAA-1101818_819\",\n",
    "}\n",
    "\n",
    "aeon3 = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/presocial0.1\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3_NTP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-03 16:40:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-03 16:55:00\"),\n",
    "        },\n",
    "        \"AEON3_TP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-03 17:01:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-03 17:22:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-03 17:23:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-03 17:43:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"AEON3_NTP_TP_local\",\n",
    "}\n",
    "\n",
    "aeon3b = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/presocial0.1\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-16 15:05:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-16 15:44:00\"),\n",
    "        },\n",
    "        \"AEON3B_TP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-16 16:00:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-16 16:36:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-16 16:37:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-16 17:19:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"AEON3B_NTP_TP_local\",\n",
    "}\n",
    "\n",
    "aeon3b_pattern = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/presocial0.1/\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 09:40:40\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 10:07:00\"),\n",
    "        },\n",
    "        \"AEON3B_TP1\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 10:10:44\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 10:46:30\"),\n",
    "        },\n",
    "        \"AEON3B_TP2\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 10:50:10\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 11:22:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 11:23:30\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 12:31:00\"),\n",
    "        }\n",
    "    },\n",
    "    \"session\": \"AEON3B_pattern_local\",\n",
    "}\n",
    "\n",
    "aeon3b_pattern_nest = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/presocial0.1/\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 09:40:40\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 10:07:00\"),\n",
    "        },\n",
    "        \"AEON3B_TP1\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 10:10:44\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 10:46:30\"),\n",
    "        },\n",
    "        \"AEON3B_TP2\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 10:50:10\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 11:22:00\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-03-29 11:23:30\"),\n",
    "            \"end\": pd.Timestamp(\"2023-03-29 12:31:00\"),\n",
    "        }\n",
    "    },\n",
    "    \"nest_coords\": [\n",
    "        (1223,475),\n",
    "        (1223,588),\n",
    "        (1352,479),\n",
    "        (1352,581),\n",
    "    ],\n",
    "    \"session\": \"AEON3B_pattern_local_nest\",\n",
    "}\n",
    "\n",
    "aeon3b_pattern_tattoo = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/presocial0.1/\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": { # BAA-1103352\n",
    "            \"start\": pd.Timestamp(\"2023-06-16 16:13:30\"),\n",
    "            \"end\": pd.Timestamp(\"2023-06-16 17:00\"),\n",
    "        },\n",
    "        \"AEON3B_TP1\": { # BAA-1103353\n",
    "            \"start\": pd.Timestamp(\"2023-06-16 17:04\"),\n",
    "            \"end\": pd.Timestamp(\"2023-06-16 18:04\"),\n",
    "        },\n",
    "        \"AEON3B_TP2\": { # BAA-1103351\n",
    "            \"start\": pd.Timestamp(\"2023-06-16 18:08:19\"),\n",
    "            \"end\": pd.Timestamp(\"2023-06-16 18:38:52\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-06-16 18:41:30\"),\n",
    "            \"end\": pd.Timestamp(\"2023-06-16 19:12:00\"),\n",
    "        }\n",
    "    },\n",
    "    \"session\": \"AEON3B_pattern_tattoo\",\n",
    "}\n",
    "\n",
    "aeon3b_pattern_tattoo2 = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/multianimal-test/\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": { # BAA-1103352\n",
    "            \"start\": pd.Timestamp(\"2023-07-04 16:28:10\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-04 17:01:57\"),\n",
    "        },\n",
    "        \"AEON3B_TP1\": { # CAA-1120139\n",
    "            \"start\": pd.Timestamp(\"2023-07-04 17:06:22\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-04 17:34:52\"),\n",
    "        },\n",
    "        \"AEON3B_TP2\": { # BAA-1103351\n",
    "            \"start\": pd.Timestamp(\"2023-07-04 15:36:19\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-04 16:16:19\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-07-04 17:37:08\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-04 18:10:13\"),\n",
    "        }\n",
    "    },\n",
    "    \"session\": \"AEON3B_pattern_tattoo2\",\n",
    "}\n",
    "\n",
    "aeon3b_pattern_tattoo3 = {\n",
    "    \"root\": \"Z:/aeon/data/raw/AEON3/multianimal-test/\",\n",
    "    \"subjects\": {\n",
    "        \"AEON3B_NTP\": { # BAA-1103352\n",
    "            \"start\": pd.Timestamp(\"2023-07-04 16:28:10\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-04 17:01:57\"),\n",
    "        },\n",
    "        \"AEON3B_TP1\": { # BAA-1103369\n",
    "            \"start\": pd.Timestamp(\"2023-07-28 14:24:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-28 15:15:00\"),\n",
    "        },\n",
    "        \"AEON3B_TP2\": { # BAA-1103351\n",
    "            \"start\": pd.Timestamp(\"2023-07-04 15:36:19\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-04 16:16:19\"),\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2023-07-28 15:21:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-07-28 16:18:00\"),\n",
    "        },\n",
    "        \"multi_animal2\": {\n",
    "            \"start\": pd.Timestamp(\"2023-08-01 10:19:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-08-01 11:07:00\"),\n",
    "        },\n",
    "        \"multi_animal3\": {\n",
    "            \"start\": pd.Timestamp(\"2023-08-03 10:40:00\"),\n",
    "            \"end\": pd.Timestamp(\"2023-08-03 11:28:00\"),\n",
    "        },\n",
    "        \"multi_animal4\": {\n",
    "            \"start\": pd.Timestamp(\"2023-08-08 14:14:10.119999886\"),\n",
    "            \"end\": pd.Timestamp(\"2023-08-08 15:01:32.288000107\"),\n",
    "        }, \n",
    "        \"multi_animal5\": {\n",
    "            \"start\": pd.Timestamp(\"2023-08-11 11:54:01.340000153\"),\n",
    "            \"end\": pd.Timestamp(\"2023-08-11 13:20:36.303999901\"),\n",
    "        }, \n",
    "    },\n",
    "    \"session\": \"AEON3B_pattern_tattoo3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract start end times for MA session\n",
    "# get subject enter, exit times\n",
    "subject_visits = visits(aeon.load(aeon3b_pattern_tattoo3[\"root\"], exp02.ExperimentalMetadata.SubjectState, start=pd.Timestamp(\"2023-08-11 00:00:00\"), end=pd.Timestamp(\"2023-08-12 00:00:00\")))\n",
    "# get maintenance, experiment times\n",
    "env_states = aeon.load(aeon3b_pattern_tattoo3[\"root\"], exp02.ExperimentalMetadata.EnvironmentState, start=pd.Timestamp(\"2023-08-11 00:00:00\"), end=pd.Timestamp(\"2023-08-12 00:00:00\"))\n",
    "env_states = env_states[~env_states.index.duplicated(keep=\"first\")]\n",
    "# get env_states timestamps after largest enter and before smallest exit\n",
    "maintenance_states = env_states[env_states[\"state\"] == \"Maintenance\"]\n",
    "experiment_states = env_states[env_states[\"state\"] == \"Experiment\"]\n",
    "start_time = pd.Timestamp(experiment_states.index[experiment_states.index.get_indexer([subject_visits[\"enter\"].max()], method='bfill')][0])\n",
    "end_time = pd.Timestamp(maintenance_states.index[maintenance_states.index.get_indexer([subject_visits[\"exit\"].min()], method='ffill')][0])\n",
    "print(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract single + multi-animal frames as csv\n",
    "all_subj_data = create_session_dataset(aeon3b_pattern_tattoo3)\n",
    "all_subj_data.to_csv(f'{aeon3b_pattern_tattoo3[\"session\"]}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract multi-animal (only) frames as csv\n",
    "ma_subj_data = create_ma_dataset(aeon3b_pattern_tattoo3)\n",
    "ma_subj_data.to_csv(f'{aeon3b_pattern_tattoo3[\"session\"]}_ma.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import sleap\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sleap.io.dataset import Labels\n",
    "from sleap.io.video import Video\n",
    "from sleap.io.pathutils import fix_path_separator\n",
    "from sleap.gui.suggestions import VideoFrameSuggestions\n",
    "from sleap.nn.config import *\n",
    "from sleap.nn.inference import main as sleap_track\n",
    "from sleap.nn.inference import TopDownMultiClassPredictor, Predictor, TopDownPredictor, Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_slp_dataset(session: dict, subj_data: \"pd.DataFrame\", skeleton: \"sleap.Skeleton\"):\n",
    "    \"\"\"\n",
    "    Generates .slp dataset for a given session dict.\n",
    "\n",
    "    :param session: A dictionary containing the root path, subject IDs, and their start and end times.\n",
    "    :param subj_data: A pandas DataFrame containing the labeled data for a given session.\n",
    "    :param skeleton: A sleap Skeleton object.\n",
    "    :returns: A sleap Labels object containing labeled frames.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create tracks dictionary from subj_ids that are not multi_animal\n",
    "    tracks_dict = {\n",
    "        subj: sleap.Track(spawned_on=0, name=subj)\n",
    "        for subj in session[\"subjects\"].keys()\n",
    "        if \"multi_\" not in subj\n",
    "    }\n",
    "\n",
    "    lfs = []\n",
    "\n",
    "    # create video dictionary from new labels\n",
    "    videos_dict = {\n",
    "        video: sleap.Video.from_filename(video)\n",
    "        for video in subj_data._path.unique()\n",
    "    }\n",
    "\n",
    "    for _, row in subj_data.drop_duplicates(subset=[\"_path\", \"_frame\"]).iterrows():\n",
    "        instances = []\n",
    "        if \"multi_\" in row.id:\n",
    "            # duplicate instance for each track\n",
    "            for track in tracks_dict.keys():\n",
    "                instances.append(\n",
    "                    sleap.Instance(\n",
    "                        skeleton=skeleton,\n",
    "                        track=tracks_dict[track],\n",
    "                        points={\"centroid\": sleap.instance.Point(row.x, row.y)},\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            # create a new instance for each row\n",
    "            instances.append(\n",
    "                sleap.Instance(\n",
    "                    skeleton=skeleton,\n",
    "                    track=tracks_dict[row.id],\n",
    "                    points={\"centroid\": sleap.instance.Point(row.x, row.y)},\n",
    "                )\n",
    "            )\n",
    "        # create a new labeled frame\n",
    "        lf = sleap.instance.LabeledFrame(\n",
    "            video=videos_dict[row._path],\n",
    "            frame_idx=row._frame,\n",
    "            instances=instances,\n",
    "        )\n",
    "        lfs.append(lf)\n",
    "\n",
    "    return sleap.Labels(labeled_frames=lfs)\n",
    "\n",
    "\n",
    "def update_slp_video_paths(labels: \"sleap.Labels\", old_path: str, new_path: str):\n",
    "    \"\"\"\n",
    "    Updates video paths in a SLEAP labels object (e.g., to move training from local to remote machine).\n",
    "    \n",
    "    :param labels: SLEAP Labels object.\n",
    "    :param old_path: Old path to video files.\n",
    "    :param new_path: New path to video files.\n",
    "    :returns: SLEAP Labels object with updated video paths.\n",
    "    \"\"\"\n",
    "\n",
    "    videos =  [sleap.Video.from_filename(fix_path_separator(vid.filename).replace(old_path, new_path)) for vid in labels.videos]\n",
    "\n",
    "    lfs = []\n",
    "    for lf in labels.labeled_frames:\n",
    "        lf = sleap.instance.LabeledFrame(\n",
    "                video=videos[labels.videos.index(lf.video)],\n",
    "                frame_idx=lf.frame_idx,\n",
    "                instances=lf.instances,\n",
    "            )\n",
    "        lfs.append(lf)\n",
    "    \n",
    "    return sleap.Labels(labeled_frames=lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new skeleton\n",
    "skeleton = sleap.Skeleton()\n",
    "skeleton.add_node(\"centroid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate slp training dataset for all subjects\n",
    "subj_data = pd.read_csv(f'{aeon3b_pattern_tattoo3[\"session\"]}.csv')\n",
    "labels = generate_slp_dataset(aeon3b_pattern_tattoo3, subj_data, skeleton)\n",
    "sleap.Labels.save_file(labels, f'{aeon3b_pattern_tattoo3[\"session\"]}.slp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate multi-animal \"user-labeled\" slp datasets for each session for prediction/evaluation\n",
    "subj_data = pd.read_csv(f'{aeon3b_pattern_tattoo3[\"session\"]}_ma.csv')\n",
    "subj_ids = [subj for subj in aeon3b_pattern_tattoo3[\"subjects\"].keys() if \"multi_\" in subj]\n",
    "for subj_id in subj_ids:\n",
    "    labels = generate_slp_dataset(aeon3b_pattern_tattoo3, subj_data[subj_data[\"id\"]==subj_id], skeleton)\n",
    "    sleap.Labels.save_file(labels, f'{aeon3b_pattern_tattoo3[\"session\"]}_{subj_id}.slp')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial parameters\n",
    "subj_id = f'{aeon3b_pattern_tattoo3[\"session\"]}'\n",
    "run_name_centroid = f'{subj_id}_topdown_top.centroid'\n",
    "run_name_centered_instance = f'{subj_id}_topdown_top.centered_instance_multiclass'\n",
    "root = \"/ceph/aeon/aeon/code/scratchpad/sleap/tail_pattern/\"\n",
    "runs_folder = root + \"models/\"\n",
    "predictions_folder = root + \"predictions/\"\n",
    "groundtruth_folder = root + \"groundtruth/\"\n",
    "\n",
    "try:\n",
    "    skeleton\n",
    "except NameError:\n",
    "    # create new skeleton\n",
    "    skeleton = sleap.Skeleton()\n",
    "    skeleton.add_node(\"centroid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update local video paths to ceph\n",
    "new_labels = update_slp_video_paths(\n",
    "    labels=sleap.load_file(f'{subj_id}.slp'), \n",
    "    old_path=\"Z:\", \n",
    "    new_path=\"/ceph/aeon\")\n",
    "sleap.Labels.save_file(new_labels,  f'{root}{subj_id}.slp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels into train/val/test\n",
    "labels = sleap.load_file(f'{root}{subj_id}.slp')\n",
    "\n",
    "# generate a 0.8/0.1/0.1 train/val/test split\n",
    "labels_train, labels_val_test = labels.split(n=0.8) \n",
    "labels_val, labels_test = labels_val_test.split(n=0.5)\n",
    "\n",
    "# Save with images\n",
    "labels_train.save(f'{root}{subj_id}.train.pkg.slp')#, with_images=True)\n",
    "labels_val.save(f'{root}{subj_id}.val.pkg.slp')#, with_images=True)\n",
    "labels_test.save(f'{root}{subj_id}.test.pkg.slp')#, with_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid model\n",
    "# initalise default training job config\n",
    "cfg = TrainingJobConfig()\n",
    "cfg.data.labels.training_labels = f'{root}{subj_id}.train.pkg.slp'\n",
    "cfg.data.labels.validation_labels = f'{root}{subj_id}.val.pkg.slp'\n",
    "cfg.data.labels.test_labels = f'{root}{subj_id}.test.pkg.slp'\n",
    "\n",
    "# preprocessing and training params\n",
    "cfg.data.preprocessing.input_scaling = 0.75 #0.5\n",
    "cfg.data.instance_cropping.center_on_part = \"centroid\"\n",
    "cfg.data.instance_cropping.crop_size = 128 # set crop size manually\n",
    "cfg.optimization.augmentation_config.rotate = True\n",
    "cfg.optimization.epochs = 600 #200\n",
    "cfg.optimization.batch_size = 4\n",
    "\n",
    "cfg.optimization.initial_learning_rate = 0.0001\n",
    "cfg.optimization.learning_rate_schedule.reduce_on_plateau = True\n",
    "cfg.optimization.learning_rate_schedule.reduction_factor = 0.5\n",
    "cfg.optimization.learning_rate_schedule.plateau_min_delta = 1e-06 \n",
    "cfg.optimization.learning_rate_schedule.plateau_patience = 20 #5\n",
    "cfg.optimization.learning_rate_schedule.plateau_cooldown = 3\n",
    "cfg.optimization.learning_rate_schedule.min_learning_rate = 1e-08\n",
    "\n",
    "cfg.optimization.early_stopping.stop_training_on_plateau = True\n",
    "cfg.optimization.early_stopping.plateau_min_delta = 1e-08\n",
    "cfg.optimization.early_stopping.plateau_patience = 30 #20\n",
    "\n",
    "# configure nn and model\n",
    "cfg.model.backbone.unet = UNetConfig(\n",
    "    max_stride=16,\n",
    "    filters=16,\n",
    "    filters_rate=2.00,\n",
    "    output_stride=2,\n",
    "    #up_interpolate=True, # save computations but may lower accuracy\n",
    ")\n",
    "cfg.model.heads.centroid = CentroidsHeadConfig(\n",
    "    anchor_part=\"centroid\",\n",
    "    sigma=2.5,\n",
    "    output_stride=2\n",
    ")\n",
    "\n",
    "# configure outputs\n",
    "cfg.outputs.run_name = run_name_centroid\n",
    "cfg.outputs.save_outputs = True\n",
    "cfg.outputs.runs_folder = runs_folder\n",
    "cfg.outputs.save_visualizations = True\n",
    "cfg.outputs.checkpointing.initial_model = True\n",
    "cfg.outputs.checkpointing.best_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "trainer.setup()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part detection model: centered instance + multi-class\n",
    "# initalise default training job config\n",
    "cfg = TrainingJobConfig()\n",
    "\n",
    "# update path to 0.8/0.1/0.1 train/val/test split\n",
    "cfg.data.labels.training_labels = f'{root}{subj_id}.train.pkg.slp'\n",
    "cfg.data.labels.validation_labels = f'{root}{subj_id}.val.pkg.slp'\n",
    "cfg.data.labels.test_labels = f'{root}{subj_id}.test.pkg.slp'\n",
    "cfg.data.labels.skeletons = [skeleton] # load skeleton\n",
    "\n",
    "# preprocessing and training params\n",
    "cfg.data.preprocessing.input_scaling = 1.0\n",
    "cfg.data.instance_cropping.center_on_part = \"centroid\"\n",
    "cfg.data.instance_cropping.crop_size = 128 # set crop size manually\n",
    "cfg.optimization.augmentation_config.rotate = True\n",
    "cfg.optimization.epochs = 600\n",
    "cfg.optimization.batch_size = 8 # 4\n",
    "\n",
    "cfg.optimization.initial_learning_rate = 0.0001\n",
    "cfg.optimization.learning_rate_schedule.reduce_on_plateau = True\n",
    "cfg.optimization.learning_rate_schedule.reduction_factor = 0.1 #0.5\n",
    "cfg.optimization.learning_rate_schedule.plateau_min_delta = 1e-08 #1e-06 \n",
    "cfg.optimization.learning_rate_schedule.plateau_patience = 20 #5\n",
    "cfg.optimization.learning_rate_schedule.plateau_cooldown = 3\n",
    "cfg.optimization.learning_rate_schedule.min_learning_rate = 1e-08\n",
    "\n",
    "cfg.optimization.early_stopping.stop_training_on_plateau = True\n",
    "cfg.optimization.early_stopping.plateau_min_delta = 1e-08\n",
    "cfg.optimization.early_stopping.plateau_patience = 30 #20\n",
    "\n",
    "# configure nn and model\n",
    "cfg.model.backbone.unet = UNetConfig(\n",
    "    max_stride=16, #32,\n",
    "    output_stride=2, #4,\n",
    "    filters=16, #24,\n",
    "    filters_rate=1.5,\n",
    "    #up_interpolate=True, # save computations but may lower accuracy\n",
    ")\n",
    "confmaps=CenteredInstanceConfmapsHeadConfig(\n",
    "    anchor_part=\"centroid\",\n",
    "    sigma=1.5, #2.5, \n",
    "    output_stride=2, #4, \n",
    "    loss_weight=1.0, \n",
    ") \n",
    "# load labels.slp to get track names\n",
    "labels = sleap.load_file(f'{root}{subj_id}.slp')\n",
    "class_vectors=ClassVectorsHeadConfig(\n",
    "    classes = [track.name for track in labels.tracks],\n",
    "    output_stride=2, #16, #4,\n",
    "    num_fc_layers=3,\n",
    "    num_fc_units=256,\n",
    "    global_pool=True,\n",
    "    loss_weight=0.01 # TODO: try 1.0\n",
    ")\n",
    "cfg.model.heads.multi_class_topdown = MultiClassTopDownConfig(\n",
    "    confmaps=confmaps,\n",
    "    class_vectors=class_vectors\n",
    ")\n",
    "\n",
    "# configure outputs\n",
    "cfg.outputs.run_name = run_name_centered_instance\n",
    "cfg.outputs.save_outputs = True\n",
    "cfg.outputs.runs_folder = runs_folder\n",
    "cfg.outputs.save_visualizations = True\n",
    "cfg.outputs.checkpointing.initial_model = True\n",
    "cfg.outputs.checkpointing.best_model = True\n",
    "\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "\n",
    "trainer.setup()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume training\n",
    "# Load config.\n",
    "model_path = \"models/AEON3_NTP_TP_local_topdown_top.centroid/\"\n",
    "cfg = sleap.load_config(model_path)\n",
    "\n",
    "# Create and initialize the trainer.\n",
    "trainer = sleap.nn.training.Trainer.from_config(cfg)\n",
    "trainer.setup()\n",
    "\n",
    "# Replace the randomly initialized weights with the saved weights.\n",
    "trainer.keras_model.load_weights(f'{model_path}best_model.h5')\n",
    "\n",
    "trainer.config.optimization.epochs = 200\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.keras_model.outputs[0].shape) # confmaps  \n",
    "print(trainer.keras_model.outputs[1].shape) # id part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial parameters\n",
    "subj_id = f'{aeon3b_pattern_tattoo3[\"session\"]}'\n",
    "run_name_centroid = f'{subj_id}_topdown_top.centroid'\n",
    "run_name_centered_instance = f'{subj_id}_topdown_top.centered_instance_multiclass'\n",
    "root = \"/ceph/aeon/aeon/code/scratchpad/sleap/tail_pattern/\"\n",
    "runs_folder = root + \"models/\"\n",
    "predictions_folder = root + \"predictions/\"\n",
    "groundtruth_folder = root + \"groundtruth/\"\n",
    "\n",
    "print(run_name_centroid, run_name_centered_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a single multi-animal video\n",
    "subj_idx = 0\n",
    "video_idx = 0\n",
    "\n",
    "multi_subj_ids = [subj_id for subj_id in aeon3b_pattern_tattoo3[\"subjects\"].keys() if \"multi_\" in subj_id]\n",
    "\n",
    "# select one multi-animal session only\n",
    "input_file = f'{root}{aeon3b_pattern_tattoo3[\"session\"]}_{multi_subj_ids[subj_idx]}.slp'\n",
    "\n",
    "# infer on user-labeled frames on the first video only\n",
    "output_file_pr = f'{predictions_folder}{subj_id}_{multi_subj_ids[subj_idx]}_pr.slp'\n",
    "sleap_track(\n",
    "    [\n",
    "        input_file,\n",
    "        \"--model\",\tf'{runs_folder}{run_name_centroid}',\n",
    "        \"--model\",\tf'{runs_folder}{run_name_centered_instance}',\n",
    "        \"--only-labeled-frames\",\n",
    "        \"--video.index\", str(video_idx),\n",
    "        \"--output\", output_file_pr,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 100 suggestions based on low-scoring frames to be proofread\n",
    "labels_pr = sleap.load_file(output_file_pr)\n",
    "output_file_low_gt = f'{groundtruth_folder}{subj_id}_1_{multi_subj_ids[subj_idx]}_low_gt.slp'\n",
    "labels_missing = sleap.Labels([label for label in labels_pr.labels if label.n_predicted_instances < 3])\n",
    "suggestions = VideoFrameSuggestions.suggest(\n",
    "    labels=labels_missing,\n",
    "    params=dict(\n",
    "        videos=[labels_missing.videos[video_idx]],\n",
    "        method=\"prediction_score\",\n",
    "        score_limit=0.5,\n",
    "        instance_limit_lower=1,\n",
    "        instance_limit_upper=3,\n",
    "    ),\n",
    ")\n",
    "\n",
    "if len(suggestions) > 100:\n",
    "    suggestions = random.sample(suggestions, 100)\n",
    "\n",
    "lfs = []\n",
    "for suggestion in suggestions:\n",
    "    matching_frames = labels_missing.find(video=labels_missing.videos[video_idx], frame_idx=suggestion.frame_idx)\n",
    "    if matching_frames:\n",
    "        lf = matching_frames[0]\n",
    "        instances = []\n",
    "        for instance in lf.instances_to_show:\n",
    "            instances.append(\n",
    "                sleap.Instance(\n",
    "                    skeleton=instance.skeleton,\n",
    "                    track=instance.track,\n",
    "                    points={\"centroid\": sleap.instance.Point(instance.points[0].x, instance.points[0].y)}\n",
    "\n",
    "                )\n",
    "            )\n",
    "        lfs.append(sleap.instance.LabeledFrame(\n",
    "            video=lf.video,\n",
    "            frame_idx=lf.frame_idx,\n",
    "            instances=instances,\n",
    "        ))\n",
    "sleap.Labels.save_file(sleap.Labels(labeled_frames=lfs), output_file_low_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 100 random consecutive frames as ground truth data for evaluation\n",
    "output_file_rand_gt = f'{groundtruth_folder}{subj_id}_{multi_subj_ids[subj_idx]}_rand_gt.slp'\n",
    "random_frame_from = True\n",
    "consecutive = True\n",
    "num_frames = 100 # 3000 = 1min\n",
    "\n",
    "if consecutive:\n",
    "        frame_from = random.randint(0, len(labels_pr.labeled_frames) - num_frames) if random_frame_from else 0\n",
    "        selected_frame_idx = list(range(frame_from, frame_from + num_frames))\n",
    "else: \n",
    "    selected_frame_idx = random.sample(range(len(labels_pr.labeled_frames)), num_frames)\n",
    "\n",
    "sfs = []\n",
    "for idx in selected_frame_idx:\n",
    "    sf = labels_pr.labeled_frames[idx]\n",
    "    instances = []\n",
    "    for instance in sf.instances_to_show:\n",
    "        instances.append(\n",
    "            sleap.Instance(\n",
    "                skeleton=instance.skeleton,\n",
    "                track=instance.track,\n",
    "                points={\"centroid\": sleap.instance.Point(instance.points[0].x, instance.points[0].y)}\n",
    "\n",
    "            )\n",
    "        )\n",
    "    sfs.append(sleap.instance.LabeledFrame(\n",
    "        video=sf.video,\n",
    "        frame_idx=sf.frame_idx,\n",
    "        instances=instances,\n",
    "    ))\n",
    "\n",
    "sleap.Labels.save_file(sleap.Labels(labeled_frames=sfs), output_file_rand_gt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test/gt data\n",
    "subj_idx = 0\n",
    "multi_subj_ids = [subj_id for subj_id in aeon3b_pattern_tattoo3[\"subjects\"].keys() if \"multi_\" in subj_id]\n",
    "\n",
    "test_data = f'{root}{subj_id}.test.pkg.slp'\n",
    "ground_truth_data = f'{groundtruth_folder}{subj_id}_{multi_subj_ids[subj_idx]}_rand_gt.slp'\n",
    "\n",
    "# load model\n",
    "predictor = TopDownMultiClassPredictor.from_trained_models(\n",
    "    centroid_model_path=f'{runs_folder}{run_name_centroid}',\n",
    "    confmap_model_path=f'{runs_folder}{run_name_centered_instance}',\n",
    ")\n",
    "\n",
    "# load ground truth data \n",
    "labels_gt = sleap.load_file(ground_truth_data)\n",
    "labels_pr = predictor.predict(labels_gt)\n",
    "metrics = sleap.nn.evals.evaluate(labels_gt, labels_pr, oks_scale=128)\n",
    "\n",
    "framepairs = sleap.nn.evals.find_frame_pairs(labels_gt, labels_pr)\n",
    "matches = sleap.nn.evals.match_frame_pairs(framepairs, scale=128)\n",
    "positive_pairs = matches[0]\n",
    "false_negatives = matches[1]\n",
    "# for each labeled frame in labels_gt, get each instance's points\n",
    "correct_id = []\n",
    "for positive_pair in positive_pairs:\n",
    "    correct_id.append(positive_pair[0].track.name == positive_pair[1].track.name)\n",
    "\n",
    "# compute occupancy matrix\n",
    "occupancy_matrix = np.zeros(\n",
    "    (len(labels_gt.tracks), len(labels_gt.labeled_frames)), dtype=np.uint8\n",
    ")\n",
    "\n",
    "for i, lf in enumerate(labels_pr):\n",
    "    frame_i = i\n",
    "    for inst in lf.instances_to_show:\n",
    "        # Assumes either all instances have tracks or no instances have tracks\n",
    "        if inst.track is None:\n",
    "            track_i = 0\n",
    "        else:\n",
    "            track_i = labels_pr.tracks.index(inst.track)\n",
    "\n",
    "        occupancy_matrix[track_i, frame_i] = 1\n",
    "\n",
    "n_frames_missing_tracks = len(\n",
    "    np.where(np.sum(occupancy_matrix, axis=0) < len(labels_pr.tracks))[0]\n",
    ")\n",
    "print(f'{ground_truth_data} predicted with:')\n",
    "print(f'{run_name_centroid}') \n",
    "print(f'{run_name_centered_instance}')\n",
    "print(\n",
    "    \"Frames with <\",\n",
    "    len(labels_gt.tracks),\n",
    "    \"tracks:\",\n",
    "    n_frames_missing_tracks / len(labels_gt.labeled_frames),\n",
    "    \"(\",\n",
    "    n_frames_missing_tracks,\n",
    "    \"/\",\n",
    "    len(labels_gt.labeled_frames),\n",
    "    \")\",\n",
    ")\n",
    "print(\"Track occupancy:\", np.mean(occupancy_matrix, axis=1))\n",
    "print(\"Tracks identified:\", len(correct_id))\n",
    "print(\"Tracks correctly identified:\", sum(correct_id))\n",
    "print(\"Total tracks:\", len(labels_gt.all_instances))\n",
    "print(\"ID accuracy:\", sum(correct_id) / len(correct_id))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model\n",
    "predictor = TopDownMultiClassPredictor.from_trained_models(\n",
    "    centroid_model_path=f'{runs_folder}{run_name_centroid}',\n",
    "    confmap_model_path=f'{runs_folder}{run_name_centered_instance}',\n",
    ")\n",
    "predictor.export_model(aeon3b_pattern_tattoo3[\"session\"] + \"_topdown_multiclass\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0e3000693f8b9aa662d7863f075a07b6f4295c6c6941a96ccabdea0fbe2a07d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
