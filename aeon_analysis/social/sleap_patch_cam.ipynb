{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AEON env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import aeon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from aeon.analysis.movies import gridframes\n",
    "from aeon.io.video import frames\n",
    "from aeon.schema.schemas import social02\n",
    "from dotmap import DotMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeon3_social_02 = {\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104045\": {  # tattooed\n",
    "            \"root\": \"/ceph/aeon/aeon/data/raw/AEON3/social0.2/\",\n",
    "            \"start\": pd.Timestamp(\"2024-01-31 11:28:45.543519974\"),\n",
    "            \"end\": pd.Timestamp(\"2024-02-03 16:28:29.139999866\"),\n",
    "            \"rfid\": 977200010377711,\n",
    "        },\n",
    "        \"BAA-1104047\": {  # no tattoo\n",
    "            \"root\": \"/ceph/aeon/aeon/data/raw/AEON3/social0.2/\",\n",
    "            \"start\": pd.Timestamp(\"2024-02-05 15:43:11.581535816\"),\n",
    "            \"end\": pd.Timestamp(\"2024-02-08 14:49:41.552000046\"),\n",
    "            \"rfid\": 977200010164158,\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"root\": \"/ceph/aeon/aeon/data/raw/AEON3/social0.2/\",\n",
    "            \"start\": pd.Timestamp(\"2024-02-09 16:27:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-02-15 17:03:00\"),\n",
    "        },\n",
    "    },\n",
    "    # patch 1 and 2 rfids are swapped in AEON3\n",
    "    \"patch_camera_order\": [2, 1, 3],\n",
    "    \"session\": \"aeon3_social_02\",\n",
    "}\n",
    "\n",
    "aeon4_social_02 = {\n",
    "    \"root\": \"/ceph/aeon/aeon/data/raw/AEON4/social0.2/\",\n",
    "    \"subjects\": {\n",
    "        \"BAA-1104048\": {  # tattooed\n",
    "            \"start\": pd.Timestamp(\"2024-01-31 10:23:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-02-03 16:34:00\"),\n",
    "            \"rfid\": 977200010379293,\n",
    "        },\n",
    "        \"BAA-1104049\": {  # no tattoo\n",
    "            \"start\": pd.Timestamp(\"2024-02-05 15:00:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-02-08 14:54:00\"),\n",
    "            \"rfid\": 977200010378675,\n",
    "        },\n",
    "        \"multi_animal\": {\n",
    "            \"start\": pd.Timestamp(\"2024-02-09 16:49:00\"),\n",
    "            \"end\": pd.Timestamp(\"2024-02-13 15:06:00\"),\n",
    "        },\n",
    "    },\n",
    "    \"session\": \"aeon4_social_02\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_times(\n",
    "    root: str | os.PathLike, start_time: pd.Timestamp, end_time: pd.Timestamp\n",
    ") -> DotMap:\n",
    "    \"\"\"\n",
    "    Retrieve experiment start and stop times from environment states\n",
    "    (i.e. times outside of maintenance mode) occurring within the\n",
    "    given start and end times.\n",
    "\n",
    "    Args:\n",
    "        root (str or os.PathLike): The root path where epoch data is stored.\n",
    "        start_time (pandas.Timestamp): Start time.\n",
    "        end_time (pandas.Timestamp): End time.\n",
    "\n",
    "    Returns:\n",
    "        DotMap: A DotMap object containing two keys: 'start' and 'stop',\n",
    "        corresponding to pairs of experiment start and stop times.\n",
    "\n",
    "    Notes:\n",
    "    This function uses the last 'Maintenance' event (if available, otherwise\n",
    "    `end_time`) as the last 'Experiment' stop time. If the first retrieved state\n",
    "    is 'Maintenance' (e.g. 'Experiment' mode entered before `start_time`),\n",
    "    `start_time` is used as the first 'Experiment' start time.\n",
    "    \"\"\"\n",
    "\n",
    "    experiment_times = DotMap()\n",
    "    env_states = aeon.load(\n",
    "        root,\n",
    "        social02.Environment.EnvironmentState,\n",
    "        # aeon.io.reader.Csv(\"Environment_EnvironmentState_*\", [\"state\"]),\n",
    "        start_time,\n",
    "        end_time,\n",
    "    )\n",
    "    if env_states.empty:\n",
    "        warnings.warn(\n",
    "            \"The environment state df is empty. \"\n",
    "            \"Using input `start_time` and `end_time` as experiment times.\"\n",
    "        )\n",
    "        experiment_times.start = [start_time]\n",
    "        experiment_times.stop = [end_time]\n",
    "        return experiment_times\n",
    "    if env_states[\"state\"].iloc[-1] != \"Maintenance\":\n",
    "        warnings.warn(\n",
    "            \"No 'Maintenance' event at the end of the search range. \"\n",
    "            \"Using input `end_time` as last experiment stop time.\"\n",
    "        )\n",
    "        # Pad with a \"Maintenance\" event at the end\n",
    "        env_states = pd.concat(\n",
    "            [\n",
    "                env_states,\n",
    "                pd.DataFrame(\n",
    "                    \"Maintenance\",\n",
    "                    index=[end_time],\n",
    "                    columns=env_states.columns,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    # Use the last \"Maintenance\" event as end time\n",
    "    end_time = (env_states[env_states.state == \"Maintenance\"]).index[-1]\n",
    "    env_states = env_states[~env_states.index.duplicated(keep=\"first\")]\n",
    "    # Retain only events between visit start and stop times\n",
    "    env_states = env_states.iloc[\n",
    "        env_states.index.get_indexer([start_time], method=\"bfill\")[\n",
    "            0\n",
    "        ] : env_states.index.get_indexer([end_time], method=\"ffill\")[0]\n",
    "        + 1\n",
    "    ]\n",
    "    # Retain only events where state changes (experiment-maintenance pairs)\n",
    "    env_states = env_states[env_states[\"state\"].ne(env_states[\"state\"].shift())]\n",
    "    if env_states[\"state\"].iloc[0] == \"Maintenance\":\n",
    "        warnings.warn(\n",
    "            \"No 'Experiment' event at the start of the search range. \"\n",
    "            \"Using input `end_time` as last experiment stop time.\"\n",
    "        )\n",
    "        # Pad with an \"Experiment\" event at the start\n",
    "        env_states = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(\n",
    "                    \"Experiment\",\n",
    "                    index=[start_time],\n",
    "                    columns=env_states.columns,\n",
    "                ),\n",
    "                env_states,\n",
    "            ]\n",
    "        )\n",
    "    experiment_times.start = env_states[\n",
    "        env_states[\"state\"] == \"Experiment\"\n",
    "    ].index.values\n",
    "    experiment_times.stop = env_states[\n",
    "        env_states[\"state\"] == \"Maintenance\"\n",
    "    ].index.values\n",
    "\n",
    "    return experiment_times\n",
    "\n",
    "\n",
    "def exclude_maintenance_data(\n",
    "    data: pd.DataFrame, experiment_times: DotMap\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Exclude rows not in experiment times (i.e., corresponding to maintenance times)\n",
    "    from the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The data to filter. Expected to have a DateTimeIndex.\n",
    "        experiment_times (DotMap): A DotMap object containing experiment start and stop times.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The filtered data.\n",
    "    \"\"\"\n",
    "    filtered_data = pd.concat(\n",
    "        [\n",
    "            data.loc[start:stop]\n",
    "            for start, stop in zip(experiment_times.start, experiment_times.stop)\n",
    "        ]\n",
    "    )\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def get_single_frame(\n",
    "    root: str | os.PathLike,\n",
    "    video_reader: aeon.io.reader.Video,\n",
    "    time: pd.Timestamp,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retrieve a single frame from the given root directory,\n",
    "    Video reader, and time.\n",
    "\n",
    "    Args:\n",
    "        root (str or os.PathLike): The root path where epoch data\n",
    "            is stored.\n",
    "        video_reader (aeon.io.reader.Video): The Video reader.\n",
    "        time (pd.Timestamp): The timestamp of the frame to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The raw frame.\n",
    "    \"\"\"\n",
    "    vdata = aeon.load(\n",
    "        root, video_reader, start=time, end=time + pd.Timedelta(seconds=1)\n",
    "    )\n",
    "    vframe = frames(vdata.iloc[:1])\n",
    "    return np.squeeze(list(vframe))\n",
    "\n",
    "\n",
    "def plot_overlay(\n",
    "    img: np.ndarray,\n",
    "    pose: pd.DataFrame = None,\n",
    "    roi_coords: \"list[list[tuple[int, int]]]\" = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Overlay any pose data and/or ROI points on a frame (image).\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): The raw frame.\n",
    "        pose (pandas.DataFrame): The pose data with \"id\" as a grouping column.\n",
    "        roi_coords (list[list[tuple[int, int]]]): A list of lists of tuples\n",
    "            representing the ROI x, y coordinates.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig = px.imshow(img)\n",
    "    if pose is not None:\n",
    "        scatter = px.scatter(\n",
    "            pose,\n",
    "            x=\"x\",\n",
    "            y=\"y\",\n",
    "            color=\"id\",\n",
    "            opacity=0.2,\n",
    "        )\n",
    "        for trace in scatter.data:\n",
    "            fig.add_trace(trace)\n",
    "    if roi_coords is not None:\n",
    "        for roi_coord in roi_coords:\n",
    "            pts = roi_coord + [roi_coord[0]]\n",
    "            xs, ys = zip(*pts)\n",
    "            fig.add_scatter(x=xs, y=ys, fill=\"toself\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify start, end times to be added to subject dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load subject states to retrieve enter, exit times\n",
    "root = \"/ceph/aeon/aeon/data/raw/AEON4/social0.2/\"\n",
    "# aeon.io.reader.Subject(\"Environment_SubjectState_*\"))\n",
    "aeon.load(root, social02.Environment.SubjectState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load subject region visits if subject states are not available\n",
    "# aeon.io.reader.Csv(\"Environment_SubjectVisits_*\", columns=[\"id\", \"state\", \"region\"]))\n",
    "subj_visits = aeon.load(root, social02.Environment.SubjectVisits)\n",
    "subj_visits[(subj_visits[\"region\"] == \"Environment\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curate frames from single animal videos and export as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAA-1104045\n",
      "exp start times: ['2024-01-31T11:28:45.543519974']\n",
      "exp stop times: ['2024-02-03T16:23:52.943999767']\n",
      "BAA-1104047\n",
      "exp start times: ['2024-02-05T15:43:11.581535816' '2024-02-08T14:31:46.447999954']\n",
      "exp stop times: ['2024-02-08T14:23:15.659999847' '2024-02-08T14:41:38.703999996']\n"
     ]
    }
   ],
   "source": [
    "# get experiment times for each subject within a session\n",
    "subj_ids = [subj for subj in aeon3_social_02[\"subjects\"] if \"multi_\" not in subj]\n",
    "cameras = pd.DataFrame()\n",
    "for subj in subj_ids:\n",
    "    subj_dict = aeon3_social_02[\"subjects\"][subj]\n",
    "    root = aeon3_social_02.get(\"root\", subj_dict[\"root\"])\n",
    "    # patch 1 and 2 rfids are swapped in AEON3\n",
    "    camera_idx = aeon3_social_02.get(\"patch_camera_order\", [1, 2, 3])\n",
    "    start_time = subj_dict[\"start\"]\n",
    "    end_time = subj_dict[\"end\"]\n",
    "    experiment_times = get_experiment_times(root, start_time, end_time)\n",
    "    print(\n",
    "        f\"{subj}\"\n",
    "        f\"\\nexp start times: {experiment_times.start}\"\n",
    "        f\"\\nexp stop times: {experiment_times.stop}\"\n",
    "    )\n",
    "    for i in range(1, 4):\n",
    "        rfid_reads = aeon.load(\n",
    "            root,\n",
    "            aeon.io.reader.Harp(pattern=f\"Patch{i}Rfid_32_*\", columns=[\"Rfid\"]),\n",
    "            start=start_time,\n",
    "            end=end_time,\n",
    "        )\n",
    "        rfid_reads = exclude_maintenance_data(rfid_reads, experiment_times)\n",
    "        # get the 3 hours with the most RFID reads\n",
    "        most_reads = rfid_reads.groupby(pd.Grouper(freq=\"h\")).size().nlargest(3).index\n",
    "        rfid_most_reads = pd.DataFrame()\n",
    "        for hour in most_reads:\n",
    "            rfid_most_reads = pd.concat(\n",
    "                [rfid_most_reads, rfid_reads.loc[hour : hour + pd.Timedelta(hours=1)]]\n",
    "            )\n",
    "        n_samples = min(100, len(rfid_most_reads))\n",
    "        # randomly sample max 100 rows per patch\n",
    "        rfid_most_reads = rfid_most_reads.sample(n=n_samples)\n",
    "        camera = aeon.load(\n",
    "            root,\n",
    "            aeon.io.reader.Video(f\"CameraPatch{camera_idx[i-1]}_*\"),\n",
    "            time=rfid_most_reads.index,\n",
    "        )\n",
    "        camera[\"id\"] = subj\n",
    "        cameras = pd.concat([cameras, camera])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras.groupby(\"id\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras.to_csv(\n",
    "    \"/ceph/aeon/aeon/code/scratchpad/sleap/social0.2/\"\n",
    "    f\"AEON3/{aeon3_social_02['session']}_rfid_patch_frames_tail2.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional sanity check to make sure frames contain animals\n",
    "fig = px.imshow(\n",
    "    gridframes(list(frames(cameras.sample(n=25))),\n",
    "               width=1440, height=1080, shape=25)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curate multi-animal frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1079968/1921877639.py:71: UserWarning:\n",
      "\n",
      "No 'Experiment' event at the start of the search range. Using input `end_time` as last experiment stop time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root = aeon3_social_02.get(\"root\", aeon3_social_02[\"subjects\"][\"BAA-1104047\"][\"root\"])\n",
    "start_time = aeon3_social_02[\"subjects\"][\"multi_animal\"][\"start\"]\n",
    "end_time = aeon3_social_02[\"subjects\"][\"multi_animal\"][\"end\"]\n",
    "experiment_times = get_experiment_times(root, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfid_times = pd.concat(\n",
    "    [\n",
    "        aeon.load(\n",
    "            root,\n",
    "            aeon.io.reader.Harp(pattern=f\"Patch{i}Rfid_32_*\", columns=[\"Rfid\"]),\n",
    "            start=start_time,\n",
    "            end=end_time,\n",
    "        ).assign(Patch=i)\n",
    "        for i in range(1, 4)\n",
    "    ]\n",
    ")\n",
    "rfid_times.sort_index(inplace=True)\n",
    "rfid_hour_chunks = rfid_times.index.round(\"h\").unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get patch coordinates\n",
    "metadata = aeon.load(\n",
    "    root, social02.Metadata, start=pd.Timestamp(start_time.date()), end=end_time\n",
    ")[\"metadata\"].iloc[0]\n",
    "patch_coords = [\n",
    "    [\n",
    "        (int(pt.X), int(pt.Y))\n",
    "        for pt in eval(f\"metadata.ActiveRegion.Patch{i}Region.ArrayOfPoint\")\n",
    "    ]\n",
    "    for i in range(1, 4)\n",
    "]\n",
    "# Visualise patch coordinates\n",
    "img = get_single_frame(root, social02.CameraTop.Video, start_time)\n",
    "plot_overlay(img, roi_coords=patch_coords)\n",
    "# Get patch midpoints\n",
    "patch_midpoints = [\n",
    "    (sum(x for x, y in patch_coords[i]) / 4, sum(y for x, y in patch_coords[i]) / 4)\n",
    "    for i in range(3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter pose data to only include frames where all animals are within 100 pixels of the patch midpoint\n",
    "def within_patch_radius(row, patch_midpoints, radius):\n",
    "    for num, midpoint in enumerate(patch_midpoints):\n",
    "        if (\n",
    "            np.sqrt((row[\"x\"] - midpoint[0]) ** 2 + (row[\"y\"] - midpoint[1]) ** 2)\n",
    "            <= radius\n",
    "        ):\n",
    "            return f\"Patch{str(num + 1)}\"\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "pose = (\n",
    "    aeon.load(\n",
    "        root,\n",
    "        social02.CameraTop.Pose,\n",
    "        start=start_time,\n",
    "        end=end_time,\n",
    "    )\n",
    "    # exclude maintenance data\n",
    "    .pipe(exclude_maintenance_data, experiment_times)\n",
    "    # exclude hour chunks without RFID reads\n",
    "    .loc[lambda df: df.index.round(\"h\").isin(rfid_hour_chunks)]\n",
    "    .assign(\n",
    "        in_patch=lambda df: df.apply(\n",
    "            within_patch_radius, axis=1, args=(patch_midpoints, 100)\n",
    "        )\n",
    "    )\n",
    "    # include only rows with animals in patch\n",
    "    .dropna(subset=[\"in_patch\"])\n",
    "    # include only timestamps with multiple animals\n",
    "    .loc[lambda df: df.index.duplicated(keep=False)]\n",
    "    # group by timestamp\n",
    "    .groupby(level=0)\n",
    "    # include only rows with all animals in the same patch\n",
    "    .filter(lambda x: x[\"in_patch\"].nunique() == 1)\n",
    "    .assign(\n",
    "        id=lambda df: df[\"class\"].apply(\n",
    "            lambda x: \"BAA-1104047\" if x == 1.0 else \"BAA-1104045\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "plot_overlay(img, pose=pose, roi_coords=patch_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for each patch 50 frames from the 3 hours with the most co-occurrences\n",
    "most_cooccurrences = (\n",
    "    pose.groupby([\"in_patch\", pd.Grouper(freq=\"h\")])\n",
    "    .size()\n",
    "    .groupby(level=0, group_keys=False)\n",
    "    .nlargest(3)\n",
    "    .index\n",
    ")\n",
    "cameras = pd.DataFrame()\n",
    "for patch, time in most_cooccurrences:\n",
    "    group = pose.loc[(pose[\"in_patch\"] == patch) &\n",
    "                     (pose.index.floor(\"h\") == time)]\n",
    "    camera = aeon.load(\n",
    "        root,\n",
    "        aeon.io.reader.Video(f\"Camera{patch}_*\"),\n",
    "        time=group.index,\n",
    "    ).sample(n=50)\n",
    "    cameras = pd.concat([cameras, camera])\n",
    "\n",
    "cameras.to_csv(\n",
    "    \"/ceph/aeon/aeon/code/scratchpad/sleap/social0.2/\"\n",
    "    f\"AEON3/{aeon3_social_02['session']}_rfid_patch_frames_tail_ma.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional sanity check to make sure frames contain animals\n",
    "fig = px.imshow(\n",
    "    gridframes(list(frames(cameras.sample(n=25))), width=1440, height=1080, shape=25)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLEAP env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sleap\n",
    "from sleap.gui.suggestions import SuggestionFrame\n",
    "from sleap.io.pathutils import fix_path_separator\n",
    "\n",
    "\n",
    "def update_slp_video_paths(\n",
    "    labels: sleap.Labels, old_path: str, new_path: str\n",
    ") -> sleap.Labels:\n",
    "    \"\"\"\n",
    "    Updates video paths in a SLEAP labels object (e.g., to move training from local to remote machine).\n",
    "\n",
    "    Args:\n",
    "        labels (sleap.Labels): A SLEAP Labels object.\n",
    "        old_path (str): Old path to video files.\n",
    "        new_path (str): New path to video files.\n",
    "\n",
    "    Returns:\n",
    "        sleap.Labels: A SLEAP Labels object with updated video paths.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    videos = [\n",
    "        sleap.Video.from_filename(\n",
    "            fix_path_separator(vid.filename).replace(old_path, new_path), grayscale=True\n",
    "        )\n",
    "        for vid in labels.videos\n",
    "    ]\n",
    "\n",
    "    lfs = []\n",
    "    for lf in labels.labeled_frames:\n",
    "        lf = sleap.instance.LabeledFrame(\n",
    "            video=videos[labels.videos.index(lf.video)],\n",
    "            frame_idx=lf.frame_idx,\n",
    "            instances=lf.instances,\n",
    "        )\n",
    "        lfs.append(lf)\n",
    "\n",
    "    return sleap.Labels(\n",
    "        labeled_frames=lfs,\n",
    "        videos=videos,\n",
    "        skeletons=labels.skeletons,\n",
    "        tracks=labels.tracks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = pd.read_csv(\n",
    "    \"/ceph/aeon/aeon/code/scratchpad/sleap/social0.2/\"\n",
    "    f\"AEON3/{aeon3_social_02['session']}_rfid_patch_frames_tail2.csv\"\n",
    ")\n",
    "cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_dict = {\n",
    "    video: sleap.Video.from_filename(video, grayscale=True)\n",
    "    for video in cameras._path.unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create skeleton with just \"spine_bottom\", \"tail_base\"\n",
    "skeleton = sleap.Skeleton()\n",
    "skeleton.add_nodes([\"spine_bottom\", \"tail_base\"])\n",
    "# skeleton.add_edge(\"spine_top\", \"centroid\")\n",
    "# skeleton.add_edge(\"centroid\", \"spine_bottom\")\n",
    "skeleton.add_edge(\"spine_bottom\", \"tail_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sleap.load_file(\n",
    "    \"/ceph/aeon/aeon/code/scratchpad/sleap/social0.2/\"\n",
    "    f\"AEON3/{aeon3_social_02['session']}_rfid_patch_frames_tail_2_points.slp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of Tracks\n",
    "subjs = (\n",
    "    cameras[\"id\"].unique()\n",
    "    if \"id\" in cameras.columns\n",
    "    else [subj for subj in aeon3_social_02[\"subjects\"].keys() if \"multi_\" not in subj]\n",
    ")\n",
    "tracks_dict = {subj: sleap.Track(spawned_on=0, name=subj) for subj in subjs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SuggestionFrames to be inferred using existing patch cam models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs = []\n",
    "for _, row in cameras.drop_duplicates(subset=[\"_path\", \"_frame\"]).iterrows():\n",
    "    # create a new SuggestionFrame for each row\n",
    "    sfs.append(\n",
    "        SuggestionFrame(\n",
    "            video=videos_dict[row._path],\n",
    "            frame_idx=row._frame,\n",
    "        )\n",
    "    )\n",
    "labels_suggest = sleap.Labels(\n",
    "    videos=list(videos_dict.values()),\n",
    "    suggestions=sfs,\n",
    "    skeletons=[skeleton],\n",
    "    tracks=labels.tracks,  # list(tracks_dict.values()),\n",
    ")\n",
    "len(labels_suggest.suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_path = (\n",
    "    \"/ceph/aeon/aeon/code/scratchpad/sleap/social0.2/\"\n",
    "    f\"AEON3/{aeon3_social_02['session']}_rfid_patch_frames_tail_2_points_1.slp\"\n",
    ")\n",
    "sleap.Labels.save_file(labels_suggest, slp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert predictions to user labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_pr_path = (\n",
    "    \"/ceph/aeon/aeon/code/scratchpad/sleap/social0.2/AEON3/\"\n",
    "    f\"{aeon3_social_02['session']}_rfid_patch_frames_tail_2_points_2.slp\"\n",
    ")\n",
    "labels_pr = sleap.load_file(slp_pr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labels(labeled_frames=1299, videos=32, skeletons=1, tracks=2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfs = []\n",
    "for lf in labels_pr.labeled_frames:\n",
    "    instances = []\n",
    "    for inst in lf.instances_to_show:  # instances:\n",
    "        track_name = inst.track.name\n",
    "        instances.append(\n",
    "            sleap.Instance(\n",
    "                skeleton=skeleton,\n",
    "                track=tracks_dict.get(track_name),  # inst.track,\n",
    "                points={\n",
    "                    node.name: sleap.instance.Point(\n",
    "                        x=point.x, y=point.y, visible=point.visible\n",
    "                    )\n",
    "                    for node, point in inst.nodes_points\n",
    "                    if node.name in skeleton.node_names\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "    # create a new labeled frame\n",
    "    user_lf = sleap.instance.LabeledFrame(\n",
    "        video=lf.video,\n",
    "        frame_idx=lf.frame_idx,\n",
    "        instances=instances,\n",
    "    )\n",
    "    lfs.append(user_lf)\n",
    "labels_user = sleap.Labels(labeled_frames=lfs)\n",
    "labels_user = update_slp_video_paths(labels_user, \"Z:\", \"/ceph/aeon\")\n",
    "labels_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleap.Labels.save_file(\n",
    "    labels_user,\n",
    "    \"/ceph/aeon/aeon/code/scratchpad/sleap/social0.2/\"\n",
    "    f\"AEON3/{aeon3_social_02['session']\n",
    "             }_rfid_patch_frames_tail_2_points_2.slp\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
