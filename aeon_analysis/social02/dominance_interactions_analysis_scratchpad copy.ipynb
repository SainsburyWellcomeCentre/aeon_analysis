{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will have analysis to correlate of social inraction events and outcomes, e.g. tube tests, chasing, fighting. \n",
    "For this we will need:\n",
    "- CSVs exported from event detection analyses, containing start and end timestamps/frames and ID of dominant/winner mouse. \n",
    "- DJ BlockAnalysis pipeline, whih has tools for computign patch preference over time\n",
    "- external tube test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social02 exp timeline\n",
    "\n",
    "# Aeon3\n",
    "#  2024-01-31 : 2024-02-03 - BAA-1104045 pre solo\n",
    "#  2024-02-05 : 2024-02-08 - BAA-1104047 pre solo (dominant)\n",
    "#  2024-02-09 : 2024-02-23 - BAA-1104045, BAA-1104047 social\n",
    "#  2024-02-25 : 2024-02-28 - BAA-1104045 post solo\n",
    "#  2024-02-28 : 2024-03-02 - BAA-1104047 post solo\n",
    "\n",
    "\n",
    "# Aeon4\n",
    "#  2024-01-31 : 2024-02-03 - BAA-1104048 pre solo (dominant)\n",
    "#  2024-02-05 : 2024-02-08 - BAA-1104049 pre solo\n",
    "#  2024-02-09 : 2024-02-23 - BAA-1104048, BAA-1104049 social\n",
    "#  2024-02-25 : 2024-02-28 - BAA-1104048 post solo\n",
    "#  2024-02-28 : 2024-03-02 - BAA-1104049 post solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import psutil\n",
    "import os\n",
    " \n",
    "# Get the current process ID\n",
    "pid = os.getpid()\n",
    " \n",
    "# Create a Process object\n",
    "process = psutil.Process(pid)\n",
    " \n",
    "# Get the memory info\n",
    "memory_info = process.memory_info()\n",
    " \n",
    "print(f\"RAM used by the notebook process: {memory_info.rss / 1024 / 1024:.2f} MB\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import datajoint as dj\n",
    "import aeon\n",
    "from aeon.schema.schemas import social02\n",
    "from aeon.dj_pipeline.analysis.block_analysis import * #this connects to database and imports all tables in block_analysis\n",
    "\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import timedelta, datetime\n",
    "from aeon.io import api\n",
    "from scipy import stats\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get all data from one experiment and arena - so 2 mice, 2 weeks of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get social interaciton times and outcomes from heuristics based detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace with revised csv path\n",
    "base_path = '/ceph/aeon/aeon/code/scratchpad/Orsi/'\n",
    "tube_test_path = 'all_tube_test_videos/AEON3_tube_tests_revised_final.csv'\n",
    "fights_path = 'all_fighting_videos/AEON3_fights.csv'\n",
    "chasing_path = 'all_chasing_videos/AEON3_chases.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open CSV containing tube test data.\n",
    "tube_test_df = pd.read_csv(base_path + tube_test_path)\n",
    "# Open CSV containing fighting data.\n",
    "fights_df = pd.read_csv(base_path + fights_path)\n",
    "# Open CSV containing chasing data.\n",
    "chasing_df = pd.read_csv(base_path + chasing_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the start and end timestamps to datetime\n",
    "chasing_df['start_timestamp'] = pd.to_datetime(chasing_df['start_timestamp'], format='%Y-%m-%dT%H-%M-%S')\n",
    "chasing_df['end_timestamp'] = pd.to_datetime(chasing_df['end_timestamp'], format='%Y-%m-%dT%H-%M-%S')\n",
    "\n",
    "fights_df['start_timestamp'] = pd.to_datetime(fights_df['start_timestamp'], format='%Y-%m-%dT%H-%M-%S')\n",
    "fights_df['end_timestamp'] = pd.to_datetime(fights_df['end_timestamp'], format='%Y-%m-%dT%H-%M-%S')\n",
    "\n",
    "tube_test_df['start_timestamp'] = pd.to_datetime(tube_test_df['start_timestamp'], format='%Y-%m-%dT%H-%M-%S')\n",
    "tube_test_df['end_timestamp'] = pd.to_datetime(tube_test_df['end_timestamp'], format='%Y-%m-%dT%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chasing_df. head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asessing the quality of chasing data\n",
    "print(f\"{(len(chasing_df[chasing_df['chaser_id'].isna()]))/len(chasing_df)} ratio of chases with non-assigned chaser_ids\")\n",
    "print(len(chasing_df))\n",
    "\n",
    "# check te assignemnt of these maually\n",
    "import random\n",
    "random_numbers = [random.randint(0, len(chasing_df)) for _ in range(30)] #around 5% of the data\n",
    "sorted_random_numbers = sorted(random_numbers)\n",
    "print(sorted_random_numbers)\n",
    "\n",
    "#after revising a few\n",
    "revised_chasing_path = 'all_chasing_videos/AEON3_chases_revised.csv'\n",
    "revised_chasing_df = pd.read_csv(base_path + revised_chasing_path)\n",
    "# event checked manually\n",
    "print(len(revised_chasing_df[~revised_chasing_df['revision'].isna()]))\n",
    "# good checked chases\n",
    "print(len(revised_chasing_df[revised_chasing_df['revision'] == 'Ok']))\n",
    "# bad cheked chases\n",
    "print(len(revised_chasing_df[revised_chasing_df['revision'] == '47']))\n",
    "print(len(revised_chasing_df[revised_chasing_df['revision'] == '45']))\n",
    "# error rate\n",
    "error_count = len(revised_chasing_df[revised_chasing_df['revision'] == '47']) + len(revised_chasing_df[revised_chasing_df['revision'] == '45'])\n",
    "good_count = len(revised_chasing_df[revised_chasing_df['revision'] == 'Ok'])\n",
    "error_rate = error_count / good_count+error_count\n",
    "print(error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tube_test_df. head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fights_df. head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make csv for three behaviours hte saem format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'behavior_type' column to each data frame\n",
    "chasing_df['behavior_type'] = 'chasing'\n",
    "fights_df['behavior_type'] = 'fighting'\n",
    "tube_test_df['behavior_type'] = 'tube_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chasing_df.rename(columns={'chaser_id': 'dominant_id'}, inplace=True)\n",
    "tube_test_df.rename(columns={'winner_id': 'dominant_id'}, inplace=True)\n",
    "fights_df['dominant_id'] = fights_df.get('dominant_id', 'NaN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data frames\n",
    "combined_df = pd.concat([chasing_df, fights_df, tube_test_df])\n",
    "# Replace NaN values in 'dominant_id' with a string 'NaN'\n",
    "combined_df['dominant_id'] = combined_df['dominant_id'].fillna('NaN')\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = np.unique(combined_df['dominant_id'])\n",
    "unique_ids = unique_ids[unique_ids != 'NaN']\n",
    "unique_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get foraging data from DJ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = {\"experiment_name\": \"social0.2-aeon3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = dj.schema(get_schema_name(\"block_analysis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_start_day = '2024-02-09 00:00:00'\n",
    "social_end_day = '2024-02-23 23:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_start_time = '2024-02-09 16:00:00'\n",
    "social_end_time = '2024-02-23 13:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_solo_start_day = '2024-01-31 00:00:00'\n",
    "pre_solo_end_day= '2024-02-08 23:00:00'\n",
    "post_solo_start_day = '2024-02-25 00:00:00'\n",
    "post_solo_end_day = '2024-03-02 23:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define light cycle periods\n",
    "night_start = '08:00'\n",
    "night_end = '19:00'\n",
    "twilight_start = '07:00'\n",
    "twilight_end = '08:00'\n",
    "dawn_start = '19:00'\n",
    "dawn_end = '20:00'\n",
    "day_start = '20:00'\n",
    "day_end = '07:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load metadata:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just get metadata base on a random solo block\n",
    "\n",
    "exp_start = pd.Timestamp(\"2024-01-31\")\n",
    "\n",
    "metadata = (\n",
    "    api.load('/ceph/aeon/aeon/data/raw/AEON3/social0.2', social02.Metadata, exp_start, pd.Timestamp('2024-02-11 15:57:42')).iloc[0].metadata\n",
    ")\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add pre/post tubetest data maually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add pre/post tue test data manually\n",
    "\n",
    "# Create the data\n",
    "pre_tube_test = {\n",
    "    'behavior_type': ['pre_tube_test'] * 10,\n",
    "    'dominant_id': ['BAA-1104045'] * 2 + ['BAA-1104047'] * 8\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "pre_tube_test = pd.DataFrame(pre_tube_test)\n",
    "\n",
    "\n",
    "# Create the data\n",
    "post_tube_test = {\n",
    "    'behavior_type': ['pre_tube_test'] * 10,\n",
    "    'dominant_id': ['BAA-1104045'] * 1 + ['BAA-1104047'] * 9\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "post_tube_test = pd.DataFrame(post_tube_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Temporal pattern and stability of dominance interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raster of chasing per subject over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dictionary for color mapping\n",
    "id_color_map = {\n",
    "    'NaN': 'grey',\n",
    "    'BAA-1104047': 'purple',\n",
    "    'BAA-1104045': 'green'\n",
    "} \n",
    "behaviour_map = {\n",
    "    'chasing': 'blue',\n",
    "    'fighting': 'red',\n",
    "    'tube_test': 'orange'\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_start_datetime = datetime.strptime(social_start_time, '%Y-%m-%d %H:%M:%S')\n",
    "social_end_datetime = datetime.strptime(social_end_time, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "pre_solo_start_datetime =datetime.strptime(pre_solo_start_day, '%Y-%m-%d %H:%M:%S')\n",
    "pre_solo_end_datetime= datetime.strptime(pre_solo_end_day, '%Y-%m-%d %H:%M:%S')\n",
    "post_solo_start_datetime = datetime.strptime(post_solo_start_day, '%Y-%m-%d %H:%M:%S')\n",
    "post_solo_end_datetime = datetime.strptime(post_solo_end_day, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the raster plot\n",
    "fig = px.scatter(\n",
    "    combined_df,\n",
    "    x='start_timestamp',\n",
    "    y='behavior_type',\n",
    "    color='dominant_id',\n",
    "    title='Behavior Raster Plot',\n",
    "    labels={'start_timestamp': 'Time', 'behavior_type': 'Behavior Type'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Set x-axis limits\n",
    "fig.update_xaxes(range=[social_start_time, social_end_time])\n",
    "# Set x-axis limits\n",
    "fig.update_yaxes(range=[-1, 2.5])  # Example date range\n",
    "\n",
    "# Iterate over each day in the two-week period\n",
    "current_day = social_start_datetime\n",
    "while current_day < social_end_datetime:\n",
    "    # Define the start and end times for each period\n",
    "    night_start_time = current_day.replace(hour=int(night_start.split(':')[0]), minute=int(night_start.split(':')[1]))\n",
    "    night_end_time = current_day.replace(hour=int(night_end.split(':')[0]), minute=int(night_end.split(':')[1]))\n",
    "    twilight_start_time = current_day.replace(hour=int(twilight_start.split(':')[0]), minute=int(twilight_start.split(':')[1]))\n",
    "    twilight_end_time = current_day.replace(hour=int(twilight_end.split(':')[0]), minute=int(twilight_end.split(':')[1]))\n",
    "    dawn_start_time = current_day.replace(hour=int(dawn_start.split(':')[0]), minute=int(dawn_start.split(':')[1]))\n",
    "    dawn_end_time = current_day.replace(hour=int(dawn_end.split(':')[0]), minute=int(dawn_end.split(':')[1]))\n",
    "    day_start_time = current_day.replace(hour=int(day_start.split(':')[0]), minute=int(day_start.split(':')[1]))\n",
    "    day_end_time = current_day + timedelta(days=1) if int(day_end.split(':')[0]) < int(day_start.split(':')[0]) else current_day.replace(hour=int(day_end.split(':')[0]), minute=int(day_end.split(':')[1]))\n",
    "    # Add horizontal lines for light and dark periods\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=twilight_start_time,\n",
    "        x1=twilight_end_time,\n",
    "        y0=-0.5,\n",
    "        y1=-0.5,\n",
    "        line=dict(color=\"gray\", width=4)\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=night_start_time,\n",
    "        x1=night_end_time,\n",
    "        y0=-0.5,\n",
    "        y1=-0.5,\n",
    "        line=dict(color=\"black\", width=4)\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=dawn_start_time,\n",
    "        x1=dawn_end_time,\n",
    "        y0=-0.5,\n",
    "        y1=-0.5,\n",
    "        line=dict(color=\"gray\", width=4)\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=day_start_time,\n",
    "        x1=day_end_time,\n",
    "        y0=-0.5,\n",
    "        y1=-0.5,\n",
    "        line=dict(color=\"white\", width=4)\n",
    "    )\n",
    "    # Move to the next day\n",
    "    current_day += timedelta(days=1)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: could also do this based no on discrete events but duraiton of time spent interacting\n",
    "# Create a list of time bins\n",
    "time_bins = pd.date_range(start=social_start_time, end=social_end_time, freq='24h')\n",
    "\n",
    "# Bin the start_timestamp into the created time bins\n",
    "combined_df['time_bin'] = pd.cut(combined_df['start_timestamp'], bins=time_bins)\n",
    "\n",
    "# Calculate the number of interactions for each behavior in each time bin\n",
    "interaction_counts = combined_df.groupby(['time_bin', 'behavior_type']).size().reset_index(name='total_interactions')\n",
    "\n",
    "# Convert the time_bin column to strings\n",
    "# Extract the start time of each bin\n",
    "interaction_counts['time_bin_start'] = interaction_counts['time_bin'].apply(lambda x: x.left)\n",
    "\n",
    "# Convert the start time to a string format\n",
    "interaction_counts['time_bin_start'] = interaction_counts['time_bin_start'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "print(interaction_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the line plot\n",
    "fig = px.line(\n",
    "    interaction_counts,\n",
    "    x='time_bin_start',\n",
    "    y='total_interactions',\n",
    "    color='behavior_type',\n",
    "    title='Number of Interactions Over Time for Each Behavior',\n",
    "    labels={'time_bin_start': 'Time Bin', 'total_interactions': 'Number of Interactions'},\n",
    "    color_discrete_map=behaviour_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportion of winning and losing in two animals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data to count the occurrences of each dominant_id for chasing\n",
    "chaser_counts_chasing = chasing_df['dominant_id'].value_counts().reset_index()\n",
    "chaser_counts_chasing.columns = ['dominant_id', 'count']\n",
    "\n",
    "# Aggregate the data to count the occurrences of each dominant_id for tube test\n",
    "chaser_counts_tube_test = tube_test_df['dominant_id'].value_counts().reset_index()\n",
    "chaser_counts_tube_test.columns = ['dominant_id', 'count']\n",
    "\n",
    "# Aggregate the data to count the occurrences of each dominant_id for tube test\n",
    "chaser_counts_pre_tube_test = pre_tube_test['dominant_id'].value_counts().reset_index()\n",
    "chaser_counts_pre_tube_test.columns = ['dominant_id', 'count']\n",
    "\n",
    "# Aggregate the data to count the occurrences of each dominant_id for tube test\n",
    "chaser_counts_post_tube_test = post_tube_test['dominant_id'].value_counts().reset_index()\n",
    "chaser_counts_post_tube_test.columns = ['dominant_id', 'count']\n",
    "\n",
    "# Create scatter plots using plotly express\n",
    "fig_chasing = px.scatter(\n",
    "    chaser_counts_chasing,\n",
    "    x='dominant_id',\n",
    "    y='count',\n",
    "    color='dominant_id',\n",
    "    title='Count of Chases per Chaser ID',\n",
    "    labels={'dominant_id': 'Chaser ID', 'count': 'Count of Chases'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "fig_tube_test = px.scatter(\n",
    "    chaser_counts_tube_test,\n",
    "    x='dominant_id',\n",
    "    y='count',\n",
    "    color='dominant_id',\n",
    "    title='Count of Tube Tests per Chaser ID',\n",
    "    labels={'dominant_id': 'Chaser ID', 'count': 'Count of Tube Tests'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "fig_pre_tube_test = px.scatter(\n",
    "    chaser_counts_pre_tube_test,\n",
    "    x='dominant_id',\n",
    "    y='count',\n",
    "    color='dominant_id',\n",
    "    title='Count of Tube Tests per Chaser ID',\n",
    "    labels={'dominant_id': 'Chaser ID', 'count': 'Count of Tube Tests'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "fig_post_tube_test = px.scatter(\n",
    "    chaser_counts_post_tube_test,\n",
    "    x='dominant_id',\n",
    "    y='count',\n",
    "    color='dominant_id',\n",
    "    title='Count of Tube Tests per Chaser ID',\n",
    "    labels={'dominant_id': 'Chaser ID', 'count': 'Count of Tube Tests'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=4, subplot_titles=('Chasing', 'Tube Test', 'Pre Tube Test', 'Post Tube Test'))\n",
    "\n",
    "# Add traces to the subplots\n",
    "for trace in fig_chasing['data']:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "for trace in fig_tube_test['data']:\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "    \n",
    "for trace in fig_pre_tube_test['data']:\n",
    "    fig.add_trace(trace, row=1, col=3)\n",
    "    \n",
    "for trace in fig_post_tube_test['data']:\n",
    "    fig.add_trace(trace, row=1, col=4)\n",
    "\n",
    "# Set y-axis limits for both subplots\n",
    "fig.update_yaxes(range=[0, chaser_counts_chasing['count'].max() + 10], row=1, col=1)\n",
    "fig.update_yaxes(range=[0, chaser_counts_tube_test['count'].max() + 10], row=1, col=2)\n",
    "fig.update_yaxes(range=[0, chaser_counts_pre_tube_test['count'].max() + 10], row=1, col=3)\n",
    "fig.update_yaxes(range=[0, chaser_counts_post_tube_test['count'].max() + 10], row=1, col=4)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title_text='Count of Chases and Tube Tests per Chaser ID')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fraction of winning for chasing\n",
    "chaser_counts_chasing = chasing_df['dominant_id'].value_counts(normalize=True).reset_index()\n",
    "chaser_counts_chasing.columns = ['dominant_id', 'p_wins']\n",
    "chaser_counts_chasing['behavior_type'] = 'Chasing'\n",
    "\n",
    "# Calculate the fraction of winning for tube test\n",
    "chaser_counts_tube_test = tube_test_df['dominant_id'].value_counts(normalize=True).reset_index()\n",
    "chaser_counts_tube_test.columns = ['dominant_id', 'p_wins']\n",
    "chaser_counts_tube_test['behavior_type'] = 'Tube Test'\n",
    "\n",
    "# Calculate the fraction of winning for pre tube test\n",
    "chaser_counts_pre_tube_test = pre_tube_test['dominant_id'].value_counts(normalize=True).reset_index()\n",
    "chaser_counts_pre_tube_test.columns = ['dominant_id', 'p_wins']\n",
    "chaser_counts_pre_tube_test['behavior_type'] = 'Pre Tube Test'\n",
    "\n",
    "# Calculate the fraction of winning for post tube test\n",
    "chaser_counts_post_tube_test = post_tube_test['dominant_id'].value_counts(normalize=True).reset_index()\n",
    "chaser_counts_post_tube_test.columns = ['dominant_id', 'p_wins']\n",
    "chaser_counts_post_tube_test['behavior_type'] = 'Post Tube Test'\n",
    "\n",
    "# Combine the data\n",
    "combined_counts = pd.concat([\n",
    "    chaser_counts_chasing, \n",
    "    chaser_counts_tube_test, \n",
    "    chaser_counts_pre_tube_test, \n",
    "    chaser_counts_post_tube_test\n",
    "])\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(\n",
    "    combined_counts,\n",
    "    x='behavior_type',\n",
    "    y='p_wins',\n",
    "    color='dominant_id',\n",
    "    title='Dominance per Behavior Type and Dominant ID',\n",
    "    labels={'behavior_type': 'Behavior Type', 'p_wins': 'Proportion of wins'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time binned measure of dominance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate te number of winning events over all events for every 42h time bin and plot over time\n",
    "# Create a list of time bins\n",
    "time_bins = pd.date_range(start=social_start_time, end=social_end_time, freq='24h')\n",
    "# Bin the start_timestamp into the created time bins\n",
    "chasing_df['time_bin'] = pd.cut(chasing_df['start_timestamp'], bins=time_bins)\n",
    "# Calculate the number of winning events and total events in each time bin and id\n",
    "winning_counts = chasing_df.groupby(['time_bin', 'dominant_id']).size().reset_index(name='total_events')\n",
    "# Calculate the fraction of winning events for each dominant_id in each time bin\n",
    "winning_counts['fraction_winning'] = winning_counts.groupby('time_bin')['total_events'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# Convert the time_bin column to strings\n",
    "winning_counts['time_bin'] = winning_counts['time_bin'].astype(str)\n",
    "\n",
    "# Create the line plot\n",
    "fig = px.line(\n",
    "    winning_counts,\n",
    "    x='time_bin',\n",
    "    y='total_events',\n",
    "    color = 'dominant_id',\n",
    "    title='Winning Events Over Time (Chasing)',\n",
    "    labels={'time_bin': 'Time Bin', 'total_events': 'Winning Events'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Create the line plot\n",
    "fig = px.line(\n",
    "    winning_counts,\n",
    "    x='time_bin',\n",
    "    y='fraction_winning',\n",
    "    color = 'dominant_id',\n",
    "    title='Fraction of Winning Events Over Time (Chasing)',\n",
    "    labels={'time_bin': 'Time Bin', 'fraction_winning': 'Proportion of Winning Events'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate te number of winning events over all events for every 42h time bin and plot over time\n",
    "# Create a list of time bins\n",
    "time_bins = pd.date_range(start=social_start_time, end=social_end_time, freq='24h')\n",
    "# Bin the start_timestamp into the created time bins\n",
    "tube_test_df['time_bin'] = pd.cut(tube_test_df['start_timestamp'], bins=time_bins)\n",
    "# Calculate the number of winning events and total events in each time bin and id\n",
    "winning_counts = tube_test_df.groupby(['time_bin', 'dominant_id']).size().reset_index(name='total_events')\n",
    "# Calculate the fraction of winning events for each dominant_id in each time bin\n",
    "winning_counts['fraction_winning'] = winning_counts.groupby('time_bin')['total_events'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# Convert the time_bin column to strings\n",
    "winning_counts['time_bin'] = winning_counts['time_bin'].astype(str)\n",
    "\n",
    "\n",
    "# Create the line plot\n",
    "fig = px.line(\n",
    "    winning_counts,\n",
    "    x='time_bin',\n",
    "    y='total_events',\n",
    "    color = 'dominant_id',\n",
    "    title='Winning Events Over Time (Tube test)',\n",
    "    labels={'time_bin': 'Time Bin', 'total_events': 'Winning Events'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Create the line plot\n",
    "fig = px.line(\n",
    "    winning_counts,\n",
    "    x='time_bin',\n",
    "    y='fraction_winning',\n",
    "    color = 'dominant_id',\n",
    "    title='Fraction of Winning Events Over Time (Tube test)',\n",
    "    labels={'time_bin': 'Time Bin', 'fraction_winning': 'Proportion of Winning Events'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chasing speed and dominance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_restriction = {'block_start':'2024-02-10 16:08:15.027999'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speed of chasing and dominane - some chasing events are slower, so difernt behviour. maybe subordinate is chaser more in these slow chases?\n",
    "\n",
    "#get centroid data during chasing events\n",
    "block_subjects = (\n",
    "    BlockAnalysis.Subject.proj('position_x', 'position_y', 'position_timestamps')\n",
    "    & key\n",
    "    & f'block_start >= \"{social_start_day}\"'\n",
    "    & f'block_start <= \"{social_end_day}\"'\n",
    "    #& block_restriction\n",
    ")\n",
    "block_subjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_subjects_dict = block_subjects.fetch(as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert chasing_df timestamps to datetime\n",
    "chasing_df['start_timestamp'] = pd.to_datetime(chasing_df['start_timestamp'])\n",
    "chasing_df['end_timestamp'] = pd.to_datetime(chasing_df['end_timestamp'])\n",
    "\n",
    "# Initialize an empty list to store the filtered position data\n",
    "filtered_positions = []\n",
    "\n",
    "# Loop through each row in chasing_df so we only get position that are within a chase\n",
    "for _, row in chasing_df.iterrows():\n",
    "    start_time = row['start_timestamp']\n",
    "    end_time = row['end_timestamp']\n",
    "    \n",
    "    # Filter the position data for each subject\n",
    "    for s in block_subjects_dict:\n",
    "        # Convert position timestamps to datetime\n",
    "        position_timestamps = pd.to_datetime(s['position_timestamps'])\n",
    "        \n",
    "        # Filter based on the start and end timestamps\n",
    "        mask = (position_timestamps >= start_time) & (position_timestamps <= end_time)\n",
    "        \n",
    "        # Create a DataFrame for the filtered data\n",
    "        filtered_data = pd.DataFrame(\n",
    "      {\n",
    "                \"subject_name\": [s[\"subject_name\"]] * sum(mask),\n",
    "                \"position_timestamps\": position_timestamps[mask],\n",
    "                \"position_x\": pd.Series(s[\"position_x\"])[mask].values,\n",
    "                \"position_y\": pd.Series(s[\"position_y\"])[mask].values,\n",
    "                \"start_time\": [start_time] * sum(mask),\n",
    "                \"end_time\": [end_time] * sum(mask)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Append the filtered data to the list\n",
    "        filtered_positions.append(filtered_data)\n",
    "\n",
    "# Concatenate the list of filtered data into a single DataFrame\n",
    "subjects_positions_df = pd.concat(filtered_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the avg speed of each chasing event\n",
    "# calculate the distance travelled by each subject and divide by the duration of chase\n",
    "subjects_positions_df[\"speed\"] = (\n",
    "    subjects_positions_df.groupby(\"subject_name\")[[\"position_x\", \"position_y\"]].diff().apply(np.linalg.norm, axis=1)\n",
    "    / subjects_positions_df.reset_index()\n",
    "    .groupby(\"subject_name\")[\"position_timestamps\"]\n",
    "    .diff()\n",
    "    .dt.total_seconds()\n",
    "    .values\n",
    ")\n",
    "\n",
    "# get rid of nans, unrealistically hihg speeds and infs\n",
    "cm2px = 5.4 \n",
    "max_speed_threshold = 100 * cm2px # in cm/s\n",
    "speed_df = subjects_positions_df\n",
    "speed_df = subjects_positions_df[subjects_positions_df['speed'] < max_speed_threshold]\n",
    "speed_df = speed_df[~speed_df['speed'].isna()]\n",
    "speed_df = speed_df[~speed_df['speed'].isin([np.inf, -np.inf])]\n",
    "speed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_positions_df.to_csv('/ceph/aeon/aeon/code/scratchpad/Orsi/all_chasing_videos/chasing_speed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group speed_df by 'chase_id' and 'subject_name' to calculate avg speed per subject per chase\n",
    "avg_speed_per_subject = (\n",
    "    speed_df.groupby(['start_time', 'subject_name'])['speed']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'speed': 'avg_speed_per_subject'})\n",
    ")\n",
    "\n",
    "# Step 2: Now, group again by 'chase_id' to average over the subjects' average speeds\n",
    "avg_speed_per_chase = (\n",
    "    avg_speed_per_subject.groupby('start_time')['avg_speed_per_subject']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'avg_speed_per_subject': 'avg_speed_per_chase'})\n",
    ")\n",
    "avg_speed_per_chase.rename(columns={'start_time': 'start_timestamp'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chasing_speed_df = chasing_df.merge(avg_speed_per_chase, on='start_timestamp', how='left')\n",
    "#get rid of rows whre missings speed\n",
    "chasing_speed_df = chasing_speed_df[~chasing_speed_df['avg_speed_per_chase'].isna()]\n",
    "chasing_speed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# did we lose a lit of chases\n",
    "print(chasing_df.shape)\n",
    "print(chasing_speed_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = chasing_speed_df['dominant_id'].isna().sum()\n",
    "print(f\"Number of NaN values in dominant_id: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two chaser IDs you want to compare\n",
    "chaser1_id = 'BAA-1104045'  \n",
    "chaser2_id = 'BAA-1104047'  \n",
    "\n",
    "# Filter data for the two chaser IDs\n",
    "chaser1_speeds = chasing_speed_df[chasing_speed_df['dominant_id'] == chaser1_id]['avg_speed_per_chase']\n",
    "chaser2_speeds = chasing_speed_df[chasing_speed_df['dominant_id'] == chaser2_id]['avg_speed_per_chase']\n",
    "\n",
    "# Perform the t-test\n",
    "t_stat, p_value_ttest = stats.ttest_ind(chaser1_speeds, chaser2_speeds, equal_var=False)\n",
    "p_value_ttest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the speed of chases per chaser id\n",
    "# Initialize the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "for dominant_id in chasing_speed_df['dominant_id'].unique():\n",
    "    if pd.isna(dominant_id):\n",
    "        color = 'gray'  # Assign a default color for NaN values\n",
    "    else:\n",
    "        color = id_color_map.get(dominant_id, 'black')  # Fallback to black if the id is not in the map\n",
    "    \n",
    "    dominant_df = chasing_speed_df[chasing_speed_df['dominant_id'] == dominant_id]\n",
    "    fig.add_trace(go.Box(\n",
    "        x=dominant_df['dominant_id'],\n",
    "        y=dominant_df['avg_speed_per_chase'],\n",
    "        name=str(dominant_id),  # Convert NaN to string to display properly\n",
    "        boxpoints='all',  # Show individual points\n",
    "        jitter=0.3,  # Add some jitter to avoid overlap\n",
    "        pointpos=-1.8,  # Position of the individual points (to the right of the box)\n",
    "        marker=dict(color=color)\n",
    "    ))\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Average Speed of Chases per Chaser ID',\n",
    "    xaxis_title='Chaser ID',\n",
    "    yaxis_title='Average Speed',\n",
    "    boxmode='group',  # Group box plots by chaser ID\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Add annotations for statistical test results\n",
    "fig.add_annotation(\n",
    "    x=0.5, y=1.05, xref='paper', yref='paper',\n",
    "    text=f\"T-test p-value: {p_value_ttest:.3f}\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color='black'),\n",
    "    align='center'\n",
    ")\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaway: \n",
    "- dominance based on the outcome of these interactions is stable throughtout\n",
    "- number of fights and chases increase throughtout, while tube tests are most frequent on firs day\n",
    "- both tube test adn chasing outcomes aer more varied on first they and stabilise after \n",
    "- speed of chases is variable, and when more dominant moue is the chaser, chases are faster\n",
    "- stability shows these behaviours as good measure of dominance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dominance and foraging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_id = 'BAA-1104047' \n",
    "subordinate_id = 'BAA-1104045'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on interactions we know which one is dominant and this seems to be stable. So does dominance affect foraging behaviour? In solo vs social?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of pellets per subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pellet data\n",
    "foraging_query = (\n",
    "    BlockSubjectAnalysis.Patch.proj('pellet_count', 'pellet_timestamps', 'patch_threshold')\n",
    "    & {\"spinnaker_video_source_name\": \"CameraTop\"} #this is the video source name which we rstrict once we selected tuff to keep in table\n",
    "    & key\n",
    "    & f'block_start >= \"{social_start_day}\"'\n",
    "    & f'block_start <= \"{social_end_day}\"'\n",
    ")\n",
    "foraging_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solo_foraging_query = (\n",
    "    BlockSubjectAnalysis.Patch.proj('pellet_count', 'pellet_timestamps', 'patch_threshold')\n",
    "    & {\"spinnaker_video_source_name\": \"CameraTop\"}  # this is the video source name which we restrict once we selected stuff to keep in table\n",
    "    & key\n",
    "    & (\n",
    "        f'(block_start >= \"{pre_solo_start_day}\" AND block_start <= \"{pre_solo_end_day}\")'\n",
    "        f' OR '\n",
    "        f'(block_start >= \"{post_solo_start_day}\" AND block_start <= \"{post_solo_end_day}\")'\n",
    "    )\n",
    ")\n",
    "solo_foraging_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the data\n",
    "pellet_data = foraging_query.fetch()\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each entry in the pellet_data array\n",
    "for entry in pellet_data:\n",
    "    experiment_name = entry[0]\n",
    "    block_start = entry[1]\n",
    "    patch_name = entry[2]\n",
    "    subject_id = entry[3]\n",
    "    pellet_count = entry[4]\n",
    "    pellet_timestamps = entry[5]\n",
    "    patch_threshold =  entry[6]\n",
    "\n",
    "    \n",
    "    # For each pellet timestamp, create a dictionary and append to the list\n",
    "    for pellet_timestamp, threshold in zip(pellet_timestamps, patch_threshold):\n",
    "        data.append({\n",
    "            'time': pellet_timestamp,\n",
    "            'subject_id': subject_id,\n",
    "            'threshold': threshold,\n",
    "            'rank': 'dominant' if subject_id == dominant_id else 'subordinate',\n",
    "        })\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "pellet_df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the 'time' column to datetime\n",
    "pellet_df['time'] = pd.to_datetime(pellet_df['time'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(pellet_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out pellets that are less than 2s after previos pellet for teh subject\n",
    "pellet_df['time_diff'] = pellet_df.groupby('subject_id')['time'].diff()\n",
    "pellet_df['time_diff'] = pellet_df['time_diff'].dt.total_seconds()\n",
    "pellet_df = pellet_df[pellet_df['time_diff'] > 2]\n",
    "pellet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the data\n",
    "solo_pellet_data = solo_foraging_query.fetch()\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each entry in the pellet_data array\n",
    "for entry in solo_pellet_data:\n",
    "    experiment_name = entry[0]\n",
    "    block_start = entry[1]\n",
    "    patch_name = entry[2]\n",
    "    subject_id = entry[3]\n",
    "    pellet_count = entry[4]\n",
    "    pellet_timestamps = entry[5]\n",
    "    patch_threshold =  entry[6]\n",
    "\n",
    "    \n",
    "    # For each pellet timestamp, create a dictionary and append to the list\n",
    "    for pellet_timestamp, threshold in zip(pellet_timestamps, patch_threshold):\n",
    "        data.append({\n",
    "            'time': pellet_timestamp,\n",
    "            'subject_id': subject_id,\n",
    "            'threshold': threshold,\n",
    "            'rank': 'dominant' if subject_id == dominant_id else 'subordinate',\n",
    "        })\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "solo_pellet_df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the 'time' column to datetime\n",
    "solo_pellet_df['time'] = pd.to_datetime(solo_pellet_df['time'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(solo_pellet_df)\n",
    "\n",
    "# filter out pellets that are less than 2s after previos pellet for teh subject\n",
    "solo_pellet_df['time_diff'] = solo_pellet_df.groupby('subject_id')['time'].diff()\n",
    "solo_pellet_df['time_diff'] = solo_pellet_df['time_diff'].dt.total_seconds()\n",
    "solo_pellet_df = solo_pellet_df[solo_pellet_df['time_diff'] > 2]\n",
    "solo_pellet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pellet_df['period'] = 'social'\n",
    "solo_pellet_df['period'] = 'pre_solo' if solo_pellet_df['time'].iloc[0] < pd.Timestamp(pre_solo_end_day) else 'post_solo'\n",
    "all_pellet_df = pd.concat([pellet_df, solo_pellet_df])\n",
    "all_pellet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_raster_plot(df, period, start_time, end_time, start_datetime, end_datetime, id_color_map):\n",
    "    # Create the raster plot\n",
    "    fig = px.scatter(\n",
    "        df[df['period'] == period],\n",
    "        x='time',\n",
    "        y='rank',\n",
    "        color='subject_id',\n",
    "        title=f'Pellet Raster Plot - {period.capitalize()}',\n",
    "        labels={'time': 'Time', 'behavior_type': 'Behavior Type'},\n",
    "        color_discrete_map=id_color_map\n",
    "    )\n",
    "\n",
    "    # Set x-axis limits\n",
    "    fig.update_xaxes(range=[start_time, end_time])  # Example date range\n",
    "\n",
    "    # Iterate over each day in the period\n",
    "    current_day = start_datetime\n",
    "    while current_day < end_datetime:\n",
    "        # Define the start and end times for each period\n",
    "        night_start_time = current_day.replace(hour=int(night_start.split(':')[0]), minute=int(night_start.split(':')[1]))\n",
    "        night_end_time = current_day.replace(hour=int(night_end.split(':')[0]), minute=int(night_end.split(':')[1]))\n",
    "        twilight_start_time = current_day.replace(hour=int(twilight_start.split(':')[0]), minute=int(twilight_start.split(':')[1]))\n",
    "        twilight_end_time = current_day.replace(hour=int(twilight_end.split(':')[0]), minute=int(twilight_end.split(':')[1]))\n",
    "        dawn_start_time = current_day.replace(hour=int(dawn_start.split(':')[0]), minute=int(dawn_start.split(':')[1]))\n",
    "        dawn_end_time = current_day.replace(hour=int(dawn_end.split(':')[0]), minute=int(dawn_end.split(':')[1]))\n",
    "        day_start_time = current_day.replace(hour=int(day_start.split(':')[0]), minute=int(day_start.split(':')[1]))\n",
    "        day_end_time = current_day + timedelta(days=1) if int(day_end.split(':')[0]) < int(day_start.split(':')[0]) else current_day.replace(hour=int(day_end.split(':')[0]), minute=int(day_end.split(':')[1]))\n",
    "        \n",
    "        # Add horizontal lines for light and dark periods\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=twilight_start_time,\n",
    "            x1=twilight_end_time,\n",
    "            y0=-0.5,\n",
    "            y1=-0.5,\n",
    "            line=dict(color=\"gray\", width=4)\n",
    "        )\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=night_start_time,\n",
    "            x1=night_end_time,\n",
    "            y0=-0.5,\n",
    "            y1=-0.5,\n",
    "            line=dict(color=\"black\", width=4)\n",
    "        )\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=dawn_start_time,\n",
    "            x1=dawn_end_time,\n",
    "            y0=-0.5,\n",
    "            y1=-0.5,\n",
    "            line=dict(color=\"gray\", width=4)\n",
    "        )\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=day_start_time,\n",
    "            x1=day_end_time,\n",
    "            y0=-0.5,\n",
    "            y1=-0.5,\n",
    "            line=dict(color=\"white\", width=4)\n",
    "        )\n",
    "        \n",
    "        # Move to the next day\n",
    "        current_day += timedelta(days=1)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "# Define the parameters for each period\n",
    "periods = {\n",
    "    'pre_solo': {\n",
    "        'start_time': pre_solo_start_day,\n",
    "        'end_time': pre_solo_end_day,\n",
    "        'start_datetime': pre_solo_start_datetime,\n",
    "        'end_datetime': pre_solo_end_datetime\n",
    "    },\n",
    "    'post_solo': {\n",
    "        'start_time': post_solo_start_day,\n",
    "        'end_time': post_solo_end_day,\n",
    "        'start_datetime': post_solo_start_datetime,\n",
    "        'end_datetime': post_solo_end_datetime\n",
    "    },\n",
    "    'social': {\n",
    "        'start_time': social_start_time,\n",
    "        'end_time': social_end_time,\n",
    "        'start_datetime': social_start_datetime,\n",
    "        'end_datetime': social_end_datetime\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the raster plot for each period\n",
    "for period, params in periods.items():\n",
    "    create_raster_plot(\n",
    "        all_pellet_df,\n",
    "        period,\n",
    "        params['start_time'],\n",
    "        params['end_time'],\n",
    "        params['start_datetime'],\n",
    "        params['end_datetime'],\n",
    "        id_color_map\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time bin size (e.g., 1 hour)\n",
    "time_bin_size = '1h'\n",
    "# Create a copy of the DataFrame to avoid modifying the original\n",
    "pellet_df_copy = pellet_df.copy()\n",
    "\n",
    "# Set the 'time' as the DataFrame index\n",
    "pellet_df_copy.set_index('time', inplace=True)\n",
    "\n",
    "# Bin the data by time and count the number of pellets per bin per subject\n",
    "binned_pellet_df = pellet_df_copy.groupby([pd.Grouper(freq=time_bin_size), 'subject_id']).size().reset_index(name='pellet_count')\n",
    "\n",
    "# Create a complete time range for the bins\n",
    "time_range = pd.date_range(start=social_start_time, end=social_end_time, freq=time_bin_size)\n",
    "\n",
    "# Get unique subject IDs\n",
    "subject_ids = binned_pellet_df['subject_id'].unique()\n",
    "\n",
    "# Create a MultiIndex with all combinations of time_range and subject_ids\n",
    "multi_index = pd.MultiIndex.from_product([time_range, subject_ids], names=['time', 'subject_id'])\n",
    "\n",
    "# Create an empty DataFrame with the MultiIndex\n",
    "complete_df = pd.DataFrame(index=multi_index).reset_index()\n",
    "\n",
    "# Merge the complete DataFrame with the binned pellet data\n",
    "complete_df = complete_df.merge(binned_pellet_df, on=['time', 'subject_id'], how='left').fillna(0)\n",
    "\n",
    "# Ensure pellet_count is an integer\n",
    "complete_df['pellet_count'] = complete_df['pellet_count'].astype(int)\n",
    "\n",
    "# Create the line plot with resampled and interpolated data\n",
    "fig = px.line(\n",
    "    complete_df,\n",
    "    x='time',\n",
    "    y='pellet_count',\n",
    "    color='subject_id',\n",
    "    title='Pellets during Social Period (1h bins)',\n",
    "    labels={'pellet_count': 'Pellet Count', 'time': 'Time', 'subject_id': 'Subject ID'},    \n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binned_plot(df, period, start_time, end_time, id_color_map, time_bin_size = '1h'):\n",
    "    # Define the time bin size (e.g., 1 hour)\n",
    "    time_bin_size\n",
    "    \n",
    "    # Filter the DataFrame for the given period\n",
    "    period_df = df[(df['time'] >= start_time) & (df['time'] <= end_time)].copy()\n",
    "    \n",
    "    # Set the 'time' as the DataFrame index\n",
    "    period_df.set_index('time', inplace=True)\n",
    "    \n",
    "    # Bin the data by time and count the number of pellets per bin per subject\n",
    "    binned_df = period_df.groupby([pd.Grouper(freq=time_bin_size), 'subject_id']).size().reset_index(name='pellet_count')\n",
    "    \n",
    "    # Create a complete time range for the bins\n",
    "    time_range = pd.date_range(start=start_time, end=end_time, freq=time_bin_size)\n",
    "    \n",
    "    # Get unique subject IDs\n",
    "    subject_ids = binned_df['subject_id'].unique()\n",
    "    \n",
    "    # Create a MultiIndex with all combinations of time_range and subject_ids\n",
    "    multi_index = pd.MultiIndex.from_product([time_range, subject_ids], names=['time', 'subject_id'])\n",
    "    \n",
    "    # Create an empty DataFrame with the MultiIndex\n",
    "    complete_df = pd.DataFrame(index=multi_index).reset_index()\n",
    "    \n",
    "    # Merge the complete DataFrame with the binned pellet data\n",
    "    complete_df = complete_df.merge(binned_df, on=['time', 'subject_id'], how='left').fillna(0)\n",
    "    \n",
    "    # Ensure pellet_count is an integer\n",
    "    complete_df['pellet_count'] = complete_df['pellet_count'].astype(int)\n",
    "    \n",
    "    # Number the bins\n",
    "    complete_df['time_bin'] = complete_df.groupby('subject_id').cumcount() + 1\n",
    "    \n",
    "    # Create the line plot with resampled and interpolated data\n",
    "    fig = px.line(\n",
    "        complete_df,\n",
    "        x='time_bin',\n",
    "        y='pellet_count',\n",
    "        color='subject_id',\n",
    "        title=f'Pellets during {period.capitalize()} Period (1h bins)',\n",
    "        labels={'pellet_count': 'Pellet Count', 'time_bin': 'Time Bin', 'subject_id': 'Subject ID'},\n",
    "        color_discrete_map=id_color_map\n",
    "    )\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "# Define the parameters for each period\n",
    "periods = {\n",
    "    'pre_solo': {\n",
    "        'start_time': pre_solo_start_day,\n",
    "        'end_time': pre_solo_end_day\n",
    "    },\n",
    "    'post_solo': {\n",
    "        'start_time': post_solo_start_day,\n",
    "        'end_time': post_solo_end_day\n",
    "    },\n",
    "    'social': {\n",
    "        'start_time': social_start_time,\n",
    "        'end_time': social_end_time\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the binned plot for each period\n",
    "for period, params in periods.items():\n",
    "    create_binned_plot(\n",
    "        all_pellet_df,\n",
    "        period,\n",
    "        params['start_time'],\n",
    "        params['end_time'],\n",
    "        id_color_map\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for period, params in periods.items():\n",
    "    create_binned_plot(\n",
    "        all_pellet_df,\n",
    "        period,\n",
    "        params['start_time'],\n",
    "        params['end_time'],\n",
    "        id_color_map,\n",
    "        time_bin_size = '24h'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pellet_df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foraging during the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time bin size (e.g., 1 hour)\n",
    "time_bin_size = '24h'\n",
    "# Create a copy of the DataFrame to avoid modifying the original\n",
    "pellet_df_copy = pellet_df.copy()\n",
    "\n",
    "# Set the 'time' as the DataFrame index\n",
    "pellet_df_copy.set_index('time', inplace=True)\n",
    "\n",
    "# Bin the data by time and count the number of pellets per bin per subject\n",
    "binned_pellet_df = pellet_df_copy.groupby([pd.Grouper(freq=time_bin_size), 'subject_id']).size().reset_index(name='pellet_count')\n",
    "\n",
    "# Create a complete time range for the bins\n",
    "time_range = pd.date_range(start=pd.to_datetime(social_start_day) + pd.Timedelta(days=1), end=pd.to_datetime(social_end_day) - pd.Timedelta(days=1), freq=time_bin_size)\n",
    "\n",
    "# Get unique subject IDs\n",
    "subject_ids = binned_pellet_df['subject_id'].unique()\n",
    "\n",
    "# Create a MultiIndex with all combinations of time_range and subject_ids\n",
    "multi_index = pd.MultiIndex.from_product([time_range, subject_ids], names=['time', 'subject_id'])\n",
    "\n",
    "# Create an empty DataFrame with the MultiIndex\n",
    "complete_df = pd.DataFrame(index=multi_index).reset_index()\n",
    "\n",
    "# Merge the complete DataFrame with the binned pellet data\n",
    "complete_df = complete_df.merge(binned_pellet_df, on=['time', 'subject_id'], how='left').fillna(0)\n",
    "\n",
    "# Ensure pellet_count is an integer\n",
    "complete_df['pellet_count'] = complete_df['pellet_count'].astype(int)\n",
    "\n",
    "# Create the line plot with resampled and interpolated data\n",
    "fig = px.line(\n",
    "    complete_df,\n",
    "    x='time',\n",
    "    y='pellet_count',\n",
    "    color='subject_id',\n",
    "    title='Pellets during Social Period (24h bins)',\n",
    "    labels={'pellet_count': 'Pellet Count', 'time': 'Time', 'subject_id': 'Subject ID'},    \n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pellet threshold distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the time bin size (e.g., 24 hours)\n",
    "time_bin_size = '24h'\n",
    "\n",
    "# Create a copy of the DataFrame to avoid modifying the original\n",
    "pellet_df_copy = pellet_df.copy()\n",
    "\n",
    "# Set the 'time' as the DataFrame index\n",
    "pellet_df_copy.set_index('time', inplace=True)\n",
    "\n",
    "# Bin the data by time and calculate the mean, standard deviation, and SEM of the threshold per bin per subject\n",
    "binned_threshold_df = pellet_df_copy.groupby([pd.Grouper(freq=time_bin_size), 'subject_id'])['threshold'].agg(['mean', 'std', 'count']).reset_index()\n",
    "binned_threshold_df.rename(columns={'mean': 'average_threshold', 'std': 'std_threshold', 'count': 'n'}, inplace=True)\n",
    "\n",
    "# Calculate the standard error of the mean (SEM)\n",
    "binned_threshold_df['sem_threshold'] = binned_threshold_df['std_threshold'] / np.sqrt(binned_threshold_df['n'])\n",
    "\n",
    "# Create a complete time range for the bins\n",
    "time_range = pd.date_range(start=pd.to_datetime(social_start_day) + pd.Timedelta(days=1), end=pd.to_datetime(social_end_day) - pd.Timedelta(days=1), freq=time_bin_size)\n",
    "\n",
    "# Get unique subject IDs\n",
    "subject_ids = binned_threshold_df['subject_id'].unique()\n",
    "\n",
    "# Create a MultiIndex with all combinations of time_range and subject_ids\n",
    "multi_index = pd.MultiIndex.from_product([time_range, subject_ids], names=['time', 'subject_id'])\n",
    "\n",
    "# Create an empty DataFrame with the MultiIndex\n",
    "complete_df = pd.DataFrame(index=multi_index).reset_index()\n",
    "\n",
    "# Merge the complete DataFrame with the binned threshold data\n",
    "complete_df = complete_df.merge(binned_threshold_df, on=['time', 'subject_id'], how='left').fillna(0)\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the line plot with SEM error bars for each subject\n",
    "for subject_id in complete_df['subject_id'].unique():\n",
    "    subject_df = complete_df[complete_df['subject_id'] == subject_id]\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=subject_df['time'],\n",
    "        y=subject_df['average_threshold'],\n",
    "        mode='lines+markers',\n",
    "        name=f'{subject_id} Average Threshold',\n",
    "        line=dict(color=id_color_map[subject_id]),\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            array=subject_df['sem_threshold'],  # Error bar data (standard error of the mean)\n",
    "            visible=True\n",
    "        )\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Average Threshold with SEM Error Bars during Social Period (24h bins)',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Average Threshold',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foraging during the day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time bin size (e.g., 1 hour)\n",
    "time_bin_size = '1h'\n",
    "# Create a copy of the DataFrame to avoid modifying the original\n",
    "pellet_df_copy = pellet_df.copy()\n",
    "\n",
    "# Set the 'time' as the DataFrame index\n",
    "pellet_df_copy.set_index('time', inplace=True)\n",
    "\n",
    "# Bin the data by time and count the number of pellets per bin per subject\n",
    "binned_pellet_df = pellet_df_copy.groupby([pd.Grouper(freq=time_bin_size), 'subject_id']).size().reset_index(name='pellet_count')\n",
    "\n",
    "# Create a complete time range for the bins\n",
    "time_range = pd.date_range(start=social_start_time, end=social_end_time, freq=time_bin_size)\n",
    "\n",
    "# Get unique subject IDs\n",
    "subject_ids = binned_pellet_df['subject_id'].unique()\n",
    "\n",
    "# Create a MultiIndex with all combinations of time_range and subject_ids\n",
    "multi_index = pd.MultiIndex.from_product([time_range, subject_ids], names=['time', 'subject_id'])\n",
    "\n",
    "# Create an empty DataFrame with the MultiIndex\n",
    "complete_df = pd.DataFrame(index=multi_index).reset_index()\n",
    "\n",
    "# Merge the complete DataFrame with the binned pellet data\n",
    "complete_df = complete_df.merge(binned_pellet_df, on=['time', 'subject_id'], how='left').fillna(0)\n",
    "\n",
    "# Ensure pellet_count is an integer\n",
    "complete_df['pellet_count'] = complete_df['pellet_count'].astype(int)\n",
    "\n",
    "# Reset the index to get 'timestamp' back as a column\n",
    "complete_df.reset_index(inplace=True)\n",
    "\n",
    "# Calculate the average number of pellets per hour of day for each mouse\n",
    "complete_df['hour_of_day'] = complete_df['time'].dt.hour\n",
    "\n",
    "average_pellets_per_hour = complete_df.groupby(['subject_id', 'hour_of_day'])['pellet_count'].mean().reset_index()\n",
    "\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add individual daily data traces with faint lines\n",
    "for subject_id in complete_df['subject_id'].unique():\n",
    "    subject_data = complete_df[complete_df['subject_id'] == subject_id]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=subject_data['hour_of_day'],\n",
    "        y=subject_data['pellet_count'],\n",
    "        mode='lines',\n",
    "        name=f'{subject_id} (daily)',\n",
    "        line=dict(color=id_color_map[subject_id], width=1),\n",
    "        opacity=0.2\n",
    "    ))\n",
    "\n",
    "# Add the average data with more prominent lines\n",
    "for subject_id in average_pellets_per_hour['subject_id'].unique():\n",
    "    avg_data = average_pellets_per_hour[average_pellets_per_hour['subject_id'] == subject_id]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=avg_data['hour_of_day'],\n",
    "        y=avg_data['pellet_count'],\n",
    "        mode='lines',\n",
    "        name=f'{subject_id} (average)',\n",
    "        line=dict(color=id_color_map[subject_id], width=2)\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Pellets throughout the day',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Pellet Count',\n",
    "    legend_title='Subject ID'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch preference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_query = (\n",
    "    BlockSubjectAnalysis.Patch.proj('pellet_count', 'pellet_timestamps', 'patch_threshold')\n",
    "    & key\n",
    "    & f'block_start >= \"{social_start_day}\"'\n",
    "    & f'block_start <= \"{social_end_day}\"'\n",
    ")\n",
    "preference_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pellet data\n",
    "preference_query = (\n",
    "    BlockSubjectAnalysis.Patch.proj('pellet_count', 'pellet_timestamps', 'patch_threshold')\n",
    "    * BlockSubjectAnalysis.Preference.proj('final_preference_by_wheel','final_preference_by_time')\n",
    "    * BlockAnalysis.Patch.proj('patch_rate', 'patch_offset')\n",
    "    & key\n",
    "    & f'block_start >= \"{social_start_day}\"'\n",
    "    & f'block_start <= \"{social_end_day}\"'\n",
    ")\n",
    "preference_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_data = preference_query.fetch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the data\n",
    "preference_list = []\n",
    "\n",
    "# Loop through each entry in the pellet_data array\n",
    "for entry in preference_data:\n",
    "    experiment_name = entry[0]\n",
    "    block_start = entry[1]\n",
    "    patch_name = entry[2]\n",
    "    subject_id = entry[3]\n",
    "    pellet_count = entry[4]\n",
    "    #pellet_timestamps = entry[5]\n",
    "    #patch_threshold = entry[6]\n",
    "    final_preference_by_wheel = entry[7]\n",
    "    final_preference_by_time = entry[8]\n",
    "    patch_rate = entry[9]\n",
    "    patch_offset = entry[10]\n",
    "    rank = 'dominant' if subject_id == dominant_id else 'subordinate'\n",
    "    patch_type = 'easy' if patch_rate == 0.01 else 'medium' if patch_rate == 0.0033 else 'hard' if patch_rate == 0.002 else 'unknown'\n",
    "\n",
    "# Create a dictionary for the current entry\n",
    "    data_dict = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'block_start': block_start,\n",
    "        'patch_name': patch_name,\n",
    "        'subject_id': subject_id,\n",
    "        'pellet_count': pellet_count,\n",
    "        'final_preference_by_wheel': final_preference_by_wheel,\n",
    "        'final_preference_by_time': final_preference_by_time,\n",
    "        'patch_rate': patch_rate,\n",
    "        'patch_offset': patch_offset,\n",
    "        'rank': rank,\n",
    "        'patch_type': patch_type\n",
    "    }\n",
    "    \n",
    "    # Append the dictionary to the list\n",
    "    preference_list.append(data_dict)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "preference_df = pd.DataFrame(preference_list)\n",
    "\n",
    "# Convert the 'block_start' column to datetime\n",
    "preference_df['block_start'] = pd.to_datetime(preference_df['block_start'])\n",
    "\n",
    "# Display the DataFrame\n",
    "preference_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for blocks where both subjects got above the threshold pellets overall\n",
    "pellet_threshold = 10\n",
    "\n",
    "# Group by block_start, patch_name, and subject_id and calculate the sum of pellet_count\n",
    "grouped_df = preference_df.groupby(['block_start', 'patch_name', 'subject_id']).agg({'pellet_count': 'sum'}).reset_index()\n",
    "\n",
    "# Filter the groups where the sum of pellet_count is above the threshold\n",
    "filtered_groups = grouped_df[grouped_df['pellet_count'] >= pellet_threshold]\n",
    "\n",
    "# Get unique values for block_start, patch_name, and subject_id\n",
    "unique_blocks = filtered_groups['block_start'].unique()\n",
    "unique_patches = preference_df['patch_name'].unique()\n",
    "unique_subjects = preference_df['subject_id'].unique()\n",
    "\n",
    "# Create a DataFrame with all possible combinations of block_start, patch_name, and subject_id\n",
    "all_combinations = pd.MultiIndex.from_product([unique_blocks, unique_patches, unique_subjects], names=['block_start', 'patch_name', 'subject_id']).to_frame(index=False)\n",
    "\n",
    "# Merge the all_combinations DataFrame with the original DataFrame to retain all columns\n",
    "preference_df_filtered = all_combinations.merge(preference_df, on=['block_start', 'patch_name', 'subject_id'], how='left').fillna(0)\n",
    "\n",
    "preference_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define category order for the x-axis\n",
    "category_order = ['easy', 'medium', 'hard']\n",
    "\n",
    "# Initialize the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add box plot for each subject\n",
    "for subject_id in preference_df_filtered['subject_id'].unique():\n",
    "    subject_df = preference_df_filtered[preference_df_filtered['subject_id'] == subject_id]\n",
    "    fig.add_trace(go.Box(\n",
    "        x=subject_df['patch_type'],\n",
    "        y=subject_df['pellet_count'],\n",
    "        name=subject_id,\n",
    "        boxpoints='all',  # Show indiv points\n",
    "        jitter=0.25,  # Add some jitter for individual points\n",
    "        pointpos=-1.4,  # Position of the individual points (to the right)\n",
    "        marker=dict(color=id_color_map[subject_id])\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Pellets per Patch Rate',\n",
    "    xaxis_title='Patch Rate',\n",
    "    yaxis_title='Pellet Count',\n",
    "    boxmode='group',  # Group box plots by patch_type\n",
    "    xaxis=dict(categoryorder='array', categoryarray=category_order),  \n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define category order for the x-axis\n",
    "category_order = ['easy', 'medium', 'hard']\n",
    "\n",
    "# Initialize the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add box plot for each subject\n",
    "for subject_id in preference_df_filtered['subject_id'].unique():\n",
    "    subject_df = preference_df_filtered[preference_df_filtered['subject_id'] == subject_id]\n",
    "    fig.add_trace(go.Box(\n",
    "        x=subject_df['patch_type'],\n",
    "        y=subject_df['final_preference_by_time'],\n",
    "        name=subject_id,\n",
    "        boxpoints='all',  # Show indiv points\n",
    "        jitter=0.25,  # Add some jitter for individual points\n",
    "        pointpos=-1.4,  # Position of the individual points (to the right)\n",
    "        marker=dict(color=id_color_map[subject_id])\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Preference Index per Patch Rate',\n",
    "    xaxis_title='Patch Rate',\n",
    "    yaxis_title='Preference Index (time)',\n",
    "    boxmode='group',  # Group box plots by patch_type\n",
    "    xaxis=dict(categoryorder='array', categoryarray=category_order),  \n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define category order for the x-axis\n",
    "category_order = ['easy', 'medium', 'hard']\n",
    "\n",
    "# Initialize the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add box plot for each subject\n",
    "for subject_id in preference_df_filtered['subject_id'].unique():\n",
    "    subject_df = preference_df_filtered[preference_df_filtered['subject_id'] == subject_id]\n",
    "    fig.add_trace(go.Box(\n",
    "        x=subject_df['patch_type'],\n",
    "        y=subject_df['final_preference_by_wheel'],\n",
    "        name=subject_id,\n",
    "        boxpoints='all',  # Show indiv points\n",
    "        jitter=0.25,  # Add some jitter for individual points\n",
    "        pointpos=-1.4,  # Position of the individual points (to the right)\n",
    "        marker=dict(color=id_color_map[subject_id])\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Preference Index per Patch Rate',\n",
    "    xaxis_title='Patch Rate',\n",
    "    yaxis_title='Preference Index (wheel)',\n",
    "    boxmode='group',  # Group box plots by patch_type\n",
    "    xaxis=dict(categoryorder='array', categoryarray=category_order),  \n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure block_start is a datetime object\n",
    "preference_df_filtered['block_start'] = pd.to_datetime(preference_df_filtered['block_start'])\n",
    "\n",
    "# Group by day, subject_id, and patch_type, and calculate the average final_preference_by_wheel\n",
    "preference_df_filtered['day'] = preference_df_filtered['block_start'].dt.date\n",
    "daily_avg_df = preference_df_filtered.groupby(['day', 'subject_id', 'patch_type']).agg({'final_preference_by_wheel': 'mean'}).reset_index()\n",
    "\n",
    "# Exclude incomplete days\n",
    "first_day = daily_avg_df['day'].min()\n",
    "last_day = daily_avg_df['day'].max()\n",
    "filtered_daily_avg_df = daily_avg_df[(daily_avg_df['day'] != first_day) & (daily_avg_df['day'] != last_day)]\n",
    "\n",
    "# Create the plot\n",
    "fig = px.line(\n",
    "    filtered_daily_avg_df,\n",
    "    x='day',\n",
    "    y='final_preference_by_wheel',\n",
    "    color='subject_id',\n",
    "    line_dash='patch_type',\n",
    "    title='Average Daily Preference Index by Day',\n",
    "    labels={'day': 'Day', 'final_preference_by_wheel': 'Average Preference Index'},\n",
    "    category_orders={'patch_type': ['easy', 'medium', 'hard']},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure block_start is a datetime object\n",
    "preference_df_filtered['block_start'] = pd.to_datetime(preference_df_filtered['block_start'])\n",
    "\n",
    "# Group by day, subject_id, and patch_type, and calculate the average final_preference_by_wheel\n",
    "preference_df_filtered['day'] = preference_df_filtered['block_start'].dt.date\n",
    "daily_avg_df = preference_df_filtered.groupby(['day', 'subject_id', 'patch_type']).agg({'pellet_count': 'mean'}).reset_index()\n",
    "\n",
    "# Exclude incomplete days\n",
    "first_day = daily_avg_df['day'].min()\n",
    "last_day = daily_avg_df['day'].max()\n",
    "filtered_daily_avg_df = daily_avg_df[(daily_avg_df['day'] != first_day) & (daily_avg_df['day'] != last_day)]\n",
    "\n",
    "# Create the plot\n",
    "fig = px.line(\n",
    "    filtered_daily_avg_df,\n",
    "    x='day',\n",
    "    y='pellet_count',\n",
    "    color='subject_id',\n",
    "    line_dash='patch_type',\n",
    "    title='Average Daily Pellets by Day',\n",
    "    labels={'day': 'Day', 'pellet_count': 'Pellets'},\n",
    "    category_orders={'patch_type': ['easy', 'medium', 'hard']},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foraging_query = (\n",
    "    BlockAnalysis.Subject.proj('weights', 'weight_timestamps')\n",
    "    & {\"spinnaker_video_source_name\": \"CameraTop\"} #this is the video source name which we rstrict once we selected tuff to keep in table\n",
    "    & key\n",
    "    & f'block_start >= \"{social_start_day}\"'\n",
    "    & f'block_start <= \"{social_end_day}\"'\n",
    ")\n",
    "foraging_query\n",
    "weight_data =  foraging_query.fetch()\n",
    "type(weight_data)\n",
    "weight_data\n",
    "weight_data[0][3]\n",
    "weight_data[0][4]\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each entry in the weight_data array\n",
    "for entry in weight_data:\n",
    "    subject_name = entry[0]\n",
    "    block_start = entry[1]\n",
    "    subject_id = entry[2]\n",
    "    weights = entry[3]\n",
    "    weight_timestamps = entry[4]\n",
    "    \n",
    "    # For each weight measurement, create a dictionary and append to the list\n",
    "    for time, weight in zip(weight_timestamps, weights):\n",
    "        data.append({'time': time, 'weight': weight, 'subject_id': subject_id})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "weight_df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the 'time' column to datetime\n",
    "weight_df['time'] = pd.to_datetime(weight_df['time'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(weight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning: filter out measurements below 25g\n",
    "weight_df = weight_df[weight_df['weight'] > 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the line plot\n",
    "fig = px.line(\n",
    "    weight_df,\n",
    "    x='time',\n",
    "    y='weight',\n",
    "    color='subject_id',\n",
    "    title='Weight during social period',\n",
    "    labels={'weight': 'Weight (g)', 'time': 'Time', 'subject_id': 'Subject ID'} ,\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: check raw data\n",
    "# Set the 'time' column as the index\n",
    "time_weight_df = weight_df.set_index('time', inplace=False)\n",
    "\n",
    "# Resample the data into hourly bins and calculate the average weight for each bin\n",
    "avg_weight_df = time_weight_df.groupby('subject_id').resample('24h').mean()\n",
    "#print(avg_weight_df)\n",
    "\n",
    "# Reset the index to use 'time' as a column again\n",
    "avg_weight_df.reset_index(inplace=True)\n",
    "\n",
    "# Create the line plot with resampled and interpolated data\n",
    "fig = px.line(\n",
    "    avg_weight_df,\n",
    "    x='time',\n",
    "    y='weight',\n",
    "    color='subject_id',\n",
    "    title='Average Weight during Social Period (Daily)',\n",
    "    labels={'weight': 'Weight (g)', 'time': 'Time', 'subject_id': 'Subject ID'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaways:\n",
    "- dominant and subordinate mouse forage similar amounts, daily pattern similar, preference similar\n",
    "TODO: who starts forgaing bouts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Social interactions and foraging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Do social events influence foraging behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pellet counts after social interactions vs random times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting interactions not folowed by other interactions\n",
    "time_window_length = 5  # in minutes\n",
    "\n",
    "# Initialize a DataFrame to store good chases\n",
    "good_events_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each row in chasing_df\n",
    "for index, event in combined_df.iterrows():\n",
    "    event_time = event['start_timestamp']\n",
    "\n",
    "    # Calculate the start and end time of the time window\n",
    "    start_time = event_time\n",
    "    end_time = event_time + pd.Timedelta(minutes=time_window_length)\n",
    "    \n",
    "    # Filter out events that happened within the time window\n",
    "    events_in_time_window = combined_df[\n",
    "        (combined_df['start_timestamp'] >= start_time) & \n",
    "        (combined_df['start_timestamp'] <= end_time) & \n",
    "        (combined_df.index != index)  # Exclude the current event\n",
    "    ]\n",
    "    \n",
    "    # If no events are found in the time window, add the chase to good_chases_df\n",
    "    if events_in_time_window.empty:\n",
    "        good_events_df = pd.concat([good_events_df, event.to_frame().T])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_events_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get active times that are not sleeping:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only do this in good forgating blocks, where at least 5 pellets obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foraging_query = (\n",
    "    BlockSubjectAnalysis.Patch.proj('pellet_count', 'pellet_timestamps')\n",
    "    & {\"spinnaker_video_source_name\": \"CameraTop\"} #this is the video source name which we rstrict once we selected tuff to keep in table\n",
    "    & key\n",
    "    & f'block_start >= \"{social_start_day}\"'\n",
    "    & f'block_start <= \"{social_end_day}\"'\n",
    ")\n",
    "pellet_data = foraging_query.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the data\n",
    "pellet_data = foraging_query.fetch()\n",
    "\n",
    "# Convert the fetched data into a DataFrame\n",
    "pellet_df = pd.DataFrame(pellet_data, columns=['experiment_name', 'block_start', 'patch_name', 'subject_id', 'pellet_count', 'pellet_timestamps'])\n",
    "\n",
    "# Group by block_start and sum the pellet_count\n",
    "grouped_pellet_df = pellet_df.groupby('block_start')['pellet_count'].sum().reset_index()\n",
    "\n",
    "# Filter blocks where the total pellet count is greater than 3\n",
    "good_blocks_df = grouped_pellet_df[grouped_pellet_df['pellet_count'] > 15]\n",
    "\n",
    "# Extract block_start values as a list\n",
    "good_block_starts = good_blocks_df['block_start'].tolist()\n",
    "\n",
    "# Convert block_start timestamps to strings in the correct format\n",
    "good_block_starts_str = [block_start.strftime('%Y-%m-%d %H:%M:%S.%f') for block_start in good_block_starts]\n",
    "print(len(good_block_starts_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string that matches SQL's IN clause format\n",
    "block_starts_sql_list = ', '.join(f'\"{block_start}\"' for block_start in good_block_starts_str)\n",
    "len(block_starts_sql_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict block_subjects query to only include blocks in good_block_starts\n",
    "block_subjects = (\n",
    "    BlockAnalysis.Subject.proj('position_x', 'position_y', 'position_timestamps')\n",
    "    & key\n",
    "    & f'block_start >= \"{social_start_day}\"'\n",
    "    & f'block_start <= \"{social_end_day}\"'\n",
    "    & f'block_start IN ({block_starts_sql_list})'   # Restrict to block_start values in good_blocks_df\n",
    ")\n",
    "block_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get posiiton data for one block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do only for one example block\n",
    "block_restriction = {'block_start':'2024-02-20 10:34:17.001984'}\n",
    "\n",
    "#get centoid df\n",
    "block_subjects = (\n",
    "    BlockAnalysis.Subject.proj('position_x', 'position_y', 'position_timestamps')\n",
    "    & key\n",
    "    & f'block_start >= \"{social_start_day}\"'\n",
    "    & f'block_start <= \"{social_end_day}\"'\n",
    "    & block_restriction\n",
    ")\n",
    "block_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_subjects_dict = block_subjects.fetch(as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct subject position dataframe\n",
    "subjects_positions_df = pd.DataFrame\n",
    "subjects_positions_df = pd.concat(\n",
    "    #the folllwing list comperhension makes df for each subject\n",
    "    [\n",
    "        pd.DataFrame(  #make df from dict for both subjects\n",
    "            {\"subject_name\": [s[\"subject_name\"]] * len(s[\"position_timestamps\"])}  #make subject_name colunm filled iwth name for all tie steps\n",
    "            | { #merge dicts, start dict complehension\n",
    "                k: s[k] #for each key k get corrsponding list from s subject\n",
    "                for k in ( #iterate ove these keys\n",
    "                    \"position_timestamps\",\n",
    "                    \"position_x\",\n",
    "                    \"position_y\",\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        for s in block_subjects_dict\n",
    "    ]\n",
    ")\n",
    "#subjects_positions_df.set_index(\"position_timestamps\", inplace=True) #make timestamps the row labels\n",
    "subjects_positions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes both tracks ae assinged to same id\n",
    "# if one subject_id has two tracks for the same tiemstamps and the other subjectid as no track for that timestmp, assign hte second value to the other subjectid\n",
    "\n",
    "# Sample DataFrames for demonstration\n",
    "# Replace these with your actual DataFrames\n",
    "positions_subject0 = subjects_positions_df[subjects_positions_df['subject_name'] == 'BAA-1104045']\n",
    "positions_subject1 = subjects_positions_df[subjects_positions_df['subject_name'] == 'BAA-1104047']\n",
    "\n",
    "# Identify timestamps with multiple entries for each subject\n",
    "count_subject0 = positions_subject0['position_timestamps'].value_counts()\n",
    "count_subject1 = positions_subject1['position_timestamps'].value_counts()\n",
    "\n",
    "# Identify timestamps where one subject has multiple entries and the other has none\n",
    "duplicate_subject0 = count_subject0[count_subject0 > 1].index\n",
    "missing_subject1 = duplicate_subject0.difference(count_subject1.index)\n",
    "\n",
    "duplicate_subject1 = count_subject1[count_subject1 > 1].index\n",
    "missing_subject0 = duplicate_subject1.difference(count_subject0.index)\n",
    "\n",
    "# Initialize lists to hold the new rows for each subject\n",
    "new_rows_subject0 = []\n",
    "new_rows_subject1 = []\n",
    "\n",
    "# For timestamps where subject0 has duplicates and subject1 has none\n",
    "for timestamp in missing_subject1:\n",
    "    duplicate_rows = positions_subject0[positions_subject0['position_timestamps'] == timestamp]\n",
    "    if len(duplicate_rows) > 1:\n",
    "        # Reassign every second row to subject1\n",
    "        extra_rows = duplicate_rows.iloc[1:]\n",
    "        positions_subject0.loc[extra_rows.index, 'subject_name'] = 'BAA-1104047'\n",
    "\n",
    "# For timestamps where subject1 has duplicates and subject0 has none\n",
    "for timestamp in missing_subject0:\n",
    "    duplicate_rows = positions_subject1[positions_subject1['position_timestamps'] == timestamp]\n",
    "    if len(duplicate_rows) > 1:\n",
    "        # Reassign every second row to subject0\n",
    "        extra_rows = duplicate_rows.iloc[1:]\n",
    "        positions_subject1.loc[extra_rows.index, 'subject_name'] = 'BAA-1104045'\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "new_rows_subject0_df = pd.DataFrame(new_rows_subject0)\n",
    "new_rows_subject1_df = pd.DataFrame(new_rows_subject1)\n",
    "\n",
    "# Combine the original DataFrames with the new rows\n",
    "corrected_positions_subject0 = pd.concat([positions_subject0, new_rows_subject0_df], ignore_index=True)\n",
    "corrected_positions_subject1 = pd.concat([positions_subject1, new_rows_subject1_df], ignore_index=True)\n",
    "\n",
    "# Combine the corrected DataFrames\n",
    "corrected_positions_df = pd.concat([corrected_positions_subject0, corrected_positions_subject1])\n",
    "\n",
    "# Sort by timestamp to ensure order\n",
    "corrected_positions_df = corrected_positions_df.sort_values(by='position_timestamps').reset_index(drop=True)\n",
    "\n",
    "# Display the corrected DataFrame\n",
    "print(corrected_positions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of timestmps with one subject missing\n",
    "# Convert timestamps to datetime if not already\n",
    "corrected_positions_df['position_timestamps'] = pd.to_datetime(corrected_positions_df['position_timestamps'])\n",
    "\n",
    "# Identify unique timestamps for each subject\n",
    "timestamps_subjects = corrected_positions_df.groupby('subject_name')['position_timestamps'].apply(set)\n",
    "\n",
    "# Find common timestamps where both subjects have data\n",
    "common_timestamps = set(timestamps_subjects['BAA-1104045']).intersection(set(timestamps_subjects['BAA-1104047']))\n",
    "\n",
    "# Filter rows to keep only those with timestamps in common_timestamps\n",
    "corrected_positions_df = corrected_positions_df[corrected_positions_df['position_timestamps'].isin(common_timestamps)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(corrected_positions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide into consecutive chunks of timetamps\n",
    "# Sort the DataFrame by 'position_timestamps'\n",
    "corrected_positions_df = corrected_positions_df.sort_values('position_timestamps')\n",
    "\n",
    "# Calculate time differences between consecutive rows\n",
    "corrected_positions_df['time_diff'] = corrected_positions_df['position_timestamps'].diff()\n",
    "\n",
    "# Define maximum allowed gap\n",
    "max_gap = pd.Timedelta(seconds=0.5)\n",
    "\n",
    "# Identify chunks based on the time difference\n",
    "corrected_positions_df['chunk'] = (corrected_positions_df['time_diff'] > max_gap).cumsum()\n",
    "\n",
    "# Group by 'chunk' to process each chunk separately\n",
    "chunks = corrected_positions_df.groupby('chunk')\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect id swaps\n",
    "# Initialize a list to hold detected swaps\n",
    "all_swaps = []\n",
    "\n",
    "# Process each chunk to detect swaps\n",
    "for chunk_id, chunk_df in chunks:\n",
    "    # Extract positions for each subject\n",
    "    positions_subject0 = chunk_df[chunk_df['subject_name'] == 'BAA-1104045'][['position_x', 'position_y']].values\n",
    "    positions_subject1 = chunk_df[chunk_df['subject_name'] == 'BAA-1104047'][['position_x', 'position_y']].values\n",
    "\n",
    "    # Initialize last known positions\n",
    "    last_known_pos0 = positions_subject0[0]\n",
    "    last_known_pos1 = positions_subject1[0]\n",
    "    assert len(positions_subject0) == len(positions_subject1)\n",
    "    \n",
    "    # Loop over the frames\n",
    "    for i in range(1, len(positions_subject0)):\n",
    "        # Calculate Euclidean distances\n",
    "        dists = np.zeros((2, 2))\n",
    "        dists[0, 0] = np.sqrt(np.sum((positions_subject0[i] - last_known_pos0)**2))\n",
    "        dists[0, 1] = np.sqrt(np.sum((positions_subject0[i] - last_known_pos1)**2))\n",
    "        dists[1, 0] = np.sqrt(np.sum((positions_subject1[i] - last_known_pos0)**2))\n",
    "        dists[1, 1] = np.sqrt(np.sum((positions_subject1[i] - last_known_pos1)**2))\n",
    "\n",
    "        # Check for swaps\n",
    "        if dists[0, 0] + dists[1, 1] <= dists[0, 1] + dists[1, 0]:\n",
    "            last_known_pos0 = positions_subject0[i]\n",
    "            last_known_pos1 = positions_subject1[i]\n",
    "        else:\n",
    "            last_known_pos0 = positions_subject1[i]\n",
    "            last_known_pos1 = positions_subject0[i]\n",
    "            swap_timestamp = chunk_df.iloc[i]['position_timestamps']\n",
    "            if swap_timestamp not in all_swaps:\n",
    "                all_swaps.append(swap_timestamp)\n",
    "\n",
    "# Display the detected swaps\n",
    "print(f\"Detected ID swaps: {len(all_swaps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot x position of both subjects with id swaps overlaid\n",
    "positions_subject0 = corrected_positions_df[corrected_positions_df['subject_name'] == 'BAA-1104045']\n",
    "positions_subject1 = corrected_positions_df[corrected_positions_df['subject_name'] == 'BAA-1104047']\n",
    "\n",
    "# Initialize the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add x positions for Subject 0\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=positions_subject0['position_timestamps'],\n",
    "    y=positions_subject0['position_x'],\n",
    "    mode='lines',\n",
    "    name='Subject BAA-1104045',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "# Add x positions for Subject 1\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=positions_subject1['position_timestamps'],\n",
    "    y=positions_subject1['position_x'],\n",
    "    mode='lines',\n",
    "    name='Subject BAA-1104047',\n",
    "    line=dict(color='red')\n",
    "))\n",
    "\n",
    "# Add ID swap markers\n",
    "swap_y_value = positions_subject0['position_x'].max() if not positions_subject0.empty else 0\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=all_swaps,\n",
    "    y=[swap_y_value] * len(all_swaps),\n",
    "    mode='markers',\n",
    "    marker=dict(color='green', symbol='x', size=10),\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='X Position of Both Subjects with ID Swaps Overlaid',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='X Position (cm)',\n",
    "    legend_title='Subjects',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of swap timestamps to a set for faster lookup\n",
    "swap_timestamps_set = set(all_swaps)\n",
    "\n",
    "# Create a copy of the DataFrame to avoid modifying the original\n",
    "corrected_positions_swapped_df = corrected_positions_df.copy()\n",
    "\n",
    "# Iterate through the DataFrame and swap subject names where timestamps match\n",
    "for index, row in corrected_positions_swapped_df.iterrows():\n",
    "    if row['position_timestamps'] in swap_timestamps_set:\n",
    "        if row['subject_name'] == 'BAA-1104045':\n",
    "            corrected_positions_swapped_df.at[index, 'subject_name'] = 'BAA-1104047'\n",
    "        elif row['subject_name'] == 'BAA-1104047':\n",
    "            corrected_positions_swapped_df.at[index, 'subject_name'] = 'BAA-1104045'\n",
    "\n",
    "print(corrected_positions_swapped_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot x position of both subjects with id swaps overlaid\n",
    "positions_subject0 = corrected_positions_swapped_df[corrected_positions_swapped_df['subject_name'] == 'BAA-1104045']\n",
    "positions_subject1 = corrected_positions_swapped_df[corrected_positions_swapped_df['subject_name'] == 'BAA-1104047']\n",
    "\n",
    "# Initialize the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add x positions for Subject 0\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=positions_subject0['position_timestamps'],\n",
    "    y=positions_subject0['position_x'],\n",
    "    mode='lines',\n",
    "    name='Subject BAA-1104045',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "# Add x positions for Subject 1\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=positions_subject1['position_timestamps'],\n",
    "    y=positions_subject1['position_x'],\n",
    "    mode='lines',\n",
    "    name='Subject BAA-1104047',\n",
    "    line=dict(color='red')\n",
    "))\n",
    "\n",
    "# Add ID swap markers\n",
    "swap_y_value = positions_subject0['position_x'].max() if not positions_subject0.empty else 0\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=all_swaps,\n",
    "    y=[swap_y_value] * len(all_swaps),\n",
    "    mode='markers',\n",
    "    marker=dict(color='green', symbol='x', size=10),\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='X Position of Both Subjects with ID Swaps Overlaid',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='X Position (cm)',\n",
    "    legend_title='Subjects',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the nest region as a polygon\n",
    "nest_corners = metadata.ActiveRegion.NestRegion.ArrayOfPoint\n",
    "nest_polygon = Polygon([\n",
    "    (int(nest_corners[0][\"X\"]), int(nest_corners[0][\"Y\"])),\n",
    "    (int(nest_corners[1][\"X\"]), int(nest_corners[1][\"Y\"])),\n",
    "    (int(nest_corners[2][\"X\"]), int(nest_corners[2][\"Y\"])),\n",
    "    (int(nest_corners[3][\"X\"]), int(nest_corners[3][\"Y\"]))\n",
    "])\n",
    "\n",
    "# Function to check if a point is within the nest polygon\n",
    "def is_in_nest(x, y):\n",
    "    point = Point(x, y)\n",
    "    return nest_polygon.contains(point)\n",
    "\n",
    "# Apply the function to filter the DataFrame\n",
    "in_nest_position_df = corrected_positions_swapped_df[\n",
    "    corrected_positions_swapped_df.apply(lambda row: is_in_nest(row[\"position_x\"], row[\"position_y\"]), axis=1)\n",
    "]\n",
    "# Reset the index to make 'position_timestamps' a column\n",
    "in_nest_position_df = in_nest_position_df.reset_index()\n",
    "in_nest_position_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only compute speed if consecutive timestamps are between\n",
    "min_time_diff = 0.002\n",
    "max_time_diff = 5\n",
    "\n",
    "# Compute the time differences in seconds\n",
    "time_diffs = in_nest_position_df.reset_index().groupby(\"subject_name\")[\"position_timestamps\"].diff().dt.total_seconds()\n",
    "\n",
    "# Compute speed\n",
    "in_nest_position_df[\"speed\"] = (\n",
    "    in_nest_position_df.groupby(\"subject_name\")[[\"position_x\", \"position_y\"]].diff().apply(np.linalg.norm, axis=1)\n",
    "    / time_diffs\n",
    ")\n",
    "\n",
    "# Set speed to NaN where time difference is greater than 5 seconds\n",
    "in_nest_position_df.loc[time_diffs > max_time_diff, \"speed\"] = np.nan\n",
    "in_nest_position_df.loc[time_diffs < min_time_diff, \"speed\"] = np.nan\n",
    "\n",
    "in_nest_position_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard inf value - errors in tracking\n",
    "# Identify timestamps with inf values in the speed column\n",
    "in_nest_position_df.set_index('position_timestamps', inplace=True)\n",
    "inf_timestamps = in_nest_position_df[in_nest_position_df['speed'].isin([np.inf, -np.inf])][['subject_name', 'speed']].reset_index()\n",
    "# Get the subject-timestamp pairs where speed is inf\n",
    "inf_pairs = inf_timestamps[['subject_name', 'position_timestamps']].drop_duplicates()\n",
    "# Merge to filter out the rows with inf subject-timestamp pairs\n",
    "cleaned_df = in_nest_position_df.merge(inf_pairs, on=['subject_name', 'position_timestamps'], how='left', indicator=True)\n",
    "cleaned_df = cleaned_df[cleaned_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rolling mean with a window size of 50 = 1s\n",
    "window_size = 50\n",
    "# Set position_timestamps as the index\n",
    "cleaned_df['smoothed_speed'] = cleaned_df.groupby('subject_name')['speed'].rolling(window=window_size, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "# Reset index for plotting\n",
    "cleaned_df = cleaned_df.reset_index()\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the sampled speed values\n",
    "fig = px.line(\n",
    "    cleaned_df,\n",
    "    x=\"position_timestamps\",\n",
    "    y=\"smoothed_speed\",\n",
    "    color=\"subject_name\",\n",
    "    title=\"Speed of subjects\",\n",
    "    labels={\"smoothed_speed\": \"Speed (pixels/second)\", \"position_timestamps\": \"Time\"},\n",
    "    color_discrete_map=id_color_map,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_positions_df = cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds\n",
    "cm2px = 5.4 \n",
    "fps = 50\n",
    "speed_threshold = 5 * cm2px # in cm/s\n",
    "frame_threshold = 30 * fps  # 0.5 minutes at 50 frames per second\n",
    "\n",
    "subjects_positions_df = subjects_positions_df.reset_index(drop=True)\n",
    "\n",
    "grouped = subjects_positions_df.groupby('subject_name')\n",
    "\n",
    "# Function to apply to each group\n",
    "def filter_by_speed_threshold(group):\n",
    "    # Create a boolean column where speed is below the threshold\n",
    "    group['below_threshold'] = (group['smoothed_speed'] < speed_threshold) | (group['smoothed_speed'].isna())\n",
    "    # Calculate rolling sum of frames below threshold\n",
    "    group['below_threshold_rolling'] = group['below_threshold'].rolling(window=frame_threshold, center=True).sum()\n",
    "    # Create a boolean column where the rolling sum meets the frame threshold\n",
    "    group['meets_frame_threshold'] = group['below_threshold_rolling'] < frame_threshold\n",
    "    # Create the 'active' column based on the criteria\n",
    "    group['active'] = group['meets_frame_threshold']\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group and concatenate the results\n",
    "filtered_subjects_positions_df = pd.DataFrame\n",
    "filtered_subjects_positions_df = grouped.apply(filter_by_speed_threshold).reset_index(drop=True)\n",
    "\n",
    "\n",
    "filtered_subjects_positions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=(\"BAA-1104045\", \"BAA-1104047\"))\n",
    "\n",
    "# Filter data for each subject\n",
    "subject1_df = filtered_subjects_positions_df[filtered_subjects_positions_df['subject_name'] == 'BAA-1104045']\n",
    "subject2_df = filtered_subjects_positions_df[filtered_subjects_positions_df['subject_name'] == 'BAA-1104047']\n",
    "\n",
    "# Add line plot for Subject 1\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=subject1_df[\"position_timestamps\"],\n",
    "        y=subject1_df[\"smoothed_speed\"],\n",
    "        mode='lines',\n",
    "        name='Speed BAA-1104045',\n",
    "        line=dict(color=id_color_map['BAA-1104045'])\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "active_subject1_df = subject1_df[subject1_df[\"active\"]]  # Filter for active timestamps\n",
    "if not active_subject1_df.empty:  # Check if there are any active timestamps\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=active_subject1_df[\"position_timestamps\"],\n",
    "            y=[-1000] * len(active_subject1_df),  # Dummy y-values to position the dots\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color='green',  # Active timestamps are green\n",
    "                size=5,\n",
    "                symbol='circle'\n",
    "            ),\n",
    "            name='Active Status BAA-1104045'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "# Add line plot for Subject 2\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=subject2_df[\"position_timestamps\"],\n",
    "        y=subject2_df[\"smoothed_speed\"],\n",
    "        mode='lines',\n",
    "        name='Speed BAA-1104047',\n",
    "        line=dict(color=id_color_map['BAA-1104047'])\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "active_subject2_df = subject2_df[subject2_df[\"active\"]]  # Filter for active timestamps\n",
    "if not active_subject2_df.empty:  # Check if there are any active timestamps\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=active_subject2_df[\"position_timestamps\"],\n",
    "            y=[-1000] * len(active_subject2_df),  # Dummy y-values to position the dots\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color='green',  # Active timestamps are green\n",
    "                size=5,\n",
    "                symbol='circle'\n",
    "            ),\n",
    "            name='Active Status BAA-1104047'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Speed filtered for Subjects\",\n",
    "    height=600,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Time\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Speed (pixels/second)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Speed (pixels/second)\", row=2, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foraging_query = (\n",
    "    BlockSubjectAnalysis.Patch.proj('pellet_count', 'pellet_timestamps')\n",
    "    & {\"spinnaker_video_source_name\": \"CameraTop\"} #this is the video source name which we rstrict once we selected tuff to keep in table\n",
    "    & key\n",
    "    & f'block_start >= \"{social_start_day}\"'\n",
    "    & f'block_start <= \"{social_end_day}\"'\n",
    ")\n",
    "pellet_data = foraging_query.fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now take shortcut, and just choose timestamps duirng the night where no events occur (not getin actual awake times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and sort the start timestamps\n",
    "time_window = timedelta(minutes=5)  # 5 minutes\n",
    "start_timestamps = combined_df['start_timestamp'].sort_values().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Convert string times to datetime.time objects\n",
    "night_start_time = datetime.strptime(night_start, '%H:%M').time()\n",
    "night_end_time = datetime.strptime(night_end, '%H:%M').time()\n",
    "\n",
    "# Function to check if a timestamp is valid\n",
    "def is_valid_timestamp(new_timestamp, existing_timestamps, time_window, night_start_time, night_end_time):\n",
    "    # Extract time from the timestamp\n",
    "    new_time = new_timestamp.time()\n",
    "    \n",
    "    # Check if the timestamp falls within the light cycle period\n",
    "    if not (night_start_time <= new_time < night_end_time):\n",
    "        return False\n",
    "    \n",
    "    # Check if the timestamp is within the time window length\n",
    "    for ts in existing_timestamps:\n",
    "        if abs((new_timestamp - ts).total_seconds()) < time_window.total_seconds():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Generate 200 random valid timestamps\n",
    "valid_timestamps = []\n",
    "min_timestamp = start_timestamps.min()\n",
    "max_timestamp = start_timestamps.max()\n",
    "\n",
    "while len(valid_timestamps) < len(good_events_df):\n",
    "    random_timestamp = min_timestamp + timedelta(seconds=np.random.randint(0, int((max_timestamp - min_timestamp).total_seconds())))\n",
    "    if is_valid_timestamp(random_timestamp, start_timestamps, time_window, night_start_time, night_end_time):\n",
    "        valid_timestamps.append(random_timestamp)\n",
    "        start_timestamps = pd.concat([start_timestamps, pd.Series([random_timestamp])]).sort_values().reset_index(drop=True)\n",
    "\n",
    "# Convert the list of valid timestamps to a DataFrame\n",
    "valid_timestamps_df = pd.DataFrame(valid_timestamps, columns=['random_timestamp'])\n",
    "\n",
    "# Display the DataFrame\n",
    "valid_timestamps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate tiemstmps\n",
    "# Extract start_timestamps from good_events\n",
    "good_events_start_timestamps = good_events_df['start_timestamp']\n",
    "good_events_behaviour_type = good_events_df['behavior_type']\n",
    "good_events_dominant_id = good_events_df['dominant_id']\n",
    "\n",
    "# Create DataFrame for good_events with event column set to True\n",
    "good_events_df_with_event = pd.DataFrame({\n",
    "    'start_timestamps': good_events_start_timestamps,\n",
    "    'event': True,\n",
    "    'behaviour_type': good_events_behaviour_type,\n",
    "    'dominant_id': good_events_dominant_id\n",
    "})\n",
    "\n",
    "# Create DataFrame for valid_timestamps with event column set to False\n",
    "valid_timestamps_df_with_event = pd.DataFrame({\n",
    "    'start_timestamps': valid_timestamps_df['random_timestamp'],\n",
    "    'event': False\n",
    "})\n",
    "# Concatenate the DataFrames\n",
    "all_timestamps_df = pd.concat([good_events_df_with_event, valid_timestamps_df_with_event]).sort_values(by='start_timestamps').reset_index(drop=True)\n",
    "\n",
    "# Add end_timestamps column\n",
    "all_timestamps_df['end_timestamps'] = all_timestamps_df['start_timestamps'] + timedelta(minutes=time_window_length)\n",
    "\n",
    "# Display the new DataFrame\n",
    "all_timestamps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pellet data\n",
    "foraging_query = (\n",
    "    BlockSubjectAnalysis.Patch.proj('pellet_count', 'pellet_timestamps')\n",
    "    & {\"spinnaker_video_source_name\": \"CameraTop\"} #this is the video source name which we rstrict once we selected tuff to keep in table\n",
    "    & key\n",
    "    & f'block_start >= \"{social_start_day}\"'\n",
    "    & f'block_start <= \"{social_end_day}\"'\n",
    ")\n",
    "foraging_query\n",
    "# Fetch the data\n",
    "pellet_data = foraging_query.fetch()\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each entry in the pellet_data array\n",
    "for entry in pellet_data:\n",
    "    experiment_name = entry[0]\n",
    "    block_start = entry[1]\n",
    "    patch_name = entry[2]\n",
    "    subject_id = entry[3]\n",
    "    pellet_count = entry[4]\n",
    "    pellet_timestamps = entry[5]\n",
    "    \n",
    "    \n",
    "    # For each pellet timestamp, create a dictionary and append to the list\n",
    "    for pellet_timestamp in pellet_timestamps:\n",
    "        data.append({'time': pellet_timestamp,\n",
    "                     'subject_id': subject_id,\n",
    "                     'rank': 'dominant' if subject_id == dominant_id else 'subordinate',})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "pellet_df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the 'time' column to datetime\n",
    "pellet_df['time'] = pd.to_datetime(pellet_df['time'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(pellet_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pellets within time window\n",
    "results = []\n",
    "\n",
    "# Get all unique subject IDs\n",
    "all_subject_ids = pellet_df['subject_id'].unique()\n",
    "\n",
    "for timestamp in all_timestamps_df.iterrows():\n",
    "    start_time = timestamp[1]['start_timestamps']\n",
    "    end_time = timestamp[1]['end_timestamps']\n",
    "    \n",
    "    # Get the pellet data within the time range\n",
    "    pellet_data = pellet_df[(pellet_df['time'] >= start_time) & (pellet_df['time'] <= end_time)]\n",
    "    \n",
    "    # Calculate the total number of pellets per subject\n",
    "    total_pellets = pellet_data['subject_id'].value_counts()\n",
    "    \n",
    "    # Append results for each subject, including those with zero pellets\n",
    "    for subject_id in all_subject_ids:\n",
    "        pellet_count = total_pellets.get(subject_id, 0)\n",
    "        result = timestamp[1].to_dict()  # Copy all columns from all_timestamps_df\n",
    "        result.update({\n",
    "            'subject_id': subject_id,\n",
    "            'pellet_number': pellet_count\n",
    "        })\n",
    "        results.append(result)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Replace 'nan' values in 'behaviour_type' column with 'no_interaction'\n",
    "results_df['behaviour_type'] = results_df['behaviour_type'].fillna('no_interaction')\n",
    "\n",
    "# Display the new DataFrame\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order of categories for 'behaviour_type'\n",
    "category_order = ['no_interaction'] + [x for x in results_df['behaviour_type'].unique() if x != 'no_interaction']\n",
    "\n",
    "# Create the box plot with individual data points\n",
    "fig = px.box(\n",
    "    results_df[results_df['subject_id'] != 'nan'],\n",
    "    x='behaviour_type',\n",
    "    y='pellet_number',\n",
    "    color='subject_id',\n",
    "    title='Pellets post events',\n",
    "    labels={'subject_id': 'Subject ID', 'pellet_number': 'Pellets 5min post event', 'behaviour_type': 'Social interaction type'},\n",
    "    category_orders={'behaviour_type': category_order},\n",
    "    color_discrete_map=id_color_map,\n",
    "    points='all',\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'pellet_number' is zero\n",
    "filtered_df = results_df[(results_df['subject_id'] != 'nan') & (results_df['pellet_number'] != 0)]\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.box(\n",
    "    filtered_df,\n",
    "    x='behaviour_type',\n",
    "    y='pellet_number',\n",
    "    color='subject_id',\n",
    "    title='Pellets post events (if at least 1 pellet)',\n",
    "    labels={'subject_id': 'Subject ID', 'pellet_number': 'Pellets 5min post event', 'behaviour_type': 'Social interaction type'},\n",
    "    category_orders={'behaviour_type': category_order},\n",
    "    color_discrete_map=id_color_map,\n",
    "    points='all',\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "fig = px.box(\n",
    "    results_df[results_df['subject_id'] != 'nan'],\n",
    "    x='event',\n",
    "    y='pellet_number',\n",
    "    color='subject_id',\n",
    "    title='Pellets post events',\n",
    "    labels={'subject_id': 'Subject ID', 'pellet_number': 'Pellets 5min post event', 'event': 'Post social interation'},\n",
    "    category_orders={'behaviour_type': category_order},\n",
    "    color_discrete_map=id_color_map,\n",
    "    points='all',\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Does the outcome of social interactions influence foraging behaviour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pellets and tube test wins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 20min period after each chase\n",
    "#get number of pellets per subject in eachpost chase period\n",
    "results = []\n",
    "for index, chase in chasing_df.iterrows():\n",
    "    # Get the subject_id and the timestamp of the tube test\n",
    "    dominant_id = str(chase['dominant_id'])\n",
    "    subordinate_id = (set(unique_ids) - {dominant_id}).pop()\n",
    "\n",
    "\n",
    "    chase_time = chase['start_timestamp'] \n",
    "\n",
    "    # Calculate the start and end time of the 20-minute period\n",
    "    start_time = chase_time\n",
    "    end_time = chase_time + pd.Timedelta(minutes=20)\n",
    "    \n",
    "    # Get the pellet data within the time range\n",
    "    pellet_data = pellet_df[(pellet_df['time'] >= start_time) & (pellet_df['time'] <= end_time)]\n",
    "    \n",
    "    # Calculate the total number of pellets per subject\n",
    "    total_pellets = pellet_data['subject_id'].value_counts()\n",
    "    \n",
    "    # Get the number of pellets for the dominant and subordinate subjects\n",
    "    dominant_pellets = total_pellets.get(dominant_id, 0)\n",
    "    subordinate_pellets = total_pellets.get(subordinate_id, 0)\n",
    "\n",
    "    # Append results for the dominant subject\n",
    "    results.append({\n",
    "        'start': start_time,\n",
    "        'end': end_time,\n",
    "        'outcome': 'dominant',\n",
    "        'subject_id': dominant_id,\n",
    "        'pellets': dominant_pellets\n",
    "    })\n",
    "\n",
    "    # Append results for the subordinate subject\n",
    "    results.append({\n",
    "        'start': start_time,\n",
    "        'end': end_time,\n",
    "        'outcome': 'subordinate',\n",
    "        'subject_id': subordinate_id,\n",
    "        'pellets': subordinate_pellets\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.head()\n",
    "\n",
    "# plot number of pellets post tubetest per subject when dominant and when subordinate\n",
    "# Create the scatter plot\n",
    "fig = px.box(\n",
    "    results_df[results_df['subject_id'] != 'nan'],\n",
    "    x='subject_id',\n",
    "    y='pellets',\n",
    "    color='outcome',\n",
    "    title='Pellets post chase',\n",
    "    labels={'subject_id': 'Subject ID', 'pellets': 'Pellets 20min post event', 'outcome': 'Outcome of event'},\n",
    "    points='all',\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get number of pellets per subject in each post-tubetest period\n",
    "results = []\n",
    "for index, test in tube_test_df.iterrows():\n",
    "    # Get the subject_id and the timestamp of the tube test\n",
    "    dominant_id = str(test['dominant_id'])\n",
    "    subordinate_id = (set(unique_ids) - {dominant_id}).pop()\n",
    "\n",
    "\n",
    "    test_time = test['start_timestamp'] \n",
    "\n",
    "    # Calculate the start and end time of the 20-minute period\n",
    "    start_time = test_time\n",
    "    end_time = test_time + pd.Timedelta(minutes=20)\n",
    "    \n",
    "    # Get the pellet data within the time range\n",
    "    pellet_data = pellet_df[(pellet_df['time'] >= start_time) & (pellet_df['time'] <= end_time)]\n",
    "    \n",
    "    # Calculate the total number of pellets per subject\n",
    "    total_pellets = pellet_data['subject_id'].value_counts()\n",
    "    \n",
    "    # Get the number of pellets for the dominant and subordinate subjects\n",
    "    dominant_pellets = total_pellets.get(dominant_id, 0)\n",
    "    subordinate_pellets = total_pellets.get(subordinate_id, 0)\n",
    "\n",
    "    # Append results for the dominant subject\n",
    "    results.append({\n",
    "        'start': start_time,\n",
    "        'end': end_time,\n",
    "        'outcome': 'dominant',\n",
    "        'subject_id': dominant_id,\n",
    "        'pellets': dominant_pellets\n",
    "    })\n",
    "\n",
    "    # Append results for the subordinate subject\n",
    "    results.append({\n",
    "        'start': start_time,\n",
    "        'end': end_time,\n",
    "        'outcome': 'subordinate',\n",
    "        'subject_id': subordinate_id,\n",
    "        'pellets': subordinate_pellets\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.head()\n",
    "\n",
    "# plot number of pellets post tubetest per subject when dominant and when subordinate\n",
    "# Create the scatter plot\n",
    "fig = px.box(\n",
    "    results_df[results_df['subject_id'] != 'nan'],\n",
    "    x='subject_id',\n",
    "    y='pellets',\n",
    "    color='outcome',\n",
    "    title='Pellets post tube test',\n",
    "    labels={'subject_id': 'Subject ID', 'pellets': 'Pellets 20min post event', 'outcome': 'Outcome of event'},\n",
    "    points='all',\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap of foraging /interactions / time relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nomalise measure of intracitions\n",
    "\n",
    "# Resample the data into 1-hour bins and count the number of events for each behaviour type\n",
    "events_per_hour = combined_df.groupby('behavior_type').resample('1H').size().reset_index(name='event_count')\n",
    "\n",
    "# Display the result\n",
    "print(events_per_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise teh event counts for eac behaviour type to get a metric between 0-1\n",
    "# Calculate the min and max event count for each behavior type\n",
    "min_max_counts = events_per_hour.groupby('behavior_type')['event_count'].agg(['min', 'max']).reset_index()\n",
    "min_max_counts.rename(columns={'min': 'min_event_count', 'max': 'max_event_count'}, inplace=True)\n",
    "\n",
    "# Merge the min and max event counts back to the original DataFrame\n",
    "events_per_hour = events_per_hour.merge(min_max_counts, on='behavior_type')\n",
    "\n",
    "# Apply min-max normalization\n",
    "events_per_hour['normalized_event_count'] = (\n",
    "    (events_per_hour['event_count'] - events_per_hour['min_event_count']) /\n",
    "    (events_per_hour['max_event_count'] - events_per_hour['min_event_count'])\n",
    ")\n",
    "\n",
    "# Drop the 'min_event_count' and 'max_event_count' columns if not needed\n",
    "events_per_hour.drop(columns=['min_event_count', 'max_event_count'], inplace=True)\n",
    "\n",
    "# Display the result\n",
    "events_per_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot this measure across time\n",
    "# Create the line plot with resampled and interpolated data\n",
    "fig = px.line(\n",
    "    events_per_hour,\n",
    "    x='start_timestamp',\n",
    "    y='normalized_event_count',\n",
    "    color='behavior_type',\n",
    "    title='Normalised interactions over time',\n",
    "    labels={'start_timestamp': 'Time', 'normalized_event_count': 'Normalised count', 'behavior_type': 'Behaviour'},    \n",
    "    color_discrete_map=behaviour_map,\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pellet counts\n",
    "# Define the time bin size (e.g., 1 hour)\n",
    "time_bin_size = '1h'\n",
    "# Create a copy of the DataFrame to avoid modifying the original\n",
    "pellet_df_copy = pellet_df.copy()\n",
    "\n",
    "# Set the 'time' as the DataFrame index\n",
    "pellet_df_copy.set_index('time', inplace=True)\n",
    "\n",
    "# Bin the data by time and count the number of pellets per bin per subject\n",
    "binned_pellet_df = pellet_df_copy.groupby([pd.Grouper(freq=time_bin_size), 'subject_id']).size().reset_index(name='pellet_count')\n",
    "    \n",
    "# Create a complete time range for the bins\n",
    "time_range = pd.date_range(start=social_start_time, end=social_end_time, freq=time_bin_size)\n",
    "\n",
    "# Get unique subject IDs\n",
    "subject_ids = binned_pellet_df['subject_id'].unique()\n",
    "\n",
    "# Create a MultiIndex with all combinations of time_range and subject_ids\n",
    "multi_index = pd.MultiIndex.from_product([time_range, subject_ids], names=['time', 'subject_id'])\n",
    "\n",
    "# Create an empty DataFrame with the MultiIndex\n",
    "complete_df = pd.DataFrame(index=multi_index).reset_index()\n",
    "\n",
    "# Merge the complete DataFrame with the binned pellet data\n",
    "complete_df = complete_df.merge(binned_pellet_df, on=['time', 'subject_id'], how='left').fillna(0)\n",
    "\n",
    "# Ensure pellet_count is an integer\n",
    "complete_df['pellet_count'] = complete_df['pellet_count'].astype(int)\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise hte pellet counts pr subject\n",
    "# Calculate the min and max event count for each behavior type\n",
    "min_max_counts = complete_df.groupby('subject_id')['pellet_count'].agg(['min', 'max']).reset_index()\n",
    "min_max_counts.rename(columns={'min': 'min_event_count', 'max': 'max_event_count'}, inplace=True)\n",
    "\n",
    "# Merge the min and max event counts back to the original DataFrame\n",
    "complete_df = complete_df.merge(min_max_counts, on='subject_id')\n",
    "\n",
    "# Apply min-max normalization\n",
    "complete_df['normalized_pellet_count'] = (\n",
    "    (complete_df['pellet_count'] - complete_df['min_event_count']) /\n",
    "    (complete_df['max_event_count'] - complete_df['min_event_count'])\n",
    ")\n",
    "\n",
    "# Drop the 'min_event_count' and 'max_event_count' columns if not needed\n",
    "complete_df.drop(columns=['min_event_count', 'max_event_count'], inplace=True)\n",
    "\n",
    "# Display the result\n",
    "complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add pellet count to event table table\n",
    "events_per_hour.rename(columns={'start_timestamp': 'time'}, inplace=True)\n",
    "# Merge the DataFrames on 'time' and 'subject_id'\n",
    "merged_df = pd.merge(events_per_hour, complete_df, on=['time'], how='left')\n",
    "\n",
    "# Display the result\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unique subjects and behavior types\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "\n",
    "subjects = merged_df['subject_id'].unique()\n",
    "behavior_types = merged_df['behavior_type'].unique()\n",
    "\n",
    "# Iterate over each subject\n",
    "for subject in subjects:\n",
    "    # Create a subplot figure with 3 rows (one for each behavior type)\n",
    "    fig = sp.make_subplots(rows=len(behavior_types), cols=1, shared_xaxes=True, subplot_titles=behavior_types)\n",
    "    \n",
    "    # Iterate over each behavior type\n",
    "    for i, behavior_type in enumerate(behavior_types, start=1):\n",
    "        # Filter the DataFrame for the specific subject and behavior type\n",
    "        subject_behavior_df = merged_df[(merged_df['subject_id'] == subject) & (merged_df['behavior_type'] == behavior_type)]\n",
    "        \n",
    "        # Create a heatmap DataFrame\n",
    "        heatmap_data = subject_behavior_df.pivot_table(\n",
    "            index='time',\n",
    "            values=['normalized_event_count', 'normalized_pellet_count']\n",
    "        )\n",
    "        \n",
    "        # Create heatmap\n",
    "        heatmap_fig = px.imshow(\n",
    "            heatmap_data.T,\n",
    "            labels={'x': 'Time (1h Bins)', 'y': 'Measurement'},\n",
    "            x=heatmap_data.index,\n",
    "            y=['Normalized Event Count', 'Normalized Pellet Count'],\n",
    "            color_continuous_scale='Viridis',\n",
    "            aspect='auto'\n",
    "        )\n",
    "        \n",
    "        # Add the heatmap to the subplot\n",
    "        for trace in heatmap_fig.data:\n",
    "            fig.add_trace(trace, row=i, col=1)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Heatmaps for Subject: {subject}',\n",
    "        xaxis_title='Time (1h Bins)',\n",
    "        yaxis_title='Measurement',\n",
    "        template='plotly_white',\n",
    "        height=900  # Adjust height as needed\n",
    "    )\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Pivot the data to create a DataFrame for the heatmap\n",
    "heatmap_data = merged_df.pivot_table(\n",
    "    index='time',\n",
    "    columns=['subject_id', 'behavior_type'],\n",
    "    values=['normalized_event_count', 'normalized_pellet_count']\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex to create unique labels for the heatmap rows\n",
    "heatmap_data.columns = [f'{val}_{subj}_{beh}' for val, subj, beh in heatmap_data.columns]\n",
    "\n",
    "# Create a new DataFrame with just the rows you want to plot\n",
    "\n",
    "# Extract pellet counts for both subjects (assuming no behavior type distinction is needed)\n",
    "pellet_counts_47 = heatmap_data.filter(like='normalized_pellet_count_BAA-1104047')\n",
    "pellet_counts_45 = heatmap_data.filter(like='normalized_pellet_count_BAA-1104045')\n",
    "\n",
    "# Average event counts across both subjects for each behavior type\n",
    "event_counts_chasing = heatmap_data.filter(like='chasing').mean(axis=1) *2\n",
    "event_counts_fighting = heatmap_data.filter(like='fighting').mean(axis=1) *2\n",
    "event_counts_tube_test = heatmap_data.filter(like='tube_test').mean(axis=1) *2\n",
    "\n",
    "# Combine the data into a single DataFrame for plotting\n",
    "heatmap_final = pd.DataFrame({\n",
    "    'Pellet Count BAA-1104047': pellet_counts_47.mean(axis=1),\n",
    "    'Pellet Count BAA-1104045': pellet_counts_45.mean(axis=1),\n",
    "    'Chasing Events': event_counts_chasing,\n",
    "    'Fighting Events': event_counts_fighting,\n",
    "    'Tube Test Events': event_counts_tube_test\n",
    "}, index=heatmap_data.index)\n",
    "\n",
    "# Create the heatmap\n",
    "fig = px.imshow(\n",
    "    heatmap_final.T,  # Transpose to have the measurements on the y-axis\n",
    "    labels={'x': 'Time (1h Bins)', 'y': 'Measurement'},\n",
    "    color_continuous_scale='Viridis',\n",
    "    aspect='auto',\n",
    "    zmin=0, zmax=1  # Set the color scale to range from 0 to 1\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Heatmap of Pellet Counts and Events for Both Subjects Over Time',\n",
    "    xaxis_title='Time (1h Bins)',\n",
    "    yaxis_title='Measurement',\n",
    "    template='plotly_white',\n",
    "    height=500  # Adjust height as needed\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aeon)",
   "language": "python",
   "name": "aeon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
