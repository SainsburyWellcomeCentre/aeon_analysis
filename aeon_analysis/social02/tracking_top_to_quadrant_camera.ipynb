{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining which quadtant camera to use for each frame and mouse during social sessions, based on centoid tracking pixel locations and mapping from top to quadrant cameras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make a composite video from a chunck made up of quadrant camera frames like this:\n",
    "- for timestamps where both mice are visible in the same quadrant camera FOV, include this quadrant camera frame\n",
    "- for timestamps where mice A and B are not visible in the same quadrant camera FOV, include both the quadrant camera frame that shows mouse A AND the quadrant camera frame that shows mouse B\n",
    "- when multiple quadrant cameras include the same mouse, use the one that has the best view (most central)\n",
    "- fill in short gaps in centroid tracking if the chosen quadrant camera before and after the gap is the same (assuming mouse stayed in same quadrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notebook settings and imports\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import cv2\n",
    "\n",
    "import datajoint as dj\n",
    "from aeon.dj_pipeline.analysis.block_analysis import *\n",
    "from aeon.dj_pipeline import acquisition, streams\n",
    "import random\n",
    "import aeon\n",
    "from aeon.io import reader, video\n",
    "from aeon.schema.schemas import social02\n",
    "from shapely.geometry import Point, Polygon\n",
    "from pathlib import Path\n",
    "from aeon.io import api\n",
    "import plotly.colors as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define constants\"\"\"\n",
    "\n",
    "experiment = 'social0.2'\n",
    "arena = 'AEON3'\n",
    "\n",
    "dj_experiment_name = 'social0.2-aeon3'\n",
    "dj_chunk_start = \"2024-02-10 11:00:00\"\n",
    "\n",
    "# Constants\n",
    "CAMERA_A = 'CameraTop'\n",
    "CAMERA_B_LIST = ['CameraSouth', 'CameraNorth', 'CameraEast', 'CameraWest']\n",
    "CAMERA_DIMENSIONS = (1080, 1440)  # Dimensions for the top+quadrant cameras\n",
    "\n",
    "TIMESTAMP_ERROR_TOLERANCE = pd.Timedelta(milliseconds=9) \n",
    "MAX_GAP_TO_FILL = pd.Timedelta(seconds=15)\n",
    "\n",
    "KEY = {\"experiment_name\": dj_experiment_name}\n",
    "CHUNK_RESTRICTION = {\"chunk_start\": dj_chunk_start}\n",
    "PART_RESTRICTION = {\"part_name\": \"centroid\"}\n",
    "BASE_PATH = '/ceph/aeon/aeon/'\n",
    "VIDEO_EXPORT_DIR = BASE_PATH + f'code/scratchpad/Orsi/pixel_mapping/composite_videos/'\n",
    "ROOT = BASE_PATH + f'data/raw/{arena}/{experiment}/'\n",
    "\n",
    "# Paths for homographies\n",
    "homography_paths = [f'{BASE_PATH}code/scratchpad/Orsi/pixel_mapping/pixel_mapping_results/{experiment}/{arena}/H_{camera}.npy' \n",
    "                    for camera in CAMERA_B_LIST]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load data\"\"\"\n",
    "\n",
    "# Load homographies\n",
    "homographies = [np.load(path) for path in homography_paths]\n",
    "\n",
    "# Fetch centroid data for Camera A\n",
    "pose_query = (\n",
    "    streams.SpinnakerVideoSource\n",
    "    * tracking.SLEAPTracking.PoseIdentity.proj(\"identity_name\", \"identity_likelihood\", anchor_part=\"part_name\")\n",
    "    * tracking.SLEAPTracking.Part\n",
    "    & {\"spinnaker_video_source_name\": CAMERA_A}\n",
    "    & KEY\n",
    "    & CHUNK_RESTRICTION\n",
    "    & PART_RESTRICTION\n",
    ")\n",
    "\n",
    "centroid_df = fetch_stream(pose_query)\n",
    "centroid_df.drop(columns=[\"spinnaker_video_source_name\"], inplace=True)\n",
    "\n",
    "# Clean centroid data\n",
    "centroid_df = (\n",
    "    centroid_df.groupby(\"identity_name\")\n",
    "    .apply(lambda x: x.dropna().sort_index())\n",
    "    .droplevel(0)  # Drop the added group level\n",
    ")\n",
    "centroid_df = centroid_df.sort_index()\n",
    "centroid_df[\"x\"], centroid_df[\"y\"] = centroid_df[\"x\"].astype(np.int32), centroid_df[\"y\"].astype(np.int32)\n",
    "centroid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine quadrant camera to use based on position and camera registation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions to process coordinates and find quadrants\"\"\"\n",
    "\n",
    "# Function to get transformed corners\n",
    "def get_transformed_corners(homography_matrix, img_shape):\n",
    "    h, w = img_shape\n",
    "    \n",
    "    # Define the corners of the image\n",
    "    corners = np.array([[0, 0, 1],          # Top-left\n",
    "                        [0, h - 1, 1],      # Bottom-left\n",
    "                        [w - 1, 0, 1],      # Top-right\n",
    "                        [w - 1, h - 1, 1]]) # Bottom-right\n",
    "    \n",
    "    # Apply the homography matrix to transform the corners\n",
    "    transformed_corners = (homography_matrix @ corners.T).T\n",
    "    \n",
    "    # Normalize by the last coordinate to obtain (x, y) points\n",
    "    transformed_corners = transformed_corners[:, :2] / transformed_corners[:, 2][:, np.newaxis]\n",
    "\n",
    "    # If all four corners are valid after transformation, return them in order\n",
    "    if len(transformed_corners) == 4:\n",
    "        # Sort the corners to find the correct order\n",
    "        sorted_corners = sorted(transformed_corners, key=lambda point: (point[0], point[1]))\n",
    "        top_left, bottom_left = sorted(sorted_corners[:2], key=lambda point: point[1])\n",
    "        top_right, bottom_right = sorted(sorted_corners[2:], key=lambda point: point[1])\n",
    "        return np.array([top_left, top_right, bottom_right, bottom_left])\n",
    "    else:\n",
    "        print(f\"Only {len(transformed_corners)} valid corners found.\")\n",
    "        return transformed_corners\n",
    "\n",
    "# Calculate FOV centers\n",
    "def calculate_fov_centers(transformed_corners_list):\n",
    "    centers = [\n",
    "        (np.mean(corners[:, 0]), np.mean(corners[:, 1])) if len(corners) == 4 else None\n",
    "        for corners in transformed_corners_list\n",
    "    ]\n",
    "    return centers\n",
    "\n",
    "# Determine quadrants\n",
    "def find_quadrants_for_positions(positions_df, transformed_corners_list):\n",
    "    possible_quadrants = []\n",
    "    for _, row in positions_df.iterrows():\n",
    "        point = Point(row['x'], row['y'])\n",
    "        in_fovs = [\n",
    "            i for i, corners in enumerate(transformed_corners_list)\n",
    "            if len(corners) == 4 and Polygon(corners).contains(point)\n",
    "        ]\n",
    "        possible_quadrants.append(in_fovs)\n",
    "    return possible_quadrants\n",
    "\n",
    "# Calculate the closest quadrant\n",
    "def find_closest_quadrant(positions_df, transformed_corners_list):\n",
    "    closest_quadrants = []\n",
    "    for _, row in positions_df.iterrows():\n",
    "        point = Point(row['x'], row['y'])\n",
    "        distances = [\n",
    "            (np.sqrt((row['x'] - np.mean(corners[:, 0])) ** 2 + (row['y'] - np.mean(corners[:, 1])) ** 2), i)\n",
    "            for i, corners in enumerate(transformed_corners_list) if len(corners) == 4\n",
    "        ]\n",
    "        distances.sort()\n",
    "        closest_quadrants.append([distances[0][1]] if distances else None)\n",
    "    return closest_quadrants\n",
    "\n",
    "# Determine the most central quadrant\n",
    "def find_most_central_quadrant(positions_df, fov_centers):\n",
    "    most_central_quadrant = []\n",
    "    for _, row in positions_df.iterrows():\n",
    "        quadrants = row['possible_quadrants_extended']\n",
    "        if not quadrants:\n",
    "            most_central_quadrant.append(None)\n",
    "            continue\n",
    "        distances = [\n",
    "            (np.sqrt((row['x'] - fov_centers[q][0]) ** 2 + (row['y'] - fov_centers[q][1]) ** 2), q)\n",
    "            for q in quadrants\n",
    "        ]\n",
    "        distances.sort()\n",
    "        most_central_quadrant.append(distances[0][1])\n",
    "    return most_central_quadrant\n",
    "\n",
    "# Get quadrants that see both mice\n",
    "def find_common_quadrants_for_both_mice(positions_df):\n",
    "    both_mice_quadrants = []\n",
    "    grouped = positions_df.groupby(positions_df.index)\n",
    "    \n",
    "    for timestamp, group in grouped:\n",
    "        # If there are less than 2 entries for the timestamp, append None\n",
    "        if len(group) < 2:\n",
    "            both_mice_quadrants.append((timestamp, None))\n",
    "            continue\n",
    "        \n",
    "        # Find common quadrants between the two mice\n",
    "        quadrants_1 = set(group.iloc[0]['possible_quadrants'])\n",
    "        quadrants_2 = set(group.iloc[1]['possible_quadrants'])\n",
    "        common_quadrants = list(quadrants_1.intersection(quadrants_2))\n",
    "        \n",
    "        both_mice_quadrants.append((timestamp, common_quadrants))\n",
    "    \n",
    "    # Create a DataFrame from the results\n",
    "    common_quadrants_df = pd.DataFrame(both_mice_quadrants, columns=['time', 'both_mice_quadrants'])\n",
    "    common_quadrants_df.set_index('time', inplace=True)\n",
    "    return common_quadrants_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Proces coordintes and find quadrants\"\"\"\n",
    "\n",
    "# Step 0: Calculate corners of each camera B FOV\n",
    "transformed_corners_list = [get_transformed_corners(homography, CAMERA_DIMENSIONS) for homography in homographies]\n",
    "\n",
    "# Step 1: Calculate the center of each camera B FOV\n",
    "fov_centers = calculate_fov_centers(transformed_corners_list)\n",
    "\n",
    "# Step 2: Check if each animal position is within each FOV\n",
    "centroid_df['possible_quadrants'] = find_quadrants_for_positions(centroid_df, transformed_corners_list)\n",
    "\n",
    "# Step 3: Find the closest quadrant for positions with no possible quadrants\n",
    "closest_quadrants = find_closest_quadrant(centroid_df, transformed_corners_list)\n",
    "# Update possible_quadrants_extended to include the closest quadrant for positions with no possible quadrants\n",
    "centroid_df['possible_quadrants_extended'] = [\n",
    "    row.possible_quadrants if row.possible_quadrants else closest_quadrants[idx]\n",
    "    for idx, row in enumerate(centroid_df.itertuples())\n",
    "]\n",
    "\n",
    "# Step 4: Determine the most central quadrant for each animal position\n",
    "centroid_df['most_central_quadrant'] = find_most_central_quadrant(centroid_df, fov_centers)\n",
    "\n",
    "# Step 5: Determine which cameras see both mice\n",
    "centroid_df['both_mice_quadrants'] = find_common_quadrants_for_both_mice(centroid_df)\n",
    "\n",
    "# Step 6: Selected quadrant camera(s) for each timestamp\n",
    "def determine_quadrant_camera(positions_df):\n",
    "    selected_quadrants = []\n",
    "    for timestamp, group in positions_df.groupby(positions_df.index):\n",
    "        common_quadrants = group['both_mice_quadrants'].iloc[0]\n",
    "        if not common_quadrants:\n",
    "            selected_quadrant = group['most_central_quadrant'].tolist()\n",
    "        elif len(common_quadrants) == 1:\n",
    "            selected_quadrant = [int(common_quadrants[0])]\n",
    "        else:\n",
    "            central_quadrants = group['most_central_quadrant'].unique()\n",
    "            intersection = [int(q) for q in central_quadrants if q in common_quadrants]\n",
    "            if len(intersection) == 1:\n",
    "                selected_quadrant = intersection\n",
    "            elif len(intersection) == 2:\n",
    "                # Check if either of the two elements is the most central for at least one mouse\n",
    "                most_central_counts = group['most_central_quadrant'].value_counts()\n",
    "                if most_central_counts[intersection[0]] > 0 and most_central_counts[intersection[1]] > 0:\n",
    "                    # Prefer the one that is most central for both mice\n",
    "                    selected_quadrant = [intersection[0]] if most_central_counts[intersection[0]] > most_central_counts[intersection[1]] else [intersection[1]]\n",
    "                elif most_central_counts[intersection[0]] > 0:\n",
    "                    selected_quadrant = [intersection[0]]\n",
    "                elif most_central_counts[intersection[1]] > 0:\n",
    "                    selected_quadrant = [intersection[1]]\n",
    "                else:\n",
    "                    selected_quadrant = [int(random.choice(common_quadrants))]\n",
    "            else:\n",
    "                selected_quadrant = [int(random.choice(common_quadrants))]\n",
    "                \n",
    "        selected_quadrants.append((timestamp, selected_quadrant))\n",
    "    return pd.DataFrame(selected_quadrants, columns=['time', 'selected_quadrant']).set_index('time')\n",
    "\n",
    "# Finalize and clean up DataFrame\n",
    "selected_quadrants_df = determine_quadrant_camera(centroid_df)\n",
    "centroid_df = centroid_df.merge(selected_quadrants_df, left_index=True, right_index=True, how='left')\n",
    "centroid_df['selected_quadrant'] = centroid_df['selected_quadrant'].apply(lambda x: sorted(set(x)))\n",
    "\n",
    "# Populate DataFrame with timestamps and selected quadrants\n",
    "quadrant_timestamp_df = centroid_df.groupby(centroid_df.index).first().reset_index()[['time']]\n",
    "quadrant_timestamp_df.set_index('time', inplace=True)\n",
    "# Add selected quadrant to the DataFrame\n",
    "quadrant_timestamp_df['selected_quadrant'] = centroid_df.groupby(centroid_df.index)['selected_quadrant'].first()\n",
    "# Check for NaN values and print debug information\n",
    "nan_indices = quadrant_timestamp_df['selected_quadrant'].isna()\n",
    "if nan_indices.any():\n",
    "    print(\"NaN values found in selected_quadrant at indices:\", quadrant_timestamp_df[nan_indices].index.tolist())\n",
    "# Ensure all selected_quadrant values are lists of integers\n",
    "quadrant_timestamp_df['selected_quadrant'] = quadrant_timestamp_df['selected_quadrant'].apply(\n",
    "    lambda x: [int(q) for q in x] if isinstance(x, list) else ([int(x)] if pd.notna(x) else x)\n",
    ")\n",
    "# Map the quadrant index to the camera name based on the order in CAMERA_B_LIST\n",
    "quadrant_to_camera_name = {i: camera for i, camera in enumerate(CAMERA_B_LIST)}\n",
    "# Add the selected camera name to the DataFrame, handling NaN values\n",
    "quadrant_timestamp_df['selected_camera_name'] = quadrant_timestamp_df['selected_quadrant'].apply(\n",
    "    lambda x: [quadrant_to_camera_name[q] for q in x] if isinstance(x, list) and pd.notna(x).all() else []\n",
    ")\n",
    "\n",
    "# Step 7: Fill in missing timestamps in data if they are shorter than the threshold and flanked by the same quadrant\n",
    "# Generate a complete range of timestamps for the expected frame rate (50 fps)\n",
    "start_time = pd.Timestamp(dj_chunk_start)\n",
    "end_time = start_time + pd.Timedelta(hours=1)\n",
    "expected_timestamps = pd.date_range(start=start_time, end=end_time, freq='19.999981ms')\n",
    "\n",
    "# Create a DataFrame for the expected timestamps with NaN values\n",
    "expected_df = pd.DataFrame(index=expected_timestamps)\n",
    "\n",
    "# Merge the existing data with the expected timestamps\n",
    "quadrant_timestamp_df = pd.merge_asof(\n",
    "    expected_df.reset_index().rename(columns={'index': 'time'}),\n",
    "    quadrant_timestamp_df.reset_index().rename(columns={'index': 'time'}),\n",
    "    on='time',\n",
    "    direction='nearest',\n",
    "    tolerance=TIMESTAMP_ERROR_TOLERANCE\n",
    ").set_index('time')\n",
    "\n",
    "# Identify runs of NaNs that are max gap to fill seconds or less\n",
    "is_nan = quadrant_timestamp_df['selected_camera_name'].isna()\n",
    "nan_runs = is_nan.astype(int).groupby((~is_nan).cumsum()).cumsum()\n",
    "\n",
    "# Find the start and end of each NaN run\n",
    "nan_run_starts = nan_runs[is_nan & (nan_runs == 1)].index\n",
    "nan_run_ends = nan_runs[is_nan & (nan_runs == nan_runs.groupby((~is_nan).cumsum()).transform('max'))].index\n",
    "\n",
    "# Process NaN runs for filling\n",
    "for start, end in zip(nan_run_starts, nan_run_ends):\n",
    "    prev_timestamp = start - pd.Timedelta(milliseconds=1)\n",
    "    next_timestamp = end + pd.Timedelta(milliseconds=1)\n",
    "    \n",
    "    # Find the closest previous and next timestamps in the DataFrame\n",
    "    closest_prev_timestamp = quadrant_timestamp_df.index.asof(prev_timestamp)\n",
    "    next_index = quadrant_timestamp_df.index.searchsorted(next_timestamp)\n",
    "    closest_next_timestamp = quadrant_timestamp_df.index[next_index] if next_index < len(quadrant_timestamp_df.index) else None\n",
    "    \n",
    "    prev_camera = quadrant_timestamp_df.loc[closest_prev_timestamp, 'selected_camera_name'] if closest_prev_timestamp else 'Unknown'\n",
    "    next_camera = quadrant_timestamp_df.loc[closest_next_timestamp, 'selected_camera_name'] if closest_next_timestamp else 'Unknown'\n",
    "    \n",
    "    prev_quadrant = quadrant_timestamp_df.loc[closest_prev_timestamp, 'selected_quadrant'] if closest_prev_timestamp else 'Unknown'\n",
    "    next_quadrant = quadrant_timestamp_df.loc[closest_next_timestamp, 'selected_quadrant'] if closest_next_timestamp else 'Unknown'\n",
    "    \n",
    "    # Print debug information for each gap\n",
    "    gap_length = is_nan[start:end].sum()\n",
    "    #print(f\"Gap from {start} to {end} with previous camera {prev_camera} and next camera {next_camera}\")\n",
    "    print(f\"Length of gap: {gap_length} frames\")\n",
    "    \n",
    "    # Fill NaNs if previous and next cameras are the same and the gap is below the threshold\n",
    "    if (end - start <= MAX_GAP_TO_FILL) and (prev_camera == next_camera) and (prev_camera != 'Unknown'):\n",
    "        fill_camera = prev_camera if isinstance(prev_camera, list) else [prev_camera]\n",
    "        fill_quadrant = prev_quadrant if isinstance(prev_quadrant, list) else [prev_quadrant]\n",
    "        \n",
    "        # Fill the gap in the DataFrame\n",
    "        quadrant_timestamp_df.loc[start:end, 'selected_camera_name'] = [fill_camera] * len(quadrant_timestamp_df.loc[start:end])\n",
    "        quadrant_timestamp_df.loc[start:end, 'selected_quadrant'] = [fill_quadrant] * len(quadrant_timestamp_df.loc[start:end])\n",
    "\n",
    "print(quadrant_timestamp_df['selected_camera_name'].value_counts(dropna=False))\n",
    "quadrant_timestamp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stitch video together from selected quadrant cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: make a frames_info for each quadrant camera as dict\n",
    "\n",
    "# Initialize a dictionary to hold frames information for each camera\n",
    "frames_info_dict = {}\n",
    "\n",
    "# Loop through each camera name and load the data\n",
    "for camera in CAMERA_B_LIST:\n",
    "    # NOTE: this doesnt seem to work if I don't define these frame infos manually like this \n",
    "    if camera == 'CameraNorth':\n",
    "        frames_info = api.load(ROOT, social02.CameraNorth.Video, start=start_time, end=end_time)\n",
    "    elif camera == 'CameraSouth':\n",
    "        frames_info = api.load(ROOT, social02.CameraSouth.Video, start=start_time, end=end_time)\n",
    "    elif camera == 'CameraEast':\n",
    "        frames_info = api.load(ROOT, social02.CameraEast.Video, start=start_time, end=end_time)\n",
    "    elif camera == 'CameraWest':\n",
    "        frames_info = api.load(ROOT, social02.CameraWest.Video, start=start_time, end=end_time)\n",
    "\n",
    "    # Store the loaded frames info in the dictionary\n",
    "    frames_info_dict[camera] = frames_info\n",
    "    \n",
    "    \n",
    "# Step 2: to make frames_info_final, loop over quadrant_timestamp_df tiemstamps and get the corresponding frames_info for the selected camera\n",
    "# Initialize an empty DataFrame to store all the frames\n",
    "frames_info_final = pd.DataFrame()\n",
    "\n",
    "# Loop through each timestamp in quadrant_timestamp_df\n",
    "for index, row in quadrant_timestamp_df.iterrows():\n",
    "    timestamp = index  # Current timestamp from the DataFrame\n",
    "    selected_cameras = row['selected_camera_name']  # Cameras to get frames from for this timestamp\n",
    "    \n",
    "    # For each selected camera, get the frames closest to the current timestamp\n",
    "    for camera_name in selected_cameras:\n",
    "        frames_info = frames_info_dict[camera_name]  # Access preloaded frame info for the camera\n",
    "        \n",
    "        # Find the closest frame to the current timestamp within the time tolerance\n",
    "        closest_frame_info = frames_info.loc[\n",
    "            (frames_info.index >= (timestamp - TIMESTAMP_ERROR_TOLERANCE)) & \n",
    "            (frames_info.index <= (timestamp + TIMESTAMP_ERROR_TOLERANCE))\n",
    "        ]\n",
    "        \n",
    "        # Append these frames to the final DataFrame\n",
    "        frames_info_final = pd.concat([frames_info_final, closest_frame_info], ignore_index=True)\n",
    "\n",
    "# Sort the final DataFrame by time if necessary\n",
    "frames_info_final.sort_index(inplace=True)\n",
    "\n",
    "# Step 3: call video.frames on this to compile video and save it\n",
    "vid = video.frames(frames_info_final)\n",
    "# save the video\n",
    "start_time_str = start_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_path = VIDEO_EXPORT_DIR + f\"composite_video_{start_time_str}.mp4\"\n",
    "video.export(vid, save_path, fps=50)\n",
    "print(f\"Video saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optional: Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot mouse position with quadrant choice \"\"\"\n",
    "\n",
    "# Generate a color palette for unique quadrants\n",
    "unique_quadrants = centroid_df['selected_quadrant'].apply(lambda x: tuple(x)).unique()\n",
    "num_colors = len(unique_quadrants)\n",
    "colors = pc.qualitative.Alphabet  # Assuming `pc` is your color palette library\n",
    "\n",
    "# Convert the selected_quadrant lists to tuples\n",
    "centroid_df_copy = centroid_df[:20000].copy()\n",
    "centroid_df_copy['selected_quadrant'] = centroid_df_copy['selected_quadrant'].apply(lambda x: tuple(x))\n",
    "\n",
    "# Create a mapping from quadrant to color\n",
    "quadrant_to_color = {quadrant: colors[i] for i, quadrant in enumerate(unique_quadrants)}\n",
    "\n",
    "# Plot mouse positions with quadrant choice\n",
    "fig = go.Figure()\n",
    "for quadrant, quadrant_grp in centroid_df_copy.groupby(\"selected_quadrant\"):\n",
    "    norm_time = (\n",
    "        (quadrant_grp.index - quadrant_grp.index[0]) / (quadrant_grp.index[-1] - quadrant_grp.index[0])\n",
    "    ).values.round(3)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=quadrant_grp[\"x\"],\n",
    "            y=quadrant_grp[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            name=f\"Quadrant {quadrant}\",\n",
    "            marker={\n",
    "                \"color\": quadrant_to_color[quadrant],  # Use the color based on the quadrant\n",
    "                \"size\": 4,\n",
    "            },\n",
    "            hovertext=quadrant_grp.index.strftime('%Y-%m-%d %H:%M:%S.%f'),  # Add time information for hover\n",
    "            hoverinfo='text+x+y'  # Display hovertext, x, and y coordinates\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Overlay FOV for each quadrant camera\n",
    "fov_colors = ['black', 'green', 'purple', 'orange']  # Colors for each FOV\n",
    "for i, (camera_b, homography_matrix) in enumerate(zip(CAMERA_B_LIST, homographies)):\n",
    "    # Get the transformed corners\n",
    "    corners_top_view = get_transformed_corners(homography_matrix, CAMERA_DIMENSIONS)\n",
    "    if len(corners_top_view) == 4:  # Check if all 4 corners were successfully transformed\n",
    "        # Add FOV as a shape to the plot\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=np.append(corners_top_view[:, 0], corners_top_view[0, 0]),  # Close the rectangle\n",
    "            y=np.append(corners_top_view[:, 1], corners_top_view[0, 1]),  # Close the rectangle\n",
    "            mode=\"lines+markers\",\n",
    "            line=dict(color=fov_colors[i], width=2),\n",
    "            name=f\"FOV {camera_b}\"\n",
    "        ))\n",
    "    else:\n",
    "        print(f\"Could not plot FOV for {camera_b} due to insufficient transformed corners.\")\n",
    "\n",
    "# Update plot layout\n",
    "fig.update_layout(\n",
    "    title=\"Position Tracking over Time with Quadrant Camera FOVs\",\n",
    "    xaxis_title=\"X Coordinate\",\n",
    "    yaxis_title=\"Y Coordinate\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
