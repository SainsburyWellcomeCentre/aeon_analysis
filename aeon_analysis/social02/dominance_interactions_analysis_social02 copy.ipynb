{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import datajoint as dj\n",
    "import aeon\n",
    "from aeon.io import api\n",
    "from aeon.schema.schemas import social02\n",
    "from aeon.dj_pipeline.analysis.block_analysis import * \n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as sp\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import timedelta, datetime\n",
    "from scipy import stats\n",
    "from shapely.geometry import Point, Polygon\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment to analyse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = {\"experiment_name\": \"social0.2-aeon3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arena = 'AEON3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load social interaction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace with revised csv path\n",
    "base_path = '/ceph/aeon/aeon/code/scratchpad/Orsi/'\n",
    "\n",
    "tube_test_path = f'all_tube_test_videos/{arena}_tube_tests_revised_final.csv'\n",
    "fights_path = f'all_fighting_videos/{arena}_fights.csv'\n",
    "chasing_path = f'all_chasing_videos/{arena}_chases.csv'\n",
    "\n",
    "# Open CSV containing tube test data.\n",
    "tube_test_df = pd.read_csv(base_path + tube_test_path)\n",
    "# Open CSV containing fighting data.\n",
    "fights_df = pd.read_csv(base_path + fights_path)\n",
    "# Open CSV containing chasing data.\n",
    "chasing_df = pd.read_csv(base_path + chasing_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up social interaction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the start and end timestamps to datetime\n",
    "chasing_df['start_timestamp'] = pd.to_datetime(chasing_df['start_timestamp'], format='%Y-%m-%dT%H-%M-%S')\n",
    "chasing_df['end_timestamp'] = pd.to_datetime(chasing_df['end_timestamp'], format='%Y-%m-%dT%H-%M-%S')\n",
    "\n",
    "fights_df['start_timestamp'] = pd.to_datetime(fights_df['start_timestamp'], format='%Y-%m-%dT%H-%M-%S')\n",
    "fights_df['end_timestamp'] = pd.to_datetime(fights_df['end_timestamp'], format='%Y-%m-%dT%H-%M-%S')\n",
    "\n",
    "tube_test_df['start_timestamp'] = pd.to_datetime(tube_test_df['start_timestamp'], format='%Y-%m-%dT%H-%M-%S')\n",
    "tube_test_df['end_timestamp'] = pd.to_datetime(tube_test_df['end_timestamp'], format='%Y-%m-%dT%H-%M-%S')\n",
    "\n",
    "# Add a 'behavior_type' column to each data frame\n",
    "chasing_df['behavior_type'] = 'chasing'\n",
    "fights_df['behavior_type'] = 'fighting'\n",
    "tube_test_df['behavior_type'] = 'tube_test'\n",
    "\n",
    "#remane colunms to domiant_id\n",
    "chasing_df.rename(columns={'chaser_id': 'dominant_id'}, inplace=True)\n",
    "tube_test_df.rename(columns={'winner_id': 'dominant_id'}, inplace=True)\n",
    "fights_df['dominant_id'] = fights_df.get('dominant_id', 'NaN')\n",
    "\n",
    "# Combine the data frames\n",
    "combined_df = pd.concat([chasing_df, fights_df, tube_test_df])\n",
    "# Replace NaN values in 'dominant_id' with a string 'NaN'\n",
    "combined_df['dominant_id'] = combined_df['dominant_id'].fillna('NaN')\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add pre/post tubetest data manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data\n",
    "pre_tube_test = {\n",
    "    'behavior_type': ['pre_tube_test'] * 10,\n",
    "    'dominant_id': ['BAA-1104045'] * 2 + ['BAA-1104047'] * 8\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "pre_tube_test = pd.DataFrame(pre_tube_test)\n",
    "\n",
    "\n",
    "# Create the data\n",
    "post_tube_test = {\n",
    "    'behavior_type': ['pre_tube_test'] * 10,\n",
    "    'dominant_id': ['BAA-1104045'] * 1 + ['BAA-1104047'] * 9\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "post_tube_test = pd.DataFrame(post_tube_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get metadata like info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get start an edn as first and last block start\n",
    "first_block_start = (BlockAnalysis() & key).fetch()['block_start'][0]\n",
    "last_block_start= (BlockAnalysis() & key).fetch()['block_start'][-1]\n",
    "# Calculate the start/end of the same day\n",
    "experiment_start = datetime(first_block_start.year, first_block_start.month, first_block_start.day, 0, 0, 0)\n",
    "experiment_end = datetime(last_block_start.year, last_block_start.month, last_block_start.day+1, 0, 0, 0)\n",
    "experiment_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: find out where to get this from data, now its manual\n",
    "pre_solo_start = experiment_start\n",
    "pre_solo_end = datetime.strptime('2024-02-09 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "social_start =  datetime.strptime('2024-02-09 00:00:00', '%Y-%m-%d %H:%M:%S') \n",
    "social_end = datetime.strptime('2024-02-24 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "post_solo_start = datetime.strptime('2024-02-25 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "post_solo_end = experiment_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids of the animals\n",
    "unique_ids = np.unique(combined_df['dominant_id'])\n",
    "unique_ids = unique_ids[unique_ids != 'NaN']\n",
    "unique_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define light cycle periods\n",
    "night_start = '08:00'\n",
    "night_end = '19:00'\n",
    "twilight_start = '07:00'\n",
    "twilight_end = '08:00'\n",
    "dawn_start = '19:00'\n",
    "dawn_end = '20:00'\n",
    "day_start = '20:00'\n",
    "day_end = '07:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get metadata base on fist solo block using api\n",
    "metadata = (\n",
    "    api.load(f'/ceph/aeon/aeon/data/raw/{arena}/social0.2', social02.Metadata, experiment_start, first_block_start).iloc[0].metadata\n",
    ")\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting style settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for color mapping\n",
    "id_color_map = {\n",
    "    'NaN': 'grey',\n",
    "    unique_ids[1]: 'purple',\n",
    "    unique_ids[0]: 'green'\n",
    "} \n",
    "behaviour_map = {\n",
    "    'chasing': 'blue',\n",
    "    'fighting': 'red',\n",
    "    'tube_test': 'orange'\n",
    "} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dominance interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Temporal patterns of interactions and stability of dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the raster plot\n",
    "fig = px.scatter(\n",
    "    combined_df,\n",
    "    x='start_timestamp',\n",
    "    y='behavior_type',\n",
    "    color='dominant_id',\n",
    "    title='Behavior Raster Plot',\n",
    "    labels={'start_timestamp': 'Time', 'behavior_type': 'Behavior Type'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Set x-axis limits\n",
    "fig.update_xaxes(range=[social_start, social_end])\n",
    "# Set x-axis limits\n",
    "fig.update_yaxes(range=[-1.5, 2.5])  # Example date range\n",
    "\n",
    "# Iterate over each day in the two-week period\n",
    "# Iterate over each day in the two-week period\n",
    "current_day = social_start\n",
    "while current_day < social_end:\n",
    "    # Define the start and end times for each period\n",
    "    night_start_time = current_day.replace(hour=int(night_start.split(':')[0]), minute=int(night_start.split(':')[1]))\n",
    "    night_end_time = current_day.replace(hour=int(night_end.split(':')[0]), minute=int(night_end.split(':')[1]))\n",
    "    twilight_start_time = current_day.replace(hour=int(twilight_start.split(':')[0]), minute=int(twilight_start.split(':')[1]))\n",
    "    twilight_end_time = current_day.replace(hour=int(twilight_end.split(':')[0]), minute=int(twilight_end.split(':')[1]))\n",
    "    dawn_start_time = current_day.replace(hour=int(dawn_start.split(':')[0]), minute=int(dawn_start.split(':')[1]))\n",
    "    dawn_end_time = current_day.replace(hour=int(dawn_end.split(':')[0]), minute=int(dawn_end.split(':')[1]))\n",
    "    day_start_time = current_day.replace(hour=int(day_start.split(':')[0]), minute=int(day_start.split(':')[1]))\n",
    "    \n",
    "    # Calculate day_end_time correctly\n",
    "    if int(day_end.split(':')[0]) < int(day_start.split(':')[0]):\n",
    "        day_end_time = current_day + timedelta(days=1)\n",
    "        day_end_time = day_end_time.replace(hour=int(day_end.split(':')[0]), minute=int(day_end.split(':')[1]))\n",
    "    else:\n",
    "        day_end_time = current_day.replace(hour=int(day_end.split(':')[0]), minute=int(day_end.split(':')[1]))\n",
    "\n",
    "    # Add horizontal lines for light and dark periods\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=twilight_start_time,\n",
    "        x1=twilight_end_time,\n",
    "        y0=-1,\n",
    "        y1=-1,\n",
    "        line=dict(color=\"gray\", width=4)\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=night_start_time,\n",
    "        x1=night_end_time,\n",
    "        y0=-1,\n",
    "        y1=-1,\n",
    "        line=dict(color=\"black\", width=4)\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=dawn_start_time,\n",
    "        x1=dawn_end_time,\n",
    "        y0=-1,\n",
    "        y1=-1,\n",
    "        line=dict(color=\"gray\", width=4)\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=day_start_time,\n",
    "        x1=day_end_time,\n",
    "        y0=-1,\n",
    "        y1=-1,\n",
    "        line=dict(color=\"white\", width=4)\n",
    "    )\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_day += timedelta(days=1)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of time bins\n",
    "time_bins = pd.date_range(start=social_start, end=social_end, freq='24h')\n",
    "\n",
    "# Bin the start_timestamp into the created time bins\n",
    "combined_df['time_bin'] = pd.cut(combined_df['start_timestamp'], bins=time_bins)\n",
    "\n",
    "# Calculate the number of interactions for each behavior in each time bin\n",
    "interaction_counts = combined_df.groupby(['time_bin', 'behavior_type']).size().reset_index(name='total_interactions')\n",
    "\n",
    "# Convert the time_bin column to strings\n",
    "# Extract the start time of each bin\n",
    "interaction_counts['time_bin_start'] = interaction_counts['time_bin'].apply(lambda x: x.left)\n",
    "\n",
    "# Convert the start time to a string format\n",
    "interaction_counts['time_bin_start'] = interaction_counts['time_bin_start'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "# Create the line plot\n",
    "fig = px.line(\n",
    "    interaction_counts,\n",
    "    x='time_bin_start',\n",
    "    y='total_interactions',\n",
    "    color='behavior_type',\n",
    "    title='Number of Interactions Over Time for Each Behavior',\n",
    "    labels={'time_bin_start': 'Time Bin', 'total_interactions': 'Number of Interactions'},\n",
    "    color_discrete_map=behaviour_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fraction of winning for chasing\n",
    "chaser_counts_chasing = chasing_df['dominant_id'].value_counts(normalize=True).reset_index()\n",
    "chaser_counts_chasing.columns = ['dominant_id', 'p_wins']\n",
    "chaser_counts_chasing['behavior_type'] = 'Chasing'\n",
    "\n",
    "# Calculate the fraction of winning for tube test\n",
    "chaser_counts_tube_test = tube_test_df['dominant_id'].value_counts(normalize=True).reset_index()\n",
    "chaser_counts_tube_test.columns = ['dominant_id', 'p_wins']\n",
    "chaser_counts_tube_test['behavior_type'] = 'Tube Test'\n",
    "\n",
    "# Calculate the fraction of winning for pre tube test\n",
    "chaser_counts_pre_tube_test = pre_tube_test['dominant_id'].value_counts(normalize=True).reset_index()\n",
    "chaser_counts_pre_tube_test.columns = ['dominant_id', 'p_wins']\n",
    "chaser_counts_pre_tube_test['behavior_type'] = 'Pre Tube Test'\n",
    "\n",
    "# Calculate the fraction of winning for post tube test\n",
    "chaser_counts_post_tube_test = post_tube_test['dominant_id'].value_counts(normalize=True).reset_index()\n",
    "chaser_counts_post_tube_test.columns = ['dominant_id', 'p_wins']\n",
    "chaser_counts_post_tube_test['behavior_type'] = 'Post Tube Test'\n",
    "\n",
    "# Combine the data\n",
    "combined_counts = pd.concat([\n",
    "    chaser_counts_chasing, \n",
    "    chaser_counts_tube_test, \n",
    "    chaser_counts_pre_tube_test, \n",
    "    chaser_counts_post_tube_test\n",
    "])\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(\n",
    "    combined_counts,\n",
    "    x='behavior_type',\n",
    "    y='p_wins',\n",
    "    color='dominant_id',\n",
    "    title='Dominance per Behavior Type and Dominant ID',\n",
    "    labels={'behavior_type': 'Behavior Type', 'p_wins': 'Proportion of wins'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "fig.update_yaxes(range=[0, 1])  # Example date range\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate te number of winning events over all events for every 42h time bin and plot over time\n",
    "# Create a list of time bins\n",
    "time_bins = pd.date_range(start=social_start, end=social_end, freq='24h')\n",
    "\n",
    "# Bin the start_timestamp into the created time bins\n",
    "tube_test_df['time_bin'] = pd.cut(tube_test_df['start_timestamp'], bins=time_bins)\n",
    "\n",
    "# Calculate the number of winning events and total events in each time bin and id\n",
    "winning_counts = tube_test_df.groupby(['time_bin', 'dominant_id']).size().reset_index(name='total_events')\n",
    "\n",
    "# Calculate the fraction of winning events for each dominant_id in each time bin\n",
    "winning_counts['fraction_winning'] = winning_counts.groupby('time_bin')['total_events'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# Create a new column for day numbers starting from 1\n",
    "winning_counts['day_number'] = winning_counts['time_bin'].cat.codes + 1\n",
    "\n",
    "# Create the line plot for total events\n",
    "fig = px.line(\n",
    "    winning_counts,\n",
    "    x='day_number',\n",
    "    y='total_events',\n",
    "    color='dominant_id',\n",
    "    title='Winning Events Over Time (Tube test)',\n",
    "    labels={'day_number': 'Day Number', 'total_events': 'Winning Events'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Create the line plot for fraction of winning events\n",
    "fig = px.line(\n",
    "    winning_counts,\n",
    "    x='day_number',\n",
    "    y='fraction_winning',\n",
    "    color='dominant_id',\n",
    "    title='Fraction of Winning Events Over Time (Tube test)',\n",
    "    labels={'day_number': 'Day Number', 'fraction_winning': 'Proportion of Winning Events'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Create a list of time bins\n",
    "time_bins = pd.date_range(start=social_start, end=social_end, freq='24h')\n",
    "\n",
    "# Bin the start_timestamp into the created time bins\n",
    "chasing_df['time_bin'] = pd.cut(chasing_df['start_timestamp'], bins=time_bins)\n",
    "\n",
    "# Calculate the number of winning events and total events in each time bin and id\n",
    "winning_counts = chasing_df.groupby(['time_bin', 'dominant_id']).size().reset_index(name='total_events')\n",
    "\n",
    "# Calculate the fraction of winning events for each dominant_id in each time bin\n",
    "winning_counts['fraction_winning'] = winning_counts.groupby('time_bin')['total_events'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# Create a new column for day numbers starting from 1\n",
    "winning_counts['day_number'] = winning_counts['time_bin'].cat.codes + 1\n",
    "\n",
    "# Create the line plot for total events\n",
    "fig = px.line(\n",
    "    winning_counts,\n",
    "    x='day_number',\n",
    "    y='total_events',\n",
    "    color='dominant_id',\n",
    "    title='Winning Events Over Time (Chasing)',\n",
    "    labels={'day_number': 'Day Number', 'total_events': 'Winning Events'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Create the line plot for fraction of winning events\n",
    "fig = px.line(\n",
    "    winning_counts,\n",
    "    x='day_number',\n",
    "    y='fraction_winning',\n",
    "    color='dominant_id',\n",
    "    title='Fraction of Winning Events Over Time (Chasing)',\n",
    "    labels={'day_number': 'Day Number', 'fraction_winning': 'Proportion of Winning Events'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate te number of winning events over all events for every 42h time bin and plot over time\n",
    "# Create a list of time bins\n",
    "time_bins = pd.date_range(start=social_start, end=social_end, freq='24h')\n",
    "# Bin the start_timestamp into the created time bins\n",
    "tube_test_df['time_bin'] = pd.cut(tube_test_df['start_timestamp'], bins=time_bins)\n",
    "# Calculate the number of winning events and total events in each time bin and id\n",
    "winning_counts = tube_test_df.groupby(['time_bin', 'dominant_id']).size().reset_index(name='total_events')\n",
    "# Calculate the fraction of winning events for each dominant_id in each time bin\n",
    "winning_counts['fraction_winning'] = winning_counts.groupby('time_bin')['total_events'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# Convert the time_bin column to strings\n",
    "winning_counts['time_bin'] = winning_counts['time_bin'].astype(str)\n",
    "\n",
    "\n",
    "# Create the line plot\n",
    "fig = px.line(\n",
    "    winning_counts,\n",
    "    x='time_bin',\n",
    "    y='total_events',\n",
    "    color = 'dominant_id',\n",
    "    title='Winning Events Over Time (Tube test)',\n",
    "    labels={'time_bin': 'Time Bin', 'total_events': 'Winning Events'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Create the line plot\n",
    "fig = px.line(\n",
    "    winning_counts,\n",
    "    x='time_bin',\n",
    "    y='fraction_winning',\n",
    "    color = 'dominant_id',\n",
    "    title='Fraction of Winning Events Over Time (Tube test)',\n",
    "    labels={'time_bin': 'Time Bin', 'fraction_winning': 'Proportion of Winning Events'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Chasing speed and dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: we should do id swapping cleaning bfore calculating this (hope is that it doesnt matter much cuz two animals are pretty close to each other and synchronised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get this data if already calcuklated, if not calcualte here, but will ake a while\n",
    "chasing_speed_data = pd.read_csv(base_path + '/all_chasing_videos/chasing_speed.csv')\n",
    "chasing_speed_data\n",
    "\n",
    "if chasing_speed_data.empty:\n",
    "    print('No chasing speed data available')\n",
    "    #get centroid data during chasing events\n",
    "    block_subjects = (\n",
    "        BlockAnalysis.Subject.proj('position_x', 'position_y', 'position_timestamps')\n",
    "        & key\n",
    "        & f'block_start >= \"{social_start}\"'\n",
    "        & f'block_start <= \"{social_end}\"'\n",
    "    )\n",
    "    block_subjects_dict = block_subjects.fetch(as_dict=True)\n",
    "    \n",
    "    # Convert chasing_df timestamps to datetime\n",
    "    chasing_df['start_timestamp'] = pd.to_datetime(chasing_df['start_timestamp'])\n",
    "    chasing_df['end_timestamp'] = pd.to_datetime(chasing_df['end_timestamp'])\n",
    "\n",
    "    # Initialize an empty list to store the filtered position data\n",
    "    filtered_positions = []\n",
    "\n",
    "    # Loop through each row in chasing_df so we only get position that are within a chase\n",
    "    for _, row in chasing_df.iterrows():\n",
    "        start_time = row['start_timestamp']\n",
    "        end_time = row['end_timestamp']\n",
    "        \n",
    "        # Filter the position data for each subject\n",
    "        for s in block_subjects_dict:\n",
    "            # Convert position timestamps to datetime\n",
    "            position_timestamps = pd.to_datetime(s['position_timestamps'])\n",
    "            \n",
    "            # Filter based on the start and end timestamps\n",
    "            mask = (position_timestamps >= start_time) & (position_timestamps <= end_time)\n",
    "            \n",
    "            # Create a DataFrame for the filtered data\n",
    "            filtered_data = pd.DataFrame(\n",
    "        {\n",
    "                    \"subject_name\": [s[\"subject_name\"]] * sum(mask),\n",
    "                    \"position_timestamps\": position_timestamps[mask],\n",
    "                    \"position_x\": pd.Series(s[\"position_x\"])[mask].values,\n",
    "                    \"position_y\": pd.Series(s[\"position_y\"])[mask].values,\n",
    "                    \"start_time\": [start_time] * sum(mask),\n",
    "                    \"end_time\": [end_time] * sum(mask)\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Append the filtered data to the list\n",
    "            filtered_positions.append(filtered_data)\n",
    "\n",
    "    # Concatenate the list of filtered data into a single DataFrame\n",
    "    subjects_positions_df = pd.concat(filtered_positions)\n",
    "    \n",
    "    # Calculate the avg speed of each chasing event\n",
    "    # calculate the distance travelled by each subject and divide by the duration of chase\n",
    "    subjects_positions_df[\"speed\"] = (\n",
    "        subjects_positions_df.groupby(\"subject_name\")[[\"position_x\", \"position_y\"]].diff().apply(np.linalg.norm, axis=1)\n",
    "        / subjects_positions_df.reset_index()\n",
    "        .groupby(\"subject_name\")[\"position_timestamps\"]\n",
    "        .diff()\n",
    "        .dt.total_seconds()\n",
    "        .values\n",
    "    )\n",
    "    chasing_speed_data = subjects_positions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of nans, unrealistically high speeds and infs\n",
    "cm2px = 5.4 \n",
    "max_speed_threshold = 100 * cm2px # in cm/s\n",
    "chasing_speed_data = chasing_speed_data[chasing_speed_data['speed'] < max_speed_threshold]\n",
    "chasing_speed_data = chasing_speed_data[~chasing_speed_data['speed'].isna()]\n",
    "chasing_speed_data = chasing_speed_data[~chasing_speed_data['speed'].isin([np.inf, -np.inf])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average speed per chase\n",
    "# Step 1: Group speed_df by 'chase_id' and 'subject_name' to calculate avg speed per subject per chase\n",
    "avg_speed_per_subject = (\n",
    "    chasing_speed_data.groupby(['start_time', 'subject_name'])['speed']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'speed': 'avg_speed_per_subject'})\n",
    ")\n",
    "\n",
    "# Step 2: Now, group again by 'chase_id' to average over the subjects' average speeds\n",
    "avg_speed_per_chase = (\n",
    "    avg_speed_per_subject.groupby('start_time')['avg_speed_per_subject']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'avg_speed_per_subject': 'avg_speed_per_chase'})\n",
    ")\n",
    "avg_speed_per_chase.rename(columns={'start_time': 'start_timestamp'}, inplace=True)\n",
    "\n",
    "#merge back inot chase df\n",
    "chasing_df['start_timestamp'] = pd.to_datetime(chasing_df['start_timestamp'])\n",
    "avg_speed_per_chase['start_timestamp'] = pd.to_datetime(avg_speed_per_chase['start_timestamp'])\n",
    "avg_chasing_speed_df = chasing_df.merge(avg_speed_per_chase, on='start_timestamp', how='left')\n",
    "#get rid of rows whre missings speed\n",
    "avg_chasing_speed_df = avg_chasing_speed_df[~avg_chasing_speed_df['avg_speed_per_chase'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the speed of chases per chaser id\n",
    "# Initialize the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "for dominant_id in avg_chasing_speed_df['dominant_id'].unique():\n",
    "    if pd.isna(dominant_id):\n",
    "        color = 'gray'  # Assign a default color for NaN values\n",
    "    else:\n",
    "        color = id_color_map.get(dominant_id, 'black')  # Fallback to black if the id is not in the map\n",
    "    \n",
    "    dominant_df = avg_chasing_speed_df[avg_chasing_speed_df['dominant_id'] == dominant_id]\n",
    "    fig.add_trace(go.Box(\n",
    "        x=dominant_df['dominant_id'],\n",
    "        y=dominant_df['avg_speed_per_chase'],\n",
    "        name=str(dominant_id),  # Convert NaN to string to display properly\n",
    "        boxpoints='all',  # Show individual points\n",
    "        jitter=0.3,  # Add some jitter to avoid overlap\n",
    "        pointpos=-1.8,  # Position of the individual points (to the right of the box)\n",
    "        marker=dict(color=color)\n",
    "    ))\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Average Speed of Chases per Chaser ID',\n",
    "    xaxis_title='Chaser ID',\n",
    "    yaxis_title='Average Speed',\n",
    "    boxmode='group',  # Group box plots by chaser ID\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Filter data for the two chaser IDs\n",
    "chaser1_speeds = avg_chasing_speed_df[avg_chasing_speed_df['dominant_id'] == unique_ids[0]]['avg_speed_per_chase']\n",
    "chaser2_speeds = avg_chasing_speed_df[avg_chasing_speed_df['dominant_id'] == unique_ids[1]]['avg_speed_per_chase']\n",
    "\n",
    "# Perform the t-test\n",
    "t_stat, p_value_ttest = stats.ttest_ind(chaser1_speeds, chaser2_speeds, equal_var=False)\n",
    "\n",
    "\n",
    "# Add annotations for statistical test results\n",
    "fig.add_annotation(\n",
    "    x=0.5, y=1.05, xref='paper', yref='paper',\n",
    "    text=f\"T-test p-value: {p_value_ttest:.3f}\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=12, color='black'),\n",
    "    align='center'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dominance and foraging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign dominant id\n",
    "# Calculate the frequency of each dominant_id in combined_df\n",
    "dominant_id_counts = combined_df['dominant_id'].value_counts()\n",
    "# Identify the dominant_id with the highest frequency\n",
    "dominant_id = dominant_id_counts.idxmax()\n",
    "subordinate_id = unique_ids[0] if unique_ids[0] != dominant_id else unique_ids[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Pellet consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get foraging data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pellet data\n",
    "foraging_query = (\n",
    "    BlockSubjectAnalysis.Patch.proj('pellet_count', 'pellet_timestamps', 'patch_threshold')\n",
    "    & {\"spinnaker_video_source_name\": \"CameraTop\"} #this is the video source name which we rstrict once we selected tuff to keep in table\n",
    "    & key)\n",
    "\n",
    "# Fetch the data\n",
    "pellet_data = foraging_query.fetch()\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each entry in the pellet_data array\n",
    "for entry in pellet_data:\n",
    "    experiment_name = entry[0]\n",
    "    block_start = entry[1]\n",
    "    patch_name = entry[2]\n",
    "    subject_id = entry[3]\n",
    "    pellet_count = entry[4]\n",
    "    pellet_timestamps = entry[5]\n",
    "    patch_threshold =  entry[6]\n",
    "\n",
    "    \n",
    "    # For each pellet timestamp, create a dictionary and append to the list\n",
    "    for pellet_timestamp, threshold in zip(pellet_timestamps, patch_threshold):\n",
    "        pellet_timestamp = pd.to_datetime(pellet_timestamp)\n",
    "        # Determine the period based on the timestamp\n",
    "        if pre_solo_start <= pellet_timestamp <= pre_solo_end:\n",
    "            period = 'pre_solo'\n",
    "        elif social_start <= pellet_timestamp <= social_end:\n",
    "            period = 'social'\n",
    "        elif post_solo_start <= pellet_timestamp <= post_solo_end:\n",
    "            period = 'post_solo'\n",
    "        else:\n",
    "            ValueError(f\"Timestamp {pellet_timestamp} does not fall within any period\")\n",
    "\n",
    "        data.append({\n",
    "            'time': pellet_timestamp,\n",
    "            'subject_id': subject_id,\n",
    "            'threshold': threshold,\n",
    "            'rank': 'dominant' if subject_id == dominant_id else 'subordinate',\n",
    "            'period': period\n",
    "        })\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "pellet_df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the 'time' column to datetime\n",
    "pellet_df['time'] = pd.to_datetime(pellet_df['time'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning: filter out pellets that are less than 2s after previos pellet for teh subject\n",
    "pellet_df = pellet_df.sort_values(by=['subject_id', 'period', 'time'])\n",
    "pellet_df['time_diff'] = pellet_df.groupby(['subject_id', 'period'])['time'].diff()\n",
    "pellet_df['time_diff'] = pellet_df['time_diff'].dt.total_seconds()\n",
    "print(f'Number of pellets before filtering: {len(pellet_df)}')\n",
    "pellet_df = pellet_df[pellet_df['time_diff'].isna() | (pellet_df['time_diff'] > 2)]\n",
    "print(f'Number of pellets after filtering: {len(pellet_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raster_plot(df, id_color_map):\n",
    "    # Create subplots: two for pre_solo, two for post_solo, and one for social\n",
    "    fig = make_subplots(\n",
    "        rows=5, cols=1, shared_xaxes=False,\n",
    "        subplot_titles=(f'Pre Solo - {unique_ids[0]}', f'Pre Solo - {unique_ids[1]}', f'Post Solo - {unique_ids[0]}', f'Post Solo - {unique_ids[1]}', 'Social')\n",
    "    )\n",
    "\n",
    "    # Filter the DataFrame for pre_solo, post_solo, and social periods\n",
    "    pre_solo_df = df[df['period'] == 'pre_solo']\n",
    "    post_solo_df = df[df['period'] == 'post_solo']\n",
    "    social_df = df[df['period'] == 'social']\n",
    "\n",
    "    # Create scatter plots for each period if the DataFrame is not empty\n",
    "    if not pre_solo_df.empty:\n",
    "        for subject_id in pre_solo_df['subject_id'].unique():\n",
    "            subject_df = pre_solo_df[pre_solo_df['subject_id'] == subject_id]\n",
    "            pre_solo_scatter = px.scatter(\n",
    "                subject_df,\n",
    "                x='time',\n",
    "                y='subject_id',\n",
    "                color='subject_id',\n",
    "                labels={'time': 'Time'},\n",
    "                color_discrete_map=id_color_map\n",
    "            ).data[0]\n",
    "            pre_solo_scatter.showlegend = False\n",
    "            row = 1 if subject_id == unique_ids[0] else 2\n",
    "            fig.add_trace(pre_solo_scatter, row=row, col=1)\n",
    "\n",
    "    if not post_solo_df.empty:\n",
    "        for subject_id in post_solo_df['subject_id'].unique():\n",
    "            subject_df = post_solo_df[post_solo_df['subject_id'] == subject_id]\n",
    "            post_solo_scatter = px.scatter(\n",
    "                subject_df,\n",
    "                x='time',\n",
    "                y='subject_id',\n",
    "                color='subject_id',\n",
    "                labels={'time': 'Time'},\n",
    "                color_discrete_map=id_color_map\n",
    "            ).data[0]\n",
    "            post_solo_scatter.showlegend = False\n",
    "            row = 3 if subject_id == unique_ids[0] else 4\n",
    "            fig.add_trace(post_solo_scatter, row=row, col=1)\n",
    "        \n",
    "    if not social_df.empty:\n",
    "        for subject_id in social_df['subject_id'].unique():\n",
    "            subject_df = social_df[social_df['subject_id'] == subject_id]\n",
    "            social_scatter = px.scatter(\n",
    "                subject_df,\n",
    "                x='time',\n",
    "                y='subject_id',\n",
    "                color='subject_id',\n",
    "                labels={'time': 'Time'},\n",
    "                color_discrete_map=id_color_map\n",
    "            ).data[0]\n",
    "            fig.add_trace(social_scatter, row=5, col=1)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text='Pellet Raster Plots',\n",
    "        showlegend=True,\n",
    "        height = 800\n",
    "    )\n",
    "\n",
    "    # Remove y-axis labels\n",
    "    for i in range(1, 6):\n",
    "        fig.update_yaxes(title_text='', row=i, col=1)\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "# Create the raster plot for pre_solo, post_solo, and social periods\n",
    "create_raster_plot(pellet_df, id_color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process each period\n",
    "def process_period(period_df, row, time_bin_size, plot_type='pellet_count', daily=False):\n",
    "    if not period_df.empty:\n",
    "        # Create a copy of the DataFrame to avoid modifying the original\n",
    "        period_df_copy = period_df.copy()\n",
    "\n",
    "        # Set the 'time' as the DataFrame index\n",
    "        period_df_copy.set_index('time', inplace=True)\n",
    "\n",
    "        if plot_type == 'pellet_count':\n",
    "            # Bin the data by time and count the number of pellets per bin per subject\n",
    "            binned_df = period_df_copy.groupby([pd.Grouper(freq=time_bin_size), 'subject_id']).size().reset_index(name='pellet_count')\n",
    "        elif plot_type == 'threshold':\n",
    "            # Bin the data by time and calculate the mean, standard deviation, and SEM of the threshold per bin per subject\n",
    "            binned_df = period_df_copy.groupby([pd.Grouper(freq=time_bin_size), 'subject_id'])['threshold'].agg(['mean', 'std', 'count']).reset_index()\n",
    "            binned_df.rename(columns={'mean': 'average_threshold', 'std': 'std_threshold', 'count': 'n'}, inplace=True)\n",
    "            binned_df['sem_threshold'] = binned_df['std_threshold'] / np.sqrt(binned_df['n'])\n",
    "\n",
    "        # Create a complete time range for the bins\n",
    "        start_time = period_df['time'].min().floor(time_bin_size)\n",
    "        end_time = period_df['time'].max().ceil(time_bin_size)\n",
    "        time_range = pd.date_range(start=start_time, end=end_time, freq=time_bin_size)\n",
    "\n",
    "        # Get unique subject IDs\n",
    "        subject_ids = binned_df['subject_id'].unique()\n",
    "\n",
    "        # Create a MultiIndex with all combinations of time_range and subject_ids\n",
    "        multi_index = pd.MultiIndex.from_product([time_range, subject_ids], names=['time', 'subject_id'])\n",
    "\n",
    "        # Create an empty DataFrame with the MultiIndex\n",
    "        complete_df = pd.DataFrame(index=multi_index).reset_index()\n",
    "\n",
    "        # Merge the complete DataFrame with the binned data\n",
    "        complete_df = complete_df.merge(binned_df, on=['time', 'subject_id'], how='left').fillna(0)\n",
    "\n",
    "        if plot_type == 'pellet_count':\n",
    "            complete_df['pellet_count'] = complete_df['pellet_count'].astype(int)\n",
    "            if daily:\n",
    "                complete_df['hour_of_day'] = complete_df['time'].dt.hour\n",
    "                complete_df['date'] = complete_df['time'].dt.date\n",
    "                \n",
    "                average_pellets_per_hour = complete_df.groupby(['subject_id', 'hour_of_day'])['pellet_count'].mean().reset_index()\n",
    "\n",
    "                # Add individual daily data traces with faint lines\n",
    "                for subject_id in complete_df['subject_id'].unique():\n",
    "                    subject_data = complete_df[complete_df['subject_id'] == subject_id]\n",
    "                    for date in subject_data['date'].unique():\n",
    "                        daily_data = subject_data[subject_data['date'] == date]\n",
    "                        fig.add_trace(go.Scatter(\n",
    "                            x=daily_data['hour_of_day'],\n",
    "                            y=daily_data['pellet_count'],\n",
    "                            mode='lines',\n",
    "                            name=f'{subject_id} (daily)',\n",
    "                            line=dict(color=id_color_map[subject_id], width=1),\n",
    "                            opacity=0.2,\n",
    "                            showlegend=False\n",
    "                        ), row=row, col=1)\n",
    "\n",
    "                # Add the average data with more prominent lines\n",
    "                for subject_id in average_pellets_per_hour['subject_id'].unique():\n",
    "                    avg_data = average_pellets_per_hour[average_pellets_per_hour['subject_id'] == subject_id]\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=avg_data['hour_of_day'],\n",
    "                        y=avg_data['pellet_count'],\n",
    "                        mode='lines',\n",
    "                        name=f'{subject_id} (average)',\n",
    "                        line=dict(color=id_color_map[subject_id], width=2),\n",
    "                        showlegend=False\n",
    "                    ), row=row, col=1)\n",
    "            else:\n",
    "                for subject_id in subject_ids:\n",
    "                    subject_df = complete_df[complete_df['subject_id'] == subject_id]\n",
    "                    line_plot = px.line(\n",
    "                        subject_df,\n",
    "                        x='time',\n",
    "                        y='pellet_count',\n",
    "                        color='subject_id',\n",
    "                        labels={'pellet_count': 'Pellet Count', 'time': 'Time', 'subject_id': 'Subject ID'},\n",
    "                        color_discrete_map=id_color_map\n",
    "                    ).data[0]\n",
    "                    line_plot.showlegend = False\n",
    "                    fig.add_trace(line_plot, row=row, col=1)\n",
    "        elif plot_type == 'threshold':\n",
    "            for subject_id in complete_df['subject_id'].unique():\n",
    "                subject_df = complete_df[complete_df['subject_id'] == subject_id]\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=subject_df['time'],\n",
    "                    y=subject_df['average_threshold'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'{subject_id} Average Threshold',\n",
    "                    line=dict(color=id_color_map[subject_id]),\n",
    "                    error_y=dict(\n",
    "                        type='data',\n",
    "                        array=subject_df['sem_threshold'],\n",
    "                        visible=True\n",
    "                    )\n",
    "                ), row=row, col=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = {\"experiment_name\": \"social0.2-aeon3\"}\n",
    "foraging_query = (\n",
    "    BlockSubjectAnalysis.Patch.proj('pellet_count', 'pellet_timestamps', 'patch_threshold')\n",
    "    & {\"spinnaker_video_source_name\": \"CameraTop\"}\n",
    "    & key\n",
    "    & 'block_start >= \"2024-02-26 00:00:00\"'\n",
    "    & 'block_start <= \"2024-02-27 00:00:00\"'\n",
    ")\n",
    "\n",
    "foraging_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for pre_solo, post_solo, and social periods\n",
    "pre_solo_df = pellet_df[pellet_df['period'] == 'pre_solo']\n",
    "post_solo_df = pellet_df[pellet_df['period'] == 'post_solo']\n",
    "social_df = pellet_df[pellet_df['period'] == 'social']\n",
    "\n",
    "# Define unique IDs\n",
    "unique_ids = pellet_df['subject_id'].unique()\n",
    "\n",
    "# Create subplots for 1-hour bins\n",
    "fig = make_subplots(\n",
    "    rows=5, cols=1, shared_xaxes=False,\n",
    "    subplot_titles=(\n",
    "        f'Pre Solo - {unique_ids[0]}', \n",
    "        f'Pre Solo - {unique_ids[1]}', \n",
    "        f'Post Solo - {unique_ids[0]}', \n",
    "        f'Post Solo - {unique_ids[1]}',\n",
    "        f'Social - {unique_ids[0]} and {unique_ids[1]}'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Process each period for 1-hour bins\n",
    "process_period(pre_solo_df[pre_solo_df['subject_id'] == unique_ids[0]], 1, '1h', 'pellet_count')\n",
    "process_period(pre_solo_df[pre_solo_df['subject_id'] == unique_ids[1]], 2, '1h', 'pellet_count')\n",
    "process_period(post_solo_df[post_solo_df['subject_id'] == unique_ids[0]], 3, '1h', 'pellet_count')\n",
    "process_period(post_solo_df[post_solo_df['subject_id'] == unique_ids[1]], 4, '1h', 'pellet_count')\n",
    "process_period(social_df, 5, '1h', 'pellet_count')\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text='Pellets during Pre, Post Solo, and Social Periods (Time-binned)',\n",
    "    showlegend=True,\n",
    "    height=1000\n",
    ")\n",
    "\n",
    "# Remove y-axis labels\n",
    "for i in range(1, 6):\n",
    "    fig.update_yaxes(title_text='', row=i, col=1)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Repeat the process for 24-hour bins\n",
    "fig = make_subplots(\n",
    "    rows=5, cols=1, shared_xaxes=False,\n",
    "    subplot_titles=(\n",
    "        f'Pre Solo - {unique_ids[0]}', \n",
    "        f'Pre Solo - {unique_ids[1]}', \n",
    "        f'Post Solo - {unique_ids[0]}', \n",
    "        f'Post Solo - {unique_ids[1]}',\n",
    "        f'Social - {unique_ids[0]} and {unique_ids[1]}'\n",
    "    )\n",
    ")\n",
    "\n",
    "process_period(pre_solo_df[pre_solo_df['subject_id'] == unique_ids[0]], 1, '24h', 'pellet_count')\n",
    "process_period(pre_solo_df[pre_solo_df['subject_id'] == unique_ids[1]], 2, '24h', 'pellet_count')\n",
    "process_period(post_solo_df[post_solo_df['subject_id'] == unique_ids[0]], 3, '24h', 'pellet_count')\n",
    "process_period(post_solo_df[post_solo_df['subject_id'] == unique_ids[1]], 4, '24h', 'pellet_count')\n",
    "process_period(social_df, 5, '24h', 'pellet_count')\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text='Pellets during Pre, Post Solo, and Social Periods (Time-binned)',\n",
    "    showlegend=True,\n",
    "    height=1000\n",
    ")\n",
    "\n",
    "# Remove y-axis labels\n",
    "for i in range(1, 6):\n",
    "    fig.update_yaxes(title_text='', row=i, col=1)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Process threshold data\n",
    "fig = make_subplots(\n",
    "    rows=5, cols=1, shared_xaxes=False,\n",
    "    subplot_titles=(\n",
    "        f'Pre Solo - {unique_ids[0]}', \n",
    "        f'Pre Solo - {unique_ids[1]}', \n",
    "        f'Post Solo - {unique_ids[0]}', \n",
    "        f'Post Solo - {unique_ids[1]}',\n",
    "        f'Social - {unique_ids[0]} and {unique_ids[1]}'\n",
    "    )\n",
    ")\n",
    "\n",
    "process_period(pre_solo_df[pre_solo_df['subject_id'] == unique_ids[0]], 1, '24h', 'threshold')\n",
    "process_period(pre_solo_df[pre_solo_df['subject_id'] == unique_ids[1]], 2, '24h', 'threshold')\n",
    "process_period(post_solo_df[post_solo_df['subject_id'] == unique_ids[0]], 3, '24h', 'threshold')\n",
    "process_period(post_solo_df[post_solo_df['subject_id'] == unique_ids[1]], 4, '24h', 'threshold')\n",
    "process_period(social_df, 5, '24h', 'threshold')\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Average Threshold with SEM Error Bars during Pre, Post Solo, and Social Periods (24h bins)',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Average Threshold',\n",
    "    showlegend=False,\n",
    "    height=1000\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Process daily pellet count data\n",
    "fig = make_subplots(\n",
    "    rows=5, cols=1, shared_xaxes=False,\n",
    "    subplot_titles=(\n",
    "        f'Pre Solo - {unique_ids[0]}', \n",
    "        f'Pre Solo - {unique_ids[1]}', \n",
    "        f'Post Solo - {unique_ids[0]}', \n",
    "        f'Post Solo - {unique_ids[1]}',\n",
    "        f'Social - {unique_ids[0]} and {unique_ids[1]}'\n",
    "    )\n",
    ")\n",
    "\n",
    "process_period(pre_solo_df[pre_solo_df['subject_id'] == unique_ids[0]], 1, '1h', 'pellet_count', daily=True)\n",
    "process_period(pre_solo_df[pre_solo_df['subject_id'] == unique_ids[1]], 2, '1h', 'pellet_count', daily=True)\n",
    "process_period(post_solo_df[post_solo_df['subject_id'] == unique_ids[0]], 3, '1h', 'pellet_count', daily=True)\n",
    "process_period(post_solo_df[post_solo_df['subject_id'] == unique_ids[1]], 4, '1h', 'pellet_count', daily=True)\n",
    "process_period(social_df, 5, '1h', 'pellet_count', daily=True)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Pellets throughout the day',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Pellet Count',\n",
    "    showlegend=False,\n",
    "    height=1000  # Adjust the height as needed\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Patch types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get patch prefernce data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_query = (\n",
    "    BlockSubjectAnalysis.Patch.proj('pellet_count', 'pellet_timestamps', 'patch_threshold')\n",
    "    * BlockSubjectAnalysis.Preference.proj('final_preference_by_wheel','final_preference_by_time')\n",
    "    * BlockAnalysis.Patch.proj('patch_rate', 'patch_offset')\n",
    "    & key\n",
    "    & f'block_start >= \"{social_start}\"'\n",
    "    & f'block_start <= \"{social_end}\"'\n",
    ")\n",
    "preference_data = preference_query.fetch()\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "preference_list = []\n",
    "\n",
    "# Loop through each entry in the pellet_data array\n",
    "for entry in preference_data:\n",
    "    experiment_name = entry[0]\n",
    "    block_start = entry[1]\n",
    "    patch_name = entry[2]\n",
    "    subject_id = entry[3]\n",
    "    pellet_count = entry[4]\n",
    "    #pellet_timestamps = entry[5]\n",
    "    #patch_threshold = entry[6]\n",
    "    final_preference_by_wheel = entry[7]\n",
    "    final_preference_by_time = entry[8]\n",
    "    patch_rate = entry[9]\n",
    "    patch_offset = entry[10]\n",
    "    rank = 'dominant' if subject_id == dominant_id else 'subordinate'\n",
    "    patch_type = 'easy' if patch_rate == 0.01 else 'medium' if patch_rate == 0.0033 else 'hard' if patch_rate == 0.002 else 'unknown'\n",
    "\n",
    "# Create a dictionary for the current entry\n",
    "    data_dict = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'block_start': block_start,\n",
    "        'patch_name': patch_name,\n",
    "        'subject_id': subject_id,\n",
    "        'pellet_count': pellet_count,\n",
    "        'final_preference_by_wheel': final_preference_by_wheel,\n",
    "        'final_preference_by_time': final_preference_by_time,\n",
    "        'patch_rate': patch_rate,\n",
    "        'patch_offset': patch_offset,\n",
    "        'rank': rank,\n",
    "        'patch_type': patch_type\n",
    "    }\n",
    "    \n",
    "    # Append the dictionary to the list\n",
    "    preference_list.append(data_dict)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "preference_df = pd.DataFrame(preference_list)\n",
    "\n",
    "# Convert the 'block_start' column to datetime\n",
    "preference_df['block_start'] = pd.to_datetime(preference_df['block_start'])\n",
    "\n",
    "# Display the DataFrame\n",
    "preference_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for blocks where both subjects got above the threshold pellets overall\n",
    "pellet_threshold = 10\n",
    "\n",
    "# Group by block_start, patch_name, and subject_id and calculate the sum of pellet_count\n",
    "grouped_df = preference_df.groupby(['block_start', 'patch_name', 'subject_id']).agg({'pellet_count': 'sum'}).reset_index()\n",
    "\n",
    "# Filter the groups where the sum of pellet_count is above the threshold\n",
    "filtered_groups = grouped_df[grouped_df['pellet_count'] >= pellet_threshold]\n",
    "\n",
    "# Get unique values for block_start, patch_name, and subject_id\n",
    "unique_blocks = filtered_groups['block_start'].unique()\n",
    "unique_patches = preference_df['patch_name'].unique()\n",
    "unique_subjects = preference_df['subject_id'].unique()\n",
    "\n",
    "# Create a DataFrame with all possible combinations of block_start, patch_name, and subject_id\n",
    "all_combinations = pd.MultiIndex.from_product([unique_blocks, unique_patches, unique_subjects], names=['block_start', 'patch_name', 'subject_id']).to_frame(index=False)\n",
    "\n",
    "# Merge the all_combinations DataFrame with the original DataFrame to retain all columns\n",
    "preference_df_filtered = all_combinations.merge(preference_df, on=['block_start', 'patch_name', 'subject_id'], how='left').fillna(0)\n",
    "\n",
    "preference_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variable_by_patch_rate(df, variable, title, yaxis_title, id_color_map, category_order=['easy', 'medium', 'hard']):\n",
    "    \"\"\"\n",
    "    Plots a box plot for a given variable by patch rate for each subject.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data\n",
    "    - variable: The variable to plot (column name in the DataFrame)\n",
    "    - title: Title of the plot\n",
    "    - yaxis_title: Title of the y-axis\n",
    "    - id_color_map: Dictionary mapping subject IDs to colors\n",
    "    - category_order: Order of categories for the x-axis (default is ['easy', 'medium', 'hard'])\n",
    "    \"\"\"\n",
    "    # Initialize the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add box plot for each subject\n",
    "    for subject_id in df['subject_id'].unique():\n",
    "        subject_df = df[df['subject_id'] == subject_id]\n",
    "        fig.add_trace(go.Box(\n",
    "            x=subject_df['patch_type'],\n",
    "            y=subject_df[variable],\n",
    "            name=subject_id,\n",
    "            boxpoints='all',  # Show individual points\n",
    "            jitter=0.25,  # Add some jitter for individual points\n",
    "            pointpos=-1.4,  # Position of the individual points (to the right)\n",
    "            marker=dict(color=id_color_map[subject_id])\n",
    "        ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='Patch Rate',\n",
    "        yaxis_title=yaxis_title,\n",
    "        boxmode='group',  # Group box plots by patch_type\n",
    "        xaxis=dict(categoryorder='array', categoryarray=category_order),  \n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variable_by_patch_rate(\n",
    "    df=preference_df_filtered,\n",
    "    variable='pellet_count',\n",
    "    title='Pellets per Patch Rate',\n",
    "    yaxis_title='Pellet Count',\n",
    "    id_color_map=id_color_map\n",
    ")\n",
    "\n",
    "plot_variable_by_patch_rate(\n",
    "    df=preference_df_filtered,\n",
    "    variable='final_preference_by_time',\n",
    "    title='Preference Index per Patch Rate',\n",
    "    yaxis_title='Preference Index (time)',\n",
    "    id_color_map=id_color_map\n",
    ")\n",
    "\n",
    "plot_variable_by_patch_rate(\n",
    "    df=preference_df_filtered,\n",
    "    variable='final_preference_by_wheel',\n",
    "    title='Preference Index per Patch Rate',\n",
    "    yaxis_title='Preference Index (wheel)',\n",
    "    id_color_map=id_color_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daily_avg_variable(df, variable, id_color_map, title='Average Daily Value by Day'):\n",
    "    \"\"\"\n",
    "    Plots the average daily value of a specified variable by day for each subject and patch type.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data\n",
    "    - variable: The variable to plot (column name in the DataFrame)\n",
    "    - id_color_map: Dictionary mapping subject IDs to colors\n",
    "    - title: Title of the plot (default is 'Average Daily Value by Day')\n",
    "    \"\"\"\n",
    "    # Ensure block_start is a datetime object\n",
    "    df['block_start'] = pd.to_datetime(df['block_start'])\n",
    "\n",
    "    # Group by day, subject_id, and patch_type, and calculate the average of the specified variable\n",
    "    df['day'] = df['block_start'].dt.date\n",
    "    daily_avg_df = df.groupby(['day', 'subject_id', 'patch_type']).agg({variable: 'mean'}).reset_index()\n",
    "\n",
    "    # Exclude incomplete days\n",
    "    first_day = daily_avg_df['day'].min()\n",
    "    last_day = daily_avg_df['day'].max()\n",
    "    filtered_daily_avg_df = daily_avg_df[(daily_avg_df['day'] != first_day) & (daily_avg_df['day'] != last_day)]\n",
    "\n",
    "    # Create the plot\n",
    "    fig = px.line(\n",
    "        filtered_daily_avg_df,\n",
    "        x='day',\n",
    "        y=variable,\n",
    "        color='subject_id',\n",
    "        line_dash='patch_type',\n",
    "        title=title,\n",
    "        labels={'day': 'Day', variable: variable.replace('_', ' ').title()},\n",
    "        category_orders={'patch_type': ['easy', 'medium', 'hard']},\n",
    "        color_discrete_map=id_color_map\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_daily_avg_variable(preference_df_filtered, 'pellet_count', id_color_map, title='Average Daily Pellets by Day')\n",
    "plot_daily_avg_variable(preference_df_filtered, 'final_preference_by_wheel', id_color_map, title='Average Daily Preference Index by Day')\n",
    "plot_daily_avg_variable(preference_df_filtered, 'final_preference_by_time', id_color_map, title='Average Daily Preference Index by Day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dominance and other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_query = (\n",
    "    BlockAnalysis.Subject.proj('weights', 'weight_timestamps')\n",
    "    & {\"spinnaker_video_source_name\": \"CameraTop\"} #this is the video source name which we rstrict once we selected tuff to keep in table\n",
    "    & key\n",
    "    & f'block_start >= \"{social_start}\"'\n",
    "    & f'block_start <= \"{social_end}\"'\n",
    ")\n",
    "\n",
    "weight_data =  weight_query.fetch()\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each entry in the weight_data array\n",
    "for entry in weight_data:\n",
    "    subject_name = entry[0]\n",
    "    block_start = entry[1]\n",
    "    subject_id = entry[2]\n",
    "    weights = entry[3]\n",
    "    weight_timestamps = entry[4]\n",
    "    \n",
    "    # For each weight measurement, create a dictionary and append to the list\n",
    "    for time, weight in zip(weight_timestamps, weights):\n",
    "        data.append({'time': time, 'weight': weight, 'subject_id': subject_id})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "weight_df = pd.DataFrame(data)\n",
    "\n",
    "# Convert the 'time' column to datetime\n",
    "weight_df['time'] = pd.to_datetime(weight_df['time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning: filter out measurements below 25g TODO: come up with better way to clean data\n",
    "weight_df = weight_df[weight_df['weight'] > 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the line plot\n",
    "fig = px.line(\n",
    "    weight_df,\n",
    "    x='time',\n",
    "    y='weight',\n",
    "    color='subject_id',\n",
    "    title='Weight during social period',\n",
    "    labels={'weight': 'Weight (g)', 'time': 'Time', 'subject_id': 'Subject ID'} ,\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'time' column as the index\n",
    "time_weight_df = weight_df.set_index('time', inplace=False)\n",
    "\n",
    "# Resample the data into hourly bins and calculate the average weight for each bin\n",
    "avg_weight_df = time_weight_df.groupby('subject_id').resample('24h').mean()\n",
    "#print(avg_weight_df)\n",
    "\n",
    "# Reset the index to use 'time' as a column again\n",
    "avg_weight_df.reset_index(inplace=True)\n",
    "\n",
    "# Create the line plot with resampled and interpolated data\n",
    "fig = px.line(\n",
    "    avg_weight_df,\n",
    "    x='time',\n",
    "    y='weight',\n",
    "    color='subject_id',\n",
    "    title='Average Weight during Social Period (Daily)',\n",
    "    labels={'weight': 'Weight (g)', 'time': 'Time', 'subject_id': 'Subject ID'},\n",
    "    color_discrete_map=id_color_map\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Social interactions and foraging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Temporal relationship between interactions and foraging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perpare interaction data for heatmmaps\n",
    "\n",
    "# Check if 'start_timestamp' is already the index\n",
    "if 'start_timestamp' not in combined_df.index.names:\n",
    "    combined_df.set_index('start_timestamp', inplace=True)\n",
    "# Resample the data into 1-hour bins and count the number of events for each behaviour type\n",
    "events_per_hour = combined_df.groupby('behavior_type').resample('1h').size().reset_index(name='event_count')\n",
    "\n",
    "#normalise teh event counts for each behaviour type to get a metric between 0-1\n",
    "# Calculate the min and max event count for each behavior type\n",
    "min_max_counts = events_per_hour.groupby('behavior_type')['event_count'].agg(['min', 'max']).reset_index()\n",
    "min_max_counts.rename(columns={'min': 'min_event_count', 'max': 'max_event_count'}, inplace=True)\n",
    "\n",
    "# Merge the min and max event counts back to the original DataFrame\n",
    "events_per_hour = events_per_hour.merge(min_max_counts, on='behavior_type')\n",
    "\n",
    "# Apply min-max normalization\n",
    "events_per_hour['normalized_event_count'] = (\n",
    "    (events_per_hour['event_count'] - events_per_hour['min_event_count']) /\n",
    "    (events_per_hour['max_event_count'] - events_per_hour['min_event_count'])\n",
    ")\n",
    "\n",
    "# Drop the 'min_event_count' and 'max_event_count' columns if not needed\n",
    "events_per_hour.drop(columns=['min_event_count', 'max_event_count'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare pellet data for heatmaps\n",
    "# Define the time bin size (e.g., 1 hour)\n",
    "time_bin_size = '1h'\n",
    "# Create a copy of the DataFrame to avoid modifying the original\n",
    "pellet_df_copy = pellet_df.copy()\n",
    "\n",
    "# Set the 'time' as the DataFrame index\n",
    "pellet_df_copy.set_index('time', inplace=True)\n",
    "\n",
    "# Bin the data by time and count the number of pellets per bin per subject\n",
    "binned_pellet_df = pellet_df_copy.groupby([pd.Grouper(freq=time_bin_size), 'subject_id']).size().reset_index(name='pellet_count')\n",
    "    \n",
    "# Create a complete time range for the bins\n",
    "time_range = pd.date_range(start=social_start, end=social_end, freq=time_bin_size)\n",
    "\n",
    "# Get unique subject IDs\n",
    "subject_ids = binned_pellet_df['subject_id'].unique()\n",
    "\n",
    "# Create a MultiIndex with all combinations of time_range and subject_ids\n",
    "multi_index = pd.MultiIndex.from_product([time_range, subject_ids], names=['time', 'subject_id'])\n",
    "\n",
    "# Create an empty DataFrame with the MultiIndex\n",
    "pellets_per_hour = pd.DataFrame(index=multi_index).reset_index()\n",
    "\n",
    "# Merge the complete DataFrame with the binned pellet data\n",
    "pellets_per_hour = pellets_per_hour.merge(binned_pellet_df, on=['time', 'subject_id'], how='left').fillna(0)\n",
    "\n",
    "# Ensure pellet_count is an integer\n",
    "pellets_per_hour['pellet_count'] = pellets_per_hour['pellet_count'].astype(int)\n",
    "\n",
    "#normalise hte pellet counts pr subject\n",
    "# Calculate the min and max event count for each behavior type\n",
    "min_max_counts = pellets_per_hour.groupby('subject_id')['pellet_count'].agg(['min', 'max']).reset_index()\n",
    "min_max_counts.rename(columns={'min': 'min_event_count', 'max': 'max_event_count'}, inplace=True)\n",
    "\n",
    "# Merge the min and max event counts back to the original DataFrame\n",
    "pellets_per_hour = pellets_per_hour.merge(min_max_counts, on='subject_id')\n",
    "\n",
    "# Apply min-max normalization\n",
    "pellets_per_hour['normalized_pellet_count'] = (\n",
    "    (pellets_per_hour['pellet_count'] - pellets_per_hour['min_event_count']) /\n",
    "    (pellets_per_hour['max_event_count'] - pellets_per_hour['min_event_count'])\n",
    ")\n",
    "\n",
    "# Drop the 'min_event_count' and 'max_event_count' columns if not needed\n",
    "pellets_per_hour.drop(columns=['min_event_count', 'max_event_count'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge two dfs\n",
    "#add pellet count to event table table\n",
    "events_per_hour.rename(columns={'start_timestamp': 'time'}, inplace=True)\n",
    "# Merge the DataFrames on 'time' and 'subject_id'\n",
    "merged_df = pd.merge(events_per_hour, pellets_per_hour, on=['time'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data to create a DataFrame for the heatmap\n",
    "heatmap_data = merged_df.pivot_table(\n",
    "    index='time',\n",
    "    columns=['subject_id', 'behavior_type'],\n",
    "    values=['normalized_event_count', 'normalized_pellet_count']\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex to create unique labels for the heatmap rows\n",
    "heatmap_data.columns = [f'{val}_{subj}_{beh}' for val, subj, beh in heatmap_data.columns]\n",
    "\n",
    "# Create a new DataFrame with just the rows you want to plot\n",
    "\n",
    "# Extract pellet counts for both subjects (assuming no behavior type distinction is needed)\n",
    "pellet_counts_47 = heatmap_data.filter(like='normalized_pellet_count_BAA-1104047')\n",
    "pellet_counts_45 = heatmap_data.filter(like='normalized_pellet_count_BAA-1104045')\n",
    "\n",
    "# Average event counts across both subjects for each behavior type\n",
    "event_counts_chasing = heatmap_data.filter(like='chasing').mean(axis=1) *2\n",
    "event_counts_fighting = heatmap_data.filter(like='fighting').mean(axis=1) *2\n",
    "event_counts_tube_test = heatmap_data.filter(like='tube_test').mean(axis=1) *2\n",
    "\n",
    "# Combine the data into a single DataFrame for plotting\n",
    "heatmap_final = pd.DataFrame({\n",
    "    'Pellet Count BAA-1104047': pellet_counts_47.mean(axis=1),\n",
    "    'Pellet Count BAA-1104045': pellet_counts_45.mean(axis=1),\n",
    "    'Chasing Events': event_counts_chasing,\n",
    "    'Fighting Events': event_counts_fighting,\n",
    "    'Tube Test Events': event_counts_tube_test\n",
    "}, index=heatmap_data.index)\n",
    "\n",
    "# Create the heatmap\n",
    "fig = px.imshow(\n",
    "    heatmap_final.T,  # Transpose to have the measurements on the y-axis\n",
    "    labels={'x': 'Time (1h Bins)', 'y': 'Measurement'},\n",
    "    color_continuous_scale='Viridis',\n",
    "    aspect='auto',\n",
    "    zmin=0, zmax=1  # Set the color scale to range from 0 to 1\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Pellet Counts and Events During Social Period',\n",
    "    xaxis_title='Time (1h Bins)',\n",
    "    yaxis_title=None,\n",
    "    coloraxis_colorbar=dict(title='Normalized Value'),\n",
    "    template='plotly_white',\n",
    "    height=500  # Adjust height as needed\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing pellet counts after interactions vs at actie times when no interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting interactions not folowed by other interactions\n",
    "time_window_length = 5  # in minutes\n",
    "\n",
    "# Initialize a DataFrame to store good chases\n",
    "good_events_df = pd.DataFrame()\n",
    "combined_df = combined_df.reset_index()\n",
    "\n",
    "# Iterate through each row in chasing_df\n",
    "for index, event in combined_df.iterrows():\n",
    "    event_time = event['start_timestamp']\n",
    "\n",
    "    # Calculate the start and end time of the time window\n",
    "    start_time = event_time\n",
    "    end_time = event_time + pd.Timedelta(minutes=time_window_length)\n",
    "    \n",
    "    # Filter out events that happened within the time window\n",
    "    events_in_time_window = combined_df[\n",
    "        (combined_df['start_timestamp'] >= start_time) & \n",
    "        (combined_df['start_timestamp'] <= end_time) & \n",
    "        (combined_df.index != index)  # Exclude the current event\n",
    "    ]\n",
    "    \n",
    "    # If no events are found in the time window, add the chase to good_chases_df\n",
    "    if events_in_time_window.empty:\n",
    "        good_events_df = pd.concat([good_events_df, event.to_frame().T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For baseline, choose random timestamps duirng the light off OR not in nest (as a proxy for awake times) where no events occur \n",
    "\n",
    "# Function to check if a timestamp is valid\n",
    "def is_valid_timestamp(new_timestamp, existing_timestamps, time_window, night_start, night_end):\n",
    "    # Extract time from the timestamp\n",
    "    new_time = new_timestamp.time()\n",
    "    \n",
    "    # Check if the timestamp falls within the light cycle period\n",
    "    if not (night_start <= new_time < night_end):\n",
    "        return False\n",
    "    \n",
    "    # Check if the timestamp is within the time window length\n",
    "    for ts in existing_timestamps:\n",
    "        if abs((new_timestamp - ts).total_seconds()) < time_window.total_seconds():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "start_timestamps = combined_df['start_timestamp'].sort_values().reset_index(drop=True)\n",
    "time_window = pd.Timedelta(minutes=time_window_length)   \n",
    "\n",
    "# Generate  random valid timestamps\n",
    "valid_timestamps = []\n",
    "min_timestamp = start_timestamps.min()\n",
    "max_timestamp = start_timestamps.max()\n",
    "\n",
    "while len(valid_timestamps) < len(good_events_df):\n",
    "    random_timestamp = min_timestamp + timedelta(seconds=np.random.randint(0, int((max_timestamp - min_timestamp).total_seconds())))\n",
    "    if is_valid_timestamp(random_timestamp, start_timestamps, time_window, datetime.strptime(night_start, '%H:%M').time(), datetime.strptime(night_end, '%H:%M').time()):\n",
    "        valid_timestamps.append(random_timestamp)\n",
    "        start_timestamps = pd.concat([start_timestamps, pd.Series([random_timestamp])]).sort_values().reset_index(drop=True)\n",
    "\n",
    "# Convert the list of valid timestamps to a DataFrame\n",
    "valid_timestamps_df = pd.DataFrame(valid_timestamps, columns=['random_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cocatenate timestmps for event a no event post periods\n",
    "# Extract start_timestamps from good_events\n",
    "good_events_start_timestamps = good_events_df['start_timestamp']\n",
    "good_events_behaviour_type = good_events_df['behavior_type']\n",
    "good_events_dominant_id = good_events_df['dominant_id']\n",
    "\n",
    "# Create DataFrame for good_events with event column set to True\n",
    "good_events_df_with_event = pd.DataFrame({\n",
    "    'start_timestamps': good_events_start_timestamps,\n",
    "    'event': True,\n",
    "    'behaviour_type': good_events_behaviour_type,\n",
    "    'dominant_id': good_events_dominant_id\n",
    "})\n",
    "\n",
    "# Create DataFrame for valid_timestamps with event column set to False\n",
    "valid_timestamps_df_with_event = pd.DataFrame({\n",
    "    'start_timestamps': valid_timestamps_df['random_timestamp'],\n",
    "    'event': False\n",
    "})\n",
    "# Concatenate the DataFrames\n",
    "all_timestamps_df = pd.concat([good_events_df_with_event, valid_timestamps_df_with_event]).sort_values(by='start_timestamps').reset_index(drop=True)\n",
    "\n",
    "# Add end_timestamps column\n",
    "all_timestamps_df['end_timestamps'] = all_timestamps_df['start_timestamps'] + timedelta(minutes=time_window_length)\n",
    "\n",
    "# Display the new DataFrame\n",
    "all_timestamps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pellets within time window\n",
    "results = []\n",
    "\n",
    "# Get all unique subject IDs\n",
    "all_subject_ids = pellet_df['subject_id'].unique()\n",
    "\n",
    "for timestamp in all_timestamps_df.iterrows():\n",
    "    start_time = timestamp[1]['start_timestamps']\n",
    "    end_time = timestamp[1]['end_timestamps']\n",
    "    \n",
    "    # Get the pellet data within the time range\n",
    "    pellet_data = pellet_df[(pellet_df['time'] >= start_time) & (pellet_df['time'] <= end_time)]\n",
    "    \n",
    "    # Calculate the total number of pellets per subject\n",
    "    total_pellets = pellet_data['subject_id'].value_counts()\n",
    "    \n",
    "    # Append results for each subject, including those with zero pellets\n",
    "    for subject_id in all_subject_ids:\n",
    "        pellet_count = total_pellets.get(subject_id, 0)\n",
    "        result = timestamp[1].to_dict()  # Copy all columns from all_timestamps_df\n",
    "        result.update({\n",
    "            'subject_id': subject_id,\n",
    "            'pellet_number': pellet_count\n",
    "        })\n",
    "        results.append(result)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "event_pellets_df = pd.DataFrame(results)\n",
    "\n",
    "# Replace 'nan' values in 'behaviour_type' column with 'no_interaction'\n",
    "event_pellets_df['behaviour_type'] = event_pellets_df['behaviour_type'].fillna('no_interaction')\n",
    "\n",
    "# Display the new DataFrame\n",
    "event_pellets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only get events where both subjects had pellets after the event\n",
    "grouped = event_pellets_df.groupby('start_timestamps')\n",
    "\n",
    "# Define a function to filter groups\n",
    "def filter_group(group):\n",
    "    # Check if all pellet numbers are non-null and non-zero\n",
    "    return group['pellet_number'].notna().all() and (group['pellet_number'] != 0).all()\n",
    "\n",
    "# Apply the filter function to each group and concatenate the results\n",
    "filtered_df = pd.concat([group for name, group in grouped if filter_group(group)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order of categories for 'behaviour_type'\n",
    "category_order = ['no_interaction'] + [x for x in filtered_df['behaviour_type'].unique() if x != 'no_interaction']\n",
    "\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.box(\n",
    "    filtered_df,\n",
    "    x='behaviour_type',\n",
    "    y='pellet_number',\n",
    "    color='subject_id',\n",
    "    title='Pellets post events (if at least 1 pellet)',\n",
    "    labels={'subject_id': 'Subject ID', 'pellet_number': 'Pellets 5min post event', 'behaviour_type': 'Social interaction type'},\n",
    "    category_orders={'behaviour_type': category_order},\n",
    "    color_discrete_map=id_color_map,\n",
    "    points='all',\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.box(\n",
    "    filtered_df[filtered_df['subject_id'] != 'nan'],\n",
    "    x='event',\n",
    "    y='pellet_number',\n",
    "    color='subject_id',\n",
    "    title='Pellets after social interaction vs no interaction',\n",
    "    labels={'subject_id': 'Subject ID', 'pellet_number': 'Pellets 5min post event', 'event': 'Post social interation'},\n",
    "    category_orders={'behaviour_type': category_order},\n",
    "    color_discrete_map=id_color_map,\n",
    "    points='all',\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Outcome of intractions and foraging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get number of pellets per subject in each post-chase period\n",
    "results = []\n",
    "for index, chase in chasing_df.iterrows():\n",
    "    dominant_id = str(chase['dominant_id'])\n",
    "    subordinate_id = (set(unique_ids) - {dominant_id}).pop()\n",
    "    chase_time = chase['start_timestamp']\n",
    "    start_time = chase_time\n",
    "    end_time = chase_time + pd.Timedelta(minutes=time_window)\n",
    "    \n",
    "    pellet_data = pellet_df[(pellet_df['time'] >= start_time) & (pellet_df['time'] <= end_time)]\n",
    "    total_pellets = pellet_data['subject_id'].value_counts()\n",
    "    \n",
    "    for subject_id, outcome in [(dominant_id, 'dominant'), (subordinate_id, 'subordinate')]:\n",
    "        results.append({\n",
    "            'start_timestamps': start_time,\n",
    "            'end_timestamps': end_time,\n",
    "            'outcome': outcome,\n",
    "            'subject_id': subject_id,\n",
    "            'pellet_number': total_pellets.get(subject_id, 0)\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Filter events where both subjects had pellets\n",
    "results_df = results_df.groupby('start_timestamps').filter(lambda group: group['pellet_number'].notna().all() and (group['pellet_number'] != 0).all())\n",
    "\n",
    "# Plot number of pellets post tubetest per subject when dominant and when subordinate\n",
    "fig = px.box(\n",
    "    results_df[results_df['subject_id'] != 'nan'],\n",
    "    x='subject_id',\n",
    "    y='pellet_number',\n",
    "    color='outcome',\n",
    "    title='Pellets post chase based on outcome (all events)',\n",
    "    labels={'subject_id': 'Subject ID', 'pellet_number': f'Pellets {time_window}min post event', 'outcome': 'Outcome of event'},\n",
    "    points='all',\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Get number of pellets per subject in each post-tube test period\n",
    "results = []\n",
    "for index, test in tube_test_df.iterrows():\n",
    "    dominant_id = str(test['dominant_id'])\n",
    "    subordinate_id = (set(unique_ids) - {dominant_id}).pop()\n",
    "    test_time = test['start_timestamp']\n",
    "    start_time = test_time\n",
    "    end_time = test_time + pd.Timedelta(minutes=time_window)\n",
    "    \n",
    "    pellet_data = pellet_df[(pellet_df['time'] >= start_time) & (pellet_df['time'] <= end_time)]\n",
    "    total_pellets = pellet_data['subject_id'].value_counts()\n",
    "    \n",
    "    for subject_id, outcome in [(dominant_id, 'dominant'), (subordinate_id, 'subordinate')]:\n",
    "        results.append({\n",
    "            'start_timestamps': start_time,\n",
    "            'end_timestamps': end_time,\n",
    "            'outcome': outcome,\n",
    "            'subject_id': subject_id,\n",
    "            'pellet_number': total_pellets.get(subject_id, 0)\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Filter events where both subjects had pellets\n",
    "results_df = results_df.groupby('start_timestamps').filter(lambda group: group['pellet_number'].notna().all() and (group['pellet_number'] != 0).all())\n",
    "\n",
    "# Plot number of pellets post tube test per subject when dominant and when subordinate\n",
    "fig = px.box(\n",
    "    results_df[results_df['subject_id'] != 'nan'],\n",
    "    x='subject_id',\n",
    "    y='pellet_number',\n",
    "    color='outcome',\n",
    "    title='Pellets post tube test based on outcome (all events)',\n",
    "    labels={'subject_id': 'Subject ID', 'pellet_number': f'Pellets {time_window}min post event', 'outcome': 'Outcome of event'},\n",
    "    points='all',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the outcome variable\n",
    "event_pellets_df['outcome'] = event_pellets_df.apply(\n",
    "    lambda row: 'dominant' if (row['dominant_id'] == row['subject_id'] and \n",
    "                               row['dominant_id'] in unique_ids and \n",
    "                               row['subject_id'] in unique_ids) else \n",
    "                'subordinate' if (row['dominant_id'] != row['subject_id'] and \n",
    "                                  row['dominant_id'] in unique_ids and \n",
    "                                  row['subject_id'] in unique_ids) else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create the scatter plot\n",
    "filtered_chasing_df = event_pellets_df[\n",
    "    (event_pellets_df['behaviour_type'] == 'chasing') &\n",
    "    (event_pellets_df['dominant_id'].isin(unique_ids)) &\n",
    "    (event_pellets_df['subject_id'].isin(unique_ids))\n",
    "]\n",
    "fig = px.box(\n",
    "    filtered_chasing_df,\n",
    "    x='subject_id',\n",
    "    y='pellet_number',\n",
    "    color='outcome',\n",
    "    title='Pellets post chasing based on outcome (only if no events in time window)',\n",
    "    labels={'subject_id': 'Subject ID', 'pellet_number': 'Pellets 5min post event', 'event': 'Post social interation'},\n",
    "    category_orders={'behaviour_type': category_order},\n",
    "    points='all',\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Create the scatter plot\n",
    "filtered_tube_test_df = event_pellets_df[\n",
    "    (event_pellets_df['behaviour_type'] == 'tube_test') &\n",
    "    (event_pellets_df['dominant_id'].isin(unique_ids)) &\n",
    "    (event_pellets_df['subject_id'].isin(unique_ids))\n",
    "]\n",
    "fig = px.box(\n",
    "    filtered_tube_test_df,\n",
    "    x='subject_id',\n",
    "    y='pellet_number',\n",
    "    color='outcome',\n",
    "    title='Pellets post tue test based on outcome (only if no events in time window)',\n",
    "    labels={'subject_id': 'Subject ID', 'pellet_number': 'Pellets 5min post event', 'event': 'Post social interation'},\n",
    "    category_orders={'behaviour_type': category_order},\n",
    "    points='all',\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
